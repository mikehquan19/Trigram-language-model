{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "names = open('names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "0f0LQLcQNwd5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set of all characters mapped with index\n",
        "letters = sorted(list(set(\"\".join(names))))\n",
        "letters = ['.'] + letters\n",
        "letters_to_index = {letter: i for i, letter in enumerate(letters)}\n",
        "index_to_letters = {i: letter for letter, i in letters_to_index.items()}\n",
        "\n",
        "print(f\"letters to index: {letters_to_index}\")\n",
        "print(f\"index to letters: {index_to_letters}\")\n",
        "print()\n",
        "\n",
        "# set of all bigrams of characters mapped with index\n",
        "bigrams = []\n",
        "for l1 in letters:\n",
        "  for l2 in letters:\n",
        "    bigram = l1 + l2\n",
        "    bigrams.append(bigram)\n",
        "\n",
        "bigrams_to_index = {bigram : index for index, bigram in enumerate(bigrams)}\n",
        "index_to_bigrams = {index : bigram for index, bigram in enumerate(bigrams)}\n",
        "\n",
        "print(f\"bigrams to index: {bigrams_to_index}\")\n",
        "print(f\"index to bigrams: {index_to_bigrams}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNcMoNT9TAdT",
        "outputId": "ca50e664-06c5-4005-c67d-3b102c93c6c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "letters to index: {'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "index to letters: {0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
            "\n",
            "bigrams to index: {'..': 0, '.a': 1, '.b': 2, '.c': 3, '.d': 4, '.e': 5, '.f': 6, '.g': 7, '.h': 8, '.i': 9, '.j': 10, '.k': 11, '.l': 12, '.m': 13, '.n': 14, '.o': 15, '.p': 16, '.q': 17, '.r': 18, '.s': 19, '.t': 20, '.u': 21, '.v': 22, '.w': 23, '.x': 24, '.y': 25, '.z': 26, 'a.': 27, 'aa': 28, 'ab': 29, 'ac': 30, 'ad': 31, 'ae': 32, 'af': 33, 'ag': 34, 'ah': 35, 'ai': 36, 'aj': 37, 'ak': 38, 'al': 39, 'am': 40, 'an': 41, 'ao': 42, 'ap': 43, 'aq': 44, 'ar': 45, 'as': 46, 'at': 47, 'au': 48, 'av': 49, 'aw': 50, 'ax': 51, 'ay': 52, 'az': 53, 'b.': 54, 'ba': 55, 'bb': 56, 'bc': 57, 'bd': 58, 'be': 59, 'bf': 60, 'bg': 61, 'bh': 62, 'bi': 63, 'bj': 64, 'bk': 65, 'bl': 66, 'bm': 67, 'bn': 68, 'bo': 69, 'bp': 70, 'bq': 71, 'br': 72, 'bs': 73, 'bt': 74, 'bu': 75, 'bv': 76, 'bw': 77, 'bx': 78, 'by': 79, 'bz': 80, 'c.': 81, 'ca': 82, 'cb': 83, 'cc': 84, 'cd': 85, 'ce': 86, 'cf': 87, 'cg': 88, 'ch': 89, 'ci': 90, 'cj': 91, 'ck': 92, 'cl': 93, 'cm': 94, 'cn': 95, 'co': 96, 'cp': 97, 'cq': 98, 'cr': 99, 'cs': 100, 'ct': 101, 'cu': 102, 'cv': 103, 'cw': 104, 'cx': 105, 'cy': 106, 'cz': 107, 'd.': 108, 'da': 109, 'db': 110, 'dc': 111, 'dd': 112, 'de': 113, 'df': 114, 'dg': 115, 'dh': 116, 'di': 117, 'dj': 118, 'dk': 119, 'dl': 120, 'dm': 121, 'dn': 122, 'do': 123, 'dp': 124, 'dq': 125, 'dr': 126, 'ds': 127, 'dt': 128, 'du': 129, 'dv': 130, 'dw': 131, 'dx': 132, 'dy': 133, 'dz': 134, 'e.': 135, 'ea': 136, 'eb': 137, 'ec': 138, 'ed': 139, 'ee': 140, 'ef': 141, 'eg': 142, 'eh': 143, 'ei': 144, 'ej': 145, 'ek': 146, 'el': 147, 'em': 148, 'en': 149, 'eo': 150, 'ep': 151, 'eq': 152, 'er': 153, 'es': 154, 'et': 155, 'eu': 156, 'ev': 157, 'ew': 158, 'ex': 159, 'ey': 160, 'ez': 161, 'f.': 162, 'fa': 163, 'fb': 164, 'fc': 165, 'fd': 166, 'fe': 167, 'ff': 168, 'fg': 169, 'fh': 170, 'fi': 171, 'fj': 172, 'fk': 173, 'fl': 174, 'fm': 175, 'fn': 176, 'fo': 177, 'fp': 178, 'fq': 179, 'fr': 180, 'fs': 181, 'ft': 182, 'fu': 183, 'fv': 184, 'fw': 185, 'fx': 186, 'fy': 187, 'fz': 188, 'g.': 189, 'ga': 190, 'gb': 191, 'gc': 192, 'gd': 193, 'ge': 194, 'gf': 195, 'gg': 196, 'gh': 197, 'gi': 198, 'gj': 199, 'gk': 200, 'gl': 201, 'gm': 202, 'gn': 203, 'go': 204, 'gp': 205, 'gq': 206, 'gr': 207, 'gs': 208, 'gt': 209, 'gu': 210, 'gv': 211, 'gw': 212, 'gx': 213, 'gy': 214, 'gz': 215, 'h.': 216, 'ha': 217, 'hb': 218, 'hc': 219, 'hd': 220, 'he': 221, 'hf': 222, 'hg': 223, 'hh': 224, 'hi': 225, 'hj': 226, 'hk': 227, 'hl': 228, 'hm': 229, 'hn': 230, 'ho': 231, 'hp': 232, 'hq': 233, 'hr': 234, 'hs': 235, 'ht': 236, 'hu': 237, 'hv': 238, 'hw': 239, 'hx': 240, 'hy': 241, 'hz': 242, 'i.': 243, 'ia': 244, 'ib': 245, 'ic': 246, 'id': 247, 'ie': 248, 'if': 249, 'ig': 250, 'ih': 251, 'ii': 252, 'ij': 253, 'ik': 254, 'il': 255, 'im': 256, 'in': 257, 'io': 258, 'ip': 259, 'iq': 260, 'ir': 261, 'is': 262, 'it': 263, 'iu': 264, 'iv': 265, 'iw': 266, 'ix': 267, 'iy': 268, 'iz': 269, 'j.': 270, 'ja': 271, 'jb': 272, 'jc': 273, 'jd': 274, 'je': 275, 'jf': 276, 'jg': 277, 'jh': 278, 'ji': 279, 'jj': 280, 'jk': 281, 'jl': 282, 'jm': 283, 'jn': 284, 'jo': 285, 'jp': 286, 'jq': 287, 'jr': 288, 'js': 289, 'jt': 290, 'ju': 291, 'jv': 292, 'jw': 293, 'jx': 294, 'jy': 295, 'jz': 296, 'k.': 297, 'ka': 298, 'kb': 299, 'kc': 300, 'kd': 301, 'ke': 302, 'kf': 303, 'kg': 304, 'kh': 305, 'ki': 306, 'kj': 307, 'kk': 308, 'kl': 309, 'km': 310, 'kn': 311, 'ko': 312, 'kp': 313, 'kq': 314, 'kr': 315, 'ks': 316, 'kt': 317, 'ku': 318, 'kv': 319, 'kw': 320, 'kx': 321, 'ky': 322, 'kz': 323, 'l.': 324, 'la': 325, 'lb': 326, 'lc': 327, 'ld': 328, 'le': 329, 'lf': 330, 'lg': 331, 'lh': 332, 'li': 333, 'lj': 334, 'lk': 335, 'll': 336, 'lm': 337, 'ln': 338, 'lo': 339, 'lp': 340, 'lq': 341, 'lr': 342, 'ls': 343, 'lt': 344, 'lu': 345, 'lv': 346, 'lw': 347, 'lx': 348, 'ly': 349, 'lz': 350, 'm.': 351, 'ma': 352, 'mb': 353, 'mc': 354, 'md': 355, 'me': 356, 'mf': 357, 'mg': 358, 'mh': 359, 'mi': 360, 'mj': 361, 'mk': 362, 'ml': 363, 'mm': 364, 'mn': 365, 'mo': 366, 'mp': 367, 'mq': 368, 'mr': 369, 'ms': 370, 'mt': 371, 'mu': 372, 'mv': 373, 'mw': 374, 'mx': 375, 'my': 376, 'mz': 377, 'n.': 378, 'na': 379, 'nb': 380, 'nc': 381, 'nd': 382, 'ne': 383, 'nf': 384, 'ng': 385, 'nh': 386, 'ni': 387, 'nj': 388, 'nk': 389, 'nl': 390, 'nm': 391, 'nn': 392, 'no': 393, 'np': 394, 'nq': 395, 'nr': 396, 'ns': 397, 'nt': 398, 'nu': 399, 'nv': 400, 'nw': 401, 'nx': 402, 'ny': 403, 'nz': 404, 'o.': 405, 'oa': 406, 'ob': 407, 'oc': 408, 'od': 409, 'oe': 410, 'of': 411, 'og': 412, 'oh': 413, 'oi': 414, 'oj': 415, 'ok': 416, 'ol': 417, 'om': 418, 'on': 419, 'oo': 420, 'op': 421, 'oq': 422, 'or': 423, 'os': 424, 'ot': 425, 'ou': 426, 'ov': 427, 'ow': 428, 'ox': 429, 'oy': 430, 'oz': 431, 'p.': 432, 'pa': 433, 'pb': 434, 'pc': 435, 'pd': 436, 'pe': 437, 'pf': 438, 'pg': 439, 'ph': 440, 'pi': 441, 'pj': 442, 'pk': 443, 'pl': 444, 'pm': 445, 'pn': 446, 'po': 447, 'pp': 448, 'pq': 449, 'pr': 450, 'ps': 451, 'pt': 452, 'pu': 453, 'pv': 454, 'pw': 455, 'px': 456, 'py': 457, 'pz': 458, 'q.': 459, 'qa': 460, 'qb': 461, 'qc': 462, 'qd': 463, 'qe': 464, 'qf': 465, 'qg': 466, 'qh': 467, 'qi': 468, 'qj': 469, 'qk': 470, 'ql': 471, 'qm': 472, 'qn': 473, 'qo': 474, 'qp': 475, 'qq': 476, 'qr': 477, 'qs': 478, 'qt': 479, 'qu': 480, 'qv': 481, 'qw': 482, 'qx': 483, 'qy': 484, 'qz': 485, 'r.': 486, 'ra': 487, 'rb': 488, 'rc': 489, 'rd': 490, 're': 491, 'rf': 492, 'rg': 493, 'rh': 494, 'ri': 495, 'rj': 496, 'rk': 497, 'rl': 498, 'rm': 499, 'rn': 500, 'ro': 501, 'rp': 502, 'rq': 503, 'rr': 504, 'rs': 505, 'rt': 506, 'ru': 507, 'rv': 508, 'rw': 509, 'rx': 510, 'ry': 511, 'rz': 512, 's.': 513, 'sa': 514, 'sb': 515, 'sc': 516, 'sd': 517, 'se': 518, 'sf': 519, 'sg': 520, 'sh': 521, 'si': 522, 'sj': 523, 'sk': 524, 'sl': 525, 'sm': 526, 'sn': 527, 'so': 528, 'sp': 529, 'sq': 530, 'sr': 531, 'ss': 532, 'st': 533, 'su': 534, 'sv': 535, 'sw': 536, 'sx': 537, 'sy': 538, 'sz': 539, 't.': 540, 'ta': 541, 'tb': 542, 'tc': 543, 'td': 544, 'te': 545, 'tf': 546, 'tg': 547, 'th': 548, 'ti': 549, 'tj': 550, 'tk': 551, 'tl': 552, 'tm': 553, 'tn': 554, 'to': 555, 'tp': 556, 'tq': 557, 'tr': 558, 'ts': 559, 'tt': 560, 'tu': 561, 'tv': 562, 'tw': 563, 'tx': 564, 'ty': 565, 'tz': 566, 'u.': 567, 'ua': 568, 'ub': 569, 'uc': 570, 'ud': 571, 'ue': 572, 'uf': 573, 'ug': 574, 'uh': 575, 'ui': 576, 'uj': 577, 'uk': 578, 'ul': 579, 'um': 580, 'un': 581, 'uo': 582, 'up': 583, 'uq': 584, 'ur': 585, 'us': 586, 'ut': 587, 'uu': 588, 'uv': 589, 'uw': 590, 'ux': 591, 'uy': 592, 'uz': 593, 'v.': 594, 'va': 595, 'vb': 596, 'vc': 597, 'vd': 598, 've': 599, 'vf': 600, 'vg': 601, 'vh': 602, 'vi': 603, 'vj': 604, 'vk': 605, 'vl': 606, 'vm': 607, 'vn': 608, 'vo': 609, 'vp': 610, 'vq': 611, 'vr': 612, 'vs': 613, 'vt': 614, 'vu': 615, 'vv': 616, 'vw': 617, 'vx': 618, 'vy': 619, 'vz': 620, 'w.': 621, 'wa': 622, 'wb': 623, 'wc': 624, 'wd': 625, 'we': 626, 'wf': 627, 'wg': 628, 'wh': 629, 'wi': 630, 'wj': 631, 'wk': 632, 'wl': 633, 'wm': 634, 'wn': 635, 'wo': 636, 'wp': 637, 'wq': 638, 'wr': 639, 'ws': 640, 'wt': 641, 'wu': 642, 'wv': 643, 'ww': 644, 'wx': 645, 'wy': 646, 'wz': 647, 'x.': 648, 'xa': 649, 'xb': 650, 'xc': 651, 'xd': 652, 'xe': 653, 'xf': 654, 'xg': 655, 'xh': 656, 'xi': 657, 'xj': 658, 'xk': 659, 'xl': 660, 'xm': 661, 'xn': 662, 'xo': 663, 'xp': 664, 'xq': 665, 'xr': 666, 'xs': 667, 'xt': 668, 'xu': 669, 'xv': 670, 'xw': 671, 'xx': 672, 'xy': 673, 'xz': 674, 'y.': 675, 'ya': 676, 'yb': 677, 'yc': 678, 'yd': 679, 'ye': 680, 'yf': 681, 'yg': 682, 'yh': 683, 'yi': 684, 'yj': 685, 'yk': 686, 'yl': 687, 'ym': 688, 'yn': 689, 'yo': 690, 'yp': 691, 'yq': 692, 'yr': 693, 'ys': 694, 'yt': 695, 'yu': 696, 'yv': 697, 'yw': 698, 'yx': 699, 'yy': 700, 'yz': 701, 'z.': 702, 'za': 703, 'zb': 704, 'zc': 705, 'zd': 706, 'ze': 707, 'zf': 708, 'zg': 709, 'zh': 710, 'zi': 711, 'zj': 712, 'zk': 713, 'zl': 714, 'zm': 715, 'zn': 716, 'zo': 717, 'zp': 718, 'zq': 719, 'zr': 720, 'zs': 721, 'zt': 722, 'zu': 723, 'zv': 724, 'zw': 725, 'zx': 726, 'zy': 727, 'zz': 728}\n",
            "index to bigrams: {0: '..', 1: '.a', 2: '.b', 3: '.c', 4: '.d', 5: '.e', 6: '.f', 7: '.g', 8: '.h', 9: '.i', 10: '.j', 11: '.k', 12: '.l', 13: '.m', 14: '.n', 15: '.o', 16: '.p', 17: '.q', 18: '.r', 19: '.s', 20: '.t', 21: '.u', 22: '.v', 23: '.w', 24: '.x', 25: '.y', 26: '.z', 27: 'a.', 28: 'aa', 29: 'ab', 30: 'ac', 31: 'ad', 32: 'ae', 33: 'af', 34: 'ag', 35: 'ah', 36: 'ai', 37: 'aj', 38: 'ak', 39: 'al', 40: 'am', 41: 'an', 42: 'ao', 43: 'ap', 44: 'aq', 45: 'ar', 46: 'as', 47: 'at', 48: 'au', 49: 'av', 50: 'aw', 51: 'ax', 52: 'ay', 53: 'az', 54: 'b.', 55: 'ba', 56: 'bb', 57: 'bc', 58: 'bd', 59: 'be', 60: 'bf', 61: 'bg', 62: 'bh', 63: 'bi', 64: 'bj', 65: 'bk', 66: 'bl', 67: 'bm', 68: 'bn', 69: 'bo', 70: 'bp', 71: 'bq', 72: 'br', 73: 'bs', 74: 'bt', 75: 'bu', 76: 'bv', 77: 'bw', 78: 'bx', 79: 'by', 80: 'bz', 81: 'c.', 82: 'ca', 83: 'cb', 84: 'cc', 85: 'cd', 86: 'ce', 87: 'cf', 88: 'cg', 89: 'ch', 90: 'ci', 91: 'cj', 92: 'ck', 93: 'cl', 94: 'cm', 95: 'cn', 96: 'co', 97: 'cp', 98: 'cq', 99: 'cr', 100: 'cs', 101: 'ct', 102: 'cu', 103: 'cv', 104: 'cw', 105: 'cx', 106: 'cy', 107: 'cz', 108: 'd.', 109: 'da', 110: 'db', 111: 'dc', 112: 'dd', 113: 'de', 114: 'df', 115: 'dg', 116: 'dh', 117: 'di', 118: 'dj', 119: 'dk', 120: 'dl', 121: 'dm', 122: 'dn', 123: 'do', 124: 'dp', 125: 'dq', 126: 'dr', 127: 'ds', 128: 'dt', 129: 'du', 130: 'dv', 131: 'dw', 132: 'dx', 133: 'dy', 134: 'dz', 135: 'e.', 136: 'ea', 137: 'eb', 138: 'ec', 139: 'ed', 140: 'ee', 141: 'ef', 142: 'eg', 143: 'eh', 144: 'ei', 145: 'ej', 146: 'ek', 147: 'el', 148: 'em', 149: 'en', 150: 'eo', 151: 'ep', 152: 'eq', 153: 'er', 154: 'es', 155: 'et', 156: 'eu', 157: 'ev', 158: 'ew', 159: 'ex', 160: 'ey', 161: 'ez', 162: 'f.', 163: 'fa', 164: 'fb', 165: 'fc', 166: 'fd', 167: 'fe', 168: 'ff', 169: 'fg', 170: 'fh', 171: 'fi', 172: 'fj', 173: 'fk', 174: 'fl', 175: 'fm', 176: 'fn', 177: 'fo', 178: 'fp', 179: 'fq', 180: 'fr', 181: 'fs', 182: 'ft', 183: 'fu', 184: 'fv', 185: 'fw', 186: 'fx', 187: 'fy', 188: 'fz', 189: 'g.', 190: 'ga', 191: 'gb', 192: 'gc', 193: 'gd', 194: 'ge', 195: 'gf', 196: 'gg', 197: 'gh', 198: 'gi', 199: 'gj', 200: 'gk', 201: 'gl', 202: 'gm', 203: 'gn', 204: 'go', 205: 'gp', 206: 'gq', 207: 'gr', 208: 'gs', 209: 'gt', 210: 'gu', 211: 'gv', 212: 'gw', 213: 'gx', 214: 'gy', 215: 'gz', 216: 'h.', 217: 'ha', 218: 'hb', 219: 'hc', 220: 'hd', 221: 'he', 222: 'hf', 223: 'hg', 224: 'hh', 225: 'hi', 226: 'hj', 227: 'hk', 228: 'hl', 229: 'hm', 230: 'hn', 231: 'ho', 232: 'hp', 233: 'hq', 234: 'hr', 235: 'hs', 236: 'ht', 237: 'hu', 238: 'hv', 239: 'hw', 240: 'hx', 241: 'hy', 242: 'hz', 243: 'i.', 244: 'ia', 245: 'ib', 246: 'ic', 247: 'id', 248: 'ie', 249: 'if', 250: 'ig', 251: 'ih', 252: 'ii', 253: 'ij', 254: 'ik', 255: 'il', 256: 'im', 257: 'in', 258: 'io', 259: 'ip', 260: 'iq', 261: 'ir', 262: 'is', 263: 'it', 264: 'iu', 265: 'iv', 266: 'iw', 267: 'ix', 268: 'iy', 269: 'iz', 270: 'j.', 271: 'ja', 272: 'jb', 273: 'jc', 274: 'jd', 275: 'je', 276: 'jf', 277: 'jg', 278: 'jh', 279: 'ji', 280: 'jj', 281: 'jk', 282: 'jl', 283: 'jm', 284: 'jn', 285: 'jo', 286: 'jp', 287: 'jq', 288: 'jr', 289: 'js', 290: 'jt', 291: 'ju', 292: 'jv', 293: 'jw', 294: 'jx', 295: 'jy', 296: 'jz', 297: 'k.', 298: 'ka', 299: 'kb', 300: 'kc', 301: 'kd', 302: 'ke', 303: 'kf', 304: 'kg', 305: 'kh', 306: 'ki', 307: 'kj', 308: 'kk', 309: 'kl', 310: 'km', 311: 'kn', 312: 'ko', 313: 'kp', 314: 'kq', 315: 'kr', 316: 'ks', 317: 'kt', 318: 'ku', 319: 'kv', 320: 'kw', 321: 'kx', 322: 'ky', 323: 'kz', 324: 'l.', 325: 'la', 326: 'lb', 327: 'lc', 328: 'ld', 329: 'le', 330: 'lf', 331: 'lg', 332: 'lh', 333: 'li', 334: 'lj', 335: 'lk', 336: 'll', 337: 'lm', 338: 'ln', 339: 'lo', 340: 'lp', 341: 'lq', 342: 'lr', 343: 'ls', 344: 'lt', 345: 'lu', 346: 'lv', 347: 'lw', 348: 'lx', 349: 'ly', 350: 'lz', 351: 'm.', 352: 'ma', 353: 'mb', 354: 'mc', 355: 'md', 356: 'me', 357: 'mf', 358: 'mg', 359: 'mh', 360: 'mi', 361: 'mj', 362: 'mk', 363: 'ml', 364: 'mm', 365: 'mn', 366: 'mo', 367: 'mp', 368: 'mq', 369: 'mr', 370: 'ms', 371: 'mt', 372: 'mu', 373: 'mv', 374: 'mw', 375: 'mx', 376: 'my', 377: 'mz', 378: 'n.', 379: 'na', 380: 'nb', 381: 'nc', 382: 'nd', 383: 'ne', 384: 'nf', 385: 'ng', 386: 'nh', 387: 'ni', 388: 'nj', 389: 'nk', 390: 'nl', 391: 'nm', 392: 'nn', 393: 'no', 394: 'np', 395: 'nq', 396: 'nr', 397: 'ns', 398: 'nt', 399: 'nu', 400: 'nv', 401: 'nw', 402: 'nx', 403: 'ny', 404: 'nz', 405: 'o.', 406: 'oa', 407: 'ob', 408: 'oc', 409: 'od', 410: 'oe', 411: 'of', 412: 'og', 413: 'oh', 414: 'oi', 415: 'oj', 416: 'ok', 417: 'ol', 418: 'om', 419: 'on', 420: 'oo', 421: 'op', 422: 'oq', 423: 'or', 424: 'os', 425: 'ot', 426: 'ou', 427: 'ov', 428: 'ow', 429: 'ox', 430: 'oy', 431: 'oz', 432: 'p.', 433: 'pa', 434: 'pb', 435: 'pc', 436: 'pd', 437: 'pe', 438: 'pf', 439: 'pg', 440: 'ph', 441: 'pi', 442: 'pj', 443: 'pk', 444: 'pl', 445: 'pm', 446: 'pn', 447: 'po', 448: 'pp', 449: 'pq', 450: 'pr', 451: 'ps', 452: 'pt', 453: 'pu', 454: 'pv', 455: 'pw', 456: 'px', 457: 'py', 458: 'pz', 459: 'q.', 460: 'qa', 461: 'qb', 462: 'qc', 463: 'qd', 464: 'qe', 465: 'qf', 466: 'qg', 467: 'qh', 468: 'qi', 469: 'qj', 470: 'qk', 471: 'ql', 472: 'qm', 473: 'qn', 474: 'qo', 475: 'qp', 476: 'qq', 477: 'qr', 478: 'qs', 479: 'qt', 480: 'qu', 481: 'qv', 482: 'qw', 483: 'qx', 484: 'qy', 485: 'qz', 486: 'r.', 487: 'ra', 488: 'rb', 489: 'rc', 490: 'rd', 491: 're', 492: 'rf', 493: 'rg', 494: 'rh', 495: 'ri', 496: 'rj', 497: 'rk', 498: 'rl', 499: 'rm', 500: 'rn', 501: 'ro', 502: 'rp', 503: 'rq', 504: 'rr', 505: 'rs', 506: 'rt', 507: 'ru', 508: 'rv', 509: 'rw', 510: 'rx', 511: 'ry', 512: 'rz', 513: 's.', 514: 'sa', 515: 'sb', 516: 'sc', 517: 'sd', 518: 'se', 519: 'sf', 520: 'sg', 521: 'sh', 522: 'si', 523: 'sj', 524: 'sk', 525: 'sl', 526: 'sm', 527: 'sn', 528: 'so', 529: 'sp', 530: 'sq', 531: 'sr', 532: 'ss', 533: 'st', 534: 'su', 535: 'sv', 536: 'sw', 537: 'sx', 538: 'sy', 539: 'sz', 540: 't.', 541: 'ta', 542: 'tb', 543: 'tc', 544: 'td', 545: 'te', 546: 'tf', 547: 'tg', 548: 'th', 549: 'ti', 550: 'tj', 551: 'tk', 552: 'tl', 553: 'tm', 554: 'tn', 555: 'to', 556: 'tp', 557: 'tq', 558: 'tr', 559: 'ts', 560: 'tt', 561: 'tu', 562: 'tv', 563: 'tw', 564: 'tx', 565: 'ty', 566: 'tz', 567: 'u.', 568: 'ua', 569: 'ub', 570: 'uc', 571: 'ud', 572: 'ue', 573: 'uf', 574: 'ug', 575: 'uh', 576: 'ui', 577: 'uj', 578: 'uk', 579: 'ul', 580: 'um', 581: 'un', 582: 'uo', 583: 'up', 584: 'uq', 585: 'ur', 586: 'us', 587: 'ut', 588: 'uu', 589: 'uv', 590: 'uw', 591: 'ux', 592: 'uy', 593: 'uz', 594: 'v.', 595: 'va', 596: 'vb', 597: 'vc', 598: 'vd', 599: 've', 600: 'vf', 601: 'vg', 602: 'vh', 603: 'vi', 604: 'vj', 605: 'vk', 606: 'vl', 607: 'vm', 608: 'vn', 609: 'vo', 610: 'vp', 611: 'vq', 612: 'vr', 613: 'vs', 614: 'vt', 615: 'vu', 616: 'vv', 617: 'vw', 618: 'vx', 619: 'vy', 620: 'vz', 621: 'w.', 622: 'wa', 623: 'wb', 624: 'wc', 625: 'wd', 626: 'we', 627: 'wf', 628: 'wg', 629: 'wh', 630: 'wi', 631: 'wj', 632: 'wk', 633: 'wl', 634: 'wm', 635: 'wn', 636: 'wo', 637: 'wp', 638: 'wq', 639: 'wr', 640: 'ws', 641: 'wt', 642: 'wu', 643: 'wv', 644: 'ww', 645: 'wx', 646: 'wy', 647: 'wz', 648: 'x.', 649: 'xa', 650: 'xb', 651: 'xc', 652: 'xd', 653: 'xe', 654: 'xf', 655: 'xg', 656: 'xh', 657: 'xi', 658: 'xj', 659: 'xk', 660: 'xl', 661: 'xm', 662: 'xn', 663: 'xo', 664: 'xp', 665: 'xq', 666: 'xr', 667: 'xs', 668: 'xt', 669: 'xu', 670: 'xv', 671: 'xw', 672: 'xx', 673: 'xy', 674: 'xz', 675: 'y.', 676: 'ya', 677: 'yb', 678: 'yc', 679: 'yd', 680: 'ye', 681: 'yf', 682: 'yg', 683: 'yh', 684: 'yi', 685: 'yj', 686: 'yk', 687: 'yl', 688: 'ym', 689: 'yn', 690: 'yo', 691: 'yp', 692: 'yq', 693: 'yr', 694: 'ys', 695: 'yt', 696: 'yu', 697: 'yv', 698: 'yw', 699: 'yx', 700: 'yy', 701: 'yz', 702: 'z.', 703: 'za', 704: 'zb', 705: 'zc', 706: 'zd', 707: 'ze', 708: 'zf', 709: 'zg', 710: 'zh', 711: 'zi', 712: 'zj', 713: 'zk', 714: 'zl', 715: 'zm', 716: 'zn', 717: 'zo', 718: 'zp', 719: 'zq', 720: 'zr', 721: 'zs', 722: 'zt', 723: 'zu', 724: 'zv', 725: 'zw', 726: 'zx', 727: 'zy', 728: 'zz'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COUNTING"
      ],
      "metadata": {
        "id": "x91bMI8-yZG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAIN THE TRI-GRAM LANGUAGE MODEL WITH USE OF THE MATRIX"
      ],
      "metadata": {
        "id": "Tcb5U_Za6bEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "kApZQSA0Hi4v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the count tensor\n",
        "count_tensor = torch.zeros(size=(len(bigrams), len(letters)), dtype=torch.int32)\n",
        "for name in names:\n",
        "  chars = ['.'] + ['.'] + list(name) + ['.']\n",
        "  for char1, char2, char3 in zip(chars, chars[1:], chars[2:]):\n",
        "    bigram = char1 + char2\n",
        "    bigram_index = bigrams_to_index[bigram]\n",
        "    char_index = letters_to_index[char3]\n",
        "    count_tensor[bigram_index, char_index] +=1\n",
        "\n",
        "\n",
        "# create the probability tensor, adding fake counts to smooth out the model\n",
        "prob_tensor = count_tensor.float() + 0.5\n",
        "prob_tensor /= torch.sum(prob_tensor, dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "# sample from the probability distribution\n",
        "# generator for the sampling from distribution\n",
        "arg_generator = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "# sample 200 names\n",
        "for _ in range(200):\n",
        "  sampled_char_list = []\n",
        "  sampled_bigram_idx = 0 # starting at ..\n",
        "\n",
        "  while True:\n",
        "    # first char of the next bigram\n",
        "    next_char1 = index_to_bigrams[sampled_bigram_idx][1]\n",
        "\n",
        "    # sampled the next char of the name with the given distribution of bigram\n",
        "    row = prob_tensor[sampled_bigram_idx]\n",
        "    next_char2_idx = torch.multinomial(row, num_samples=1,\n",
        "                                    replacement=True,\n",
        "                                    generator=arg_generator).item()\n",
        "    # second char of the next bigram (also next char of the name)\n",
        "    next_char2 = index_to_letters[next_char2_idx]\n",
        "\n",
        "    # the next bigram and index to get distribution of that bigram\n",
        "    next_bigram = next_char1 + next_char2\n",
        "    sampled_bigram_idx = bigrams_to_index[next_bigram]\n",
        "\n",
        "    if next_char2_idx != 0:\n",
        "      sampled_char_list.append(next_char2)\n",
        "    else:\n",
        "      # if next char is . (idx = 0), we can stop\n",
        "      break\n",
        "\n",
        "  sampled_name = \"\".join(sampled_char_list)\n",
        "  print(sampled_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SmDLzk9C6aVc",
        "outputId": "ccf904c2-0133-4cbd-eb83-8b1a61896656"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "junide\n",
            "jakasid\n",
            "prelay\n",
            "adin\n",
            "kairritoper\n",
            "sathen\n",
            "sameia\n",
            "yanileniassibduinewin\n",
            "lessiyanayla\n",
            "te\n",
            "farmanthyfortumj\n",
            "ponn\n",
            "lena\n",
            "jaylicore\n",
            "ya\n",
            "jocken\n",
            "jamilyn\n",
            "korin\n",
            "wyn\n",
            "ne\n",
            "gadsnhavi\n",
            "monselladdon\n",
            "mathani\n",
            "zie\n",
            "paun\n",
            "ty\n",
            "tin\n",
            "sreli\n",
            "ish\n",
            "dyn\n",
            "rumel\n",
            "jemah\n",
            "dawata\n",
            "kha\n",
            "cra\n",
            "raydnh\n",
            "adorta\n",
            "malyn\n",
            "brey\n",
            "aur\n",
            "lavarocbzthemiraya\n",
            "ath\n",
            "basely\n",
            "tavisonikiyaalee\n",
            "marlen\n",
            "em\n",
            "fabethellianten\n",
            "chan\n",
            "jazaody\n",
            "drd\n",
            "johialiypvrgia\n",
            "tezra\n",
            "elia\n",
            "vywhqelvani\n",
            "sahimah\n",
            "kellette\n",
            "braceodon\n",
            "ali\n",
            "alian\n",
            "denn\n",
            "jayannyah\n",
            "kennelynkmarianner\n",
            "samotan\n",
            "kyroderihana\n",
            "shday\n",
            "ta\n",
            "olleah\n",
            "terty\n",
            "breus\n",
            "dasia\n",
            "na\n",
            "chlynevini\n",
            "aspi\n",
            "katalilondral\n",
            "fanmari\n",
            "mishama\n",
            "verykongeon\n",
            "resynivion\n",
            "uzien\n",
            "jalivyah\n",
            "alto\n",
            "marafelanaylian\n",
            "ohanorrisyli\n",
            "mson\n",
            "jorayitalkalviyaneria\n",
            "maulil\n",
            "cacvatavryaaleximrla\n",
            "sa\n",
            "pre\n",
            "azaikeri\n",
            "khanna\n",
            "abbiha\n",
            "isa\n",
            "brailyorv\n",
            "bradeelyn\n",
            "zyarierri\n",
            "chamadayda\n",
            "fbalyah\n",
            "meyton\n",
            "za\n",
            "sabdin\n",
            "han\n",
            "kennasslen\n",
            "conik\n",
            "ny\n",
            "naaston\n",
            "tisona\n",
            "isemran\n",
            "wwlixie\n",
            "kadleekam\n",
            "ljaykine\n",
            "findesiyah\n",
            "lula\n",
            "jus\n",
            "b\n",
            "den\n",
            "hawyqsemilair\n",
            "hasiam\n",
            "na\n",
            "adiamordelamiquadilah\n",
            "an\n",
            "keika\n",
            "viyah\n",
            "ax\n",
            "mandt\n",
            "tyjamanalingh\n",
            "jepekala\n",
            "koberishurays\n",
            "khin\n",
            "haud\n",
            "wesinna\n",
            "ellinileann\n",
            "madiah\n",
            "pashlianancs\n",
            "cobzmiandoub\n",
            "myah\n",
            "kaws\n",
            "eva\n",
            "rie\n",
            "laydoni\n",
            "dasiradinie\n",
            "cora\n",
            "aum\n",
            "mana\n",
            "dever\n",
            "shen\n",
            "jo\n",
            "esten\n",
            "die\n",
            "alaciordeliskyann\n",
            "ala\n",
            "der\n",
            "hambenda\n",
            "dale\n",
            "aansharalarynehmiya\n",
            "leel\n",
            "trael\n",
            "ta\n",
            "ix\n",
            "na\n",
            "ek\n",
            "maryena\n",
            "kemerytle\n",
            "kacqueelqeven\n",
            "caliavan\n",
            "ann\n",
            "inyohasyn\n",
            "frey\n",
            "an\n",
            "ya\n",
            "laydena\n",
            "bronidavicopayn\n",
            "jila\n",
            "evkyreigen\n",
            "amaylee\n",
            "tala\n",
            "laton\n",
            "gialiah\n",
            "joyarisen\n",
            "kamela\n",
            "la\n",
            "shoselynna\n",
            "ib\n",
            "sundrenn\n",
            "qai\n",
            "yuvikammarmileyah\n",
            "anus\n",
            "redon\n",
            "kilapizzavarsevitlyonni\n",
            "hanady\n",
            "wojash\n",
            "iksh\n",
            "deiantonnah\n",
            "zylah\n",
            "trayn\n",
            "ha\n",
            "ron\n",
            "alayly\n",
            "geliekhtaerri\n",
            "eloveriel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the loss function (average negative log likelihood) of this model\n",
        "log_likelihood = 0.0\n",
        "count = 0\n",
        "for name in names:\n",
        "  chars = ['.'] + ['.'] + list(name) + ['.']\n",
        "  for char1, char2, char3 in zip(chars, chars[1:], chars[2:]):\n",
        "    bigram = char1 + char2\n",
        "    bigram_index = bigrams_to_index[bigram]\n",
        "    letter_index = letters_to_index[char3]\n",
        "    prob = prob_tensor[bigram_index, letter_index]\n",
        "    # this one takes the logarithm of the tensor\n",
        "    log_likelihood += torch.log(prob)\n",
        "    count += 1\n",
        "\n",
        "loss = abs(log_likelihood) / count\n",
        "print(f\"Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzvsRvG7SoVs",
        "outputId": "97d554c1-47bf-43b0-f457-cfc5eeb2dce6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.200744867324829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The loss of the trigram model (as measured by the average negative log likehood) is less than that of the bigram model."
      ],
      "metadata": {
        "id": "57FjeXYfq86_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEURAL NETWORK"
      ],
      "metadata": {
        "id": "1qceyNKryTEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAIN THE TRIGRAM LANGUAGE MODEL"
      ],
      "metadata": {
        "id": "TFn-3QhgQvdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.functional import one_hot, cross_entropy"
      ],
      "metadata": {
        "id": "l5hNGw1EUOXl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# traning set of trigram\n",
        "inputs, targets = [], []\n",
        "count = 0\n",
        "\n",
        "for name in names:\n",
        "  chars = ['.'] + ['.'] + list(name) + ['.']\n",
        "  for char1, char2, char3 in zip(chars, chars[1:], chars[2:]):\n",
        "    bigram = char1 + char2\n",
        "    index1 = bigrams_to_index[bigram]\n",
        "    index2 = letters_to_i[char3]\n",
        "    inputs.append(index1) # the bigram at this turn\n",
        "    targets.append(index2) # the corresponding char after it\n",
        "    count += 1\n",
        "\n",
        "# turn the list into the tensor\n",
        "inputs = torch.tensor(inputs)\n",
        "targets = torch.tensor(targets)\n",
        "\n",
        "print(inputs.shape)\n",
        "print(targets.shape)\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq5ou0UMQ0rN",
        "outputId": "47353573-e2a9-4ee5-bd26-5c4fc2b2e62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([228146])\n",
            "torch.Size([228146])\n",
            "228146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arg_generator = torch.Generator().manual_seed(2147483647)\n",
        "weights = torch.randn((729, 27), generator=arg_generator, requires_grad=True)\n",
        "logits_count = weights[inputs]\n",
        "logits_count.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fml24sMZ0be",
        "outputId": "c604b5a4-7908-44be-e8ba-c63ca07852e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([228146, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(arg_inputs, arg_targets, arg_dev_inputs=None, arg_dev_targets=None):\n",
        "  # intialize the weights for the neural net,\n",
        "  # 1 layer (is dumb), 27 neurons (columns), each neuron 729 weights (rows)\n",
        "  arg_generator = torch.Generator().manual_seed(2147483647)\n",
        "  weights = torch.randn((729, 27), generator=arg_generator, requires_grad=True)\n",
        "\n",
        "  for i in range(1000):\n",
        "    # forward pass\n",
        "    # calculate the softmax\n",
        "    # index directly to the rows\n",
        "    logits_count = weights[arg_inputs]\n",
        "    # softmax the logits\n",
        "    # count_tensor = torch.exp(logits_count)\n",
        "    # prob_tensor = count_tensor / torch.sum(count_tensor, dim=1, keepdim=True)\n",
        "\n",
        "    # calculate the loss (negative log likelihood)\n",
        "    # loss = abs(prob_tensor[torch.arange(prob_tensor.shape[0]), arg_targets].log().mean())\n",
        "\n",
        "    # using cross entropy\n",
        "    loss = cross_entropy(logits_count, targets)\n",
        "    print(f\"Train loss {i+1}/1000: {loss.item()}\")\n",
        "\n",
        "    # calculate the dev loss (if any)\n",
        "    if arg_dev_inputs != None:\n",
        "      dev_logits_count = weights[arg_dev_inputs]\n",
        "      # dev_count_tensor = torch.exp(dev_logits_count)\n",
        "      # dev_prob_tensor = dev_count_tensor / torch.sum(dev_count_tensor, dim=1, keepdim=True)\n",
        "\n",
        "      # calculate the loss (using negative log likelihood)\n",
        "      # dev_loss = abs(dev_prob_tensor[torch.arange(dev_prob_tensor.shape[0]), arg_dev_targets].log().mean())\n",
        "\n",
        "      # using cross entropy\n",
        "      dev_loss = cross_entropy(dev_inputs, targets)\n",
        "      print(f\"Devel loss {i+1}/1000: {dev_loss.item()}\")\n",
        "      print()\n",
        "\n",
        "    # backward pass (backprogagation comes into place)\n",
        "    weights.grad = None # set to zero gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # update the parameters of the weights\n",
        "    weights.data += -100 * weights.grad\n",
        "\n",
        "  # return the parameters of the model\n",
        "  return weights"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Cq-ZSsEasyzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_names(arg_weights):\n",
        "  # sample 200 names from the probability distribution\n",
        "  arg_generator = torch.Generator().manual_seed(2147483647)\n",
        "  for _ in range(200):\n",
        "    sampled_char_list = []\n",
        "    sampled_idx = 0 # starting at ..\n",
        "\n",
        "    while True:\n",
        "      # first char of the next bigram\n",
        "      next_char1 = index_to_bigrams[sampled_idx][1]\n",
        "\n",
        "      # sampled the next char using forward pass of the neural net\n",
        "      logits_count = arg_weights[sampled_idx]\n",
        "      count_tensor = torch.exp(logits_count)\n",
        "      prob = count_tensor / torch.sum(count_tensor, dim=0)\n",
        "      next_char2_idx = torch.multinomial(prob, num_samples=1,\n",
        "                                      replacement=True,\n",
        "                                      generator=arg_generator).item()\n",
        "\n",
        "      # second char of the next bigram (also next char of the name)\n",
        "      next_char2 = i_to_letters[next_char2_idx]\n",
        "\n",
        "      # the next bigram and index to get distribution of that bigram\n",
        "      next_bigram = next_char1 + next_char2\n",
        "      sampled_idx = bigrams_to_index[next_bigram]\n",
        "\n",
        "      if next_char2_idx != 0:\n",
        "        # if next char isn't ., add them to the name\n",
        "        sampled_char_list.append(next_char2)\n",
        "      else:\n",
        "        # if next char is . (idx = 0), we can stop\n",
        "        break\n",
        "\n",
        "    sampled_name = \"\".join(sampled_char_list)\n",
        "    print(sampled_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cECPgDlxuEPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model using cross entropy loss\n",
        "model_weights = train_model(inputs, targets)\n",
        "sample_names(model_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D-oQgY6QasOw",
        "outputId": "f84232b7-a572-4a88-c792-ac7d1cd32560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 1/1000: 3.792776346206665\n",
            "Train loss 2/1000: 3.53816819190979\n",
            "Train loss 3/1000: 3.417757511138916\n",
            "Train loss 4/1000: 3.3257200717926025\n",
            "Train loss 5/1000: 3.2489399909973145\n",
            "Train loss 6/1000: 3.1840078830718994\n",
            "Train loss 7/1000: 3.1282405853271484\n",
            "Train loss 8/1000: 3.0796456336975098\n",
            "Train loss 9/1000: 3.036648750305176\n",
            "Train loss 10/1000: 2.998178005218506\n",
            "Train loss 11/1000: 2.963472366333008\n",
            "Train loss 12/1000: 2.931981086730957\n",
            "Train loss 13/1000: 2.9032771587371826\n",
            "Train loss 14/1000: 2.877018451690674\n",
            "Train loss 15/1000: 2.8529207706451416\n",
            "Train loss 16/1000: 2.8307430744171143\n",
            "Train loss 17/1000: 2.810277223587036\n",
            "Train loss 18/1000: 2.79133939743042\n",
            "Train loss 19/1000: 2.7737646102905273\n",
            "Train loss 20/1000: 2.7574081420898438\n",
            "Train loss 21/1000: 2.7421391010284424\n",
            "Train loss 22/1000: 2.7278409004211426\n",
            "Train loss 23/1000: 2.7144131660461426\n",
            "Train loss 24/1000: 2.7017662525177\n",
            "Train loss 25/1000: 2.689824342727661\n",
            "Train loss 26/1000: 2.6785190105438232\n",
            "Train loss 27/1000: 2.667792797088623\n",
            "Train loss 28/1000: 2.6575944423675537\n",
            "Train loss 29/1000: 2.647880792617798\n",
            "Train loss 30/1000: 2.638612747192383\n",
            "Train loss 31/1000: 2.6297545433044434\n",
            "Train loss 32/1000: 2.621277093887329\n",
            "Train loss 33/1000: 2.613152265548706\n",
            "Train loss 34/1000: 2.6053566932678223\n",
            "Train loss 35/1000: 2.597867488861084\n",
            "Train loss 36/1000: 2.590665817260742\n",
            "Train loss 37/1000: 2.583732843399048\n",
            "Train loss 38/1000: 2.577052593231201\n",
            "Train loss 39/1000: 2.570610523223877\n",
            "Train loss 40/1000: 2.5643932819366455\n",
            "Train loss 41/1000: 2.5583877563476562\n",
            "Train loss 42/1000: 2.5525832176208496\n",
            "Train loss 43/1000: 2.546969175338745\n",
            "Train loss 44/1000: 2.541534900665283\n",
            "Train loss 45/1000: 2.5362725257873535\n",
            "Train loss 46/1000: 2.5311732292175293\n",
            "Train loss 47/1000: 2.5262293815612793\n",
            "Train loss 48/1000: 2.5214338302612305\n",
            "Train loss 49/1000: 2.5167791843414307\n",
            "Train loss 50/1000: 2.5122599601745605\n",
            "Train loss 51/1000: 2.5078697204589844\n",
            "Train loss 52/1000: 2.503602981567383\n",
            "Train loss 53/1000: 2.4994540214538574\n",
            "Train loss 54/1000: 2.4954187870025635\n",
            "Train loss 55/1000: 2.4914917945861816\n",
            "Train loss 56/1000: 2.487668991088867\n",
            "Train loss 57/1000: 2.4839460849761963\n",
            "Train loss 58/1000: 2.4803199768066406\n",
            "Train loss 59/1000: 2.476785659790039\n",
            "Train loss 60/1000: 2.4733402729034424\n",
            "Train loss 61/1000: 2.469980478286743\n",
            "Train loss 62/1000: 2.466702699661255\n",
            "Train loss 63/1000: 2.4635047912597656\n",
            "Train loss 64/1000: 2.460383415222168\n",
            "Train loss 65/1000: 2.4573352336883545\n",
            "Train loss 66/1000: 2.4543588161468506\n",
            "Train loss 67/1000: 2.4514505863189697\n",
            "Train loss 68/1000: 2.4486091136932373\n",
            "Train loss 69/1000: 2.445831537246704\n",
            "Train loss 70/1000: 2.4431159496307373\n",
            "Train loss 71/1000: 2.4404609203338623\n",
            "Train loss 72/1000: 2.4378628730773926\n",
            "Train loss 73/1000: 2.435321569442749\n",
            "Train loss 74/1000: 2.4328343868255615\n",
            "Train loss 75/1000: 2.4303998947143555\n",
            "Train loss 76/1000: 2.42801570892334\n",
            "Train loss 77/1000: 2.4256815910339355\n",
            "Train loss 78/1000: 2.4233949184417725\n",
            "Train loss 79/1000: 2.421154737472534\n",
            "Train loss 80/1000: 2.418959140777588\n",
            "Train loss 81/1000: 2.4168076515197754\n",
            "Train loss 82/1000: 2.4146976470947266\n",
            "Train loss 83/1000: 2.4126291275024414\n",
            "Train loss 84/1000: 2.410600185394287\n",
            "Train loss 85/1000: 2.4086098670959473\n",
            "Train loss 86/1000: 2.4066569805145264\n",
            "Train loss 87/1000: 2.404740810394287\n",
            "Train loss 88/1000: 2.402859687805176\n",
            "Train loss 89/1000: 2.401012659072876\n",
            "Train loss 90/1000: 2.3991987705230713\n",
            "Train loss 91/1000: 2.397418260574341\n",
            "Train loss 92/1000: 2.3956687450408936\n",
            "Train loss 93/1000: 2.3939497470855713\n",
            "Train loss 94/1000: 2.3922605514526367\n",
            "Train loss 95/1000: 2.3906004428863525\n",
            "Train loss 96/1000: 2.3889684677124023\n",
            "Train loss 97/1000: 2.387364149093628\n",
            "Train loss 98/1000: 2.3857860565185547\n",
            "Train loss 99/1000: 2.3842344284057617\n",
            "Train loss 100/1000: 2.3827080726623535\n",
            "Train loss 101/1000: 2.3812062740325928\n",
            "Train loss 102/1000: 2.379728317260742\n",
            "Train loss 103/1000: 2.3782742023468018\n",
            "Train loss 104/1000: 2.376842737197876\n",
            "Train loss 105/1000: 2.3754336833953857\n",
            "Train loss 106/1000: 2.3740460872650146\n",
            "Train loss 107/1000: 2.3726799488067627\n",
            "Train loss 108/1000: 2.3713345527648926\n",
            "Train loss 109/1000: 2.370008945465088\n",
            "Train loss 110/1000: 2.3687033653259277\n",
            "Train loss 111/1000: 2.3674168586730957\n",
            "Train loss 112/1000: 2.366149425506592\n",
            "Train loss 113/1000: 2.3649001121520996\n",
            "Train loss 114/1000: 2.36366868019104\n",
            "Train loss 115/1000: 2.362455129623413\n",
            "Train loss 116/1000: 2.3612585067749023\n",
            "Train loss 117/1000: 2.3600785732269287\n",
            "Train loss 118/1000: 2.358915090560913\n",
            "Train loss 119/1000: 2.3577678203582764\n",
            "Train loss 120/1000: 2.3566360473632812\n",
            "Train loss 121/1000: 2.3555197715759277\n",
            "Train loss 122/1000: 2.3544185161590576\n",
            "Train loss 123/1000: 2.353332042694092\n",
            "Train loss 124/1000: 2.352259874343872\n",
            "Train loss 125/1000: 2.3512020111083984\n",
            "Train loss 126/1000: 2.3501577377319336\n",
            "Train loss 127/1000: 2.3491270542144775\n",
            "Train loss 128/1000: 2.3481101989746094\n",
            "Train loss 129/1000: 2.3471055030822754\n",
            "Train loss 130/1000: 2.346114158630371\n",
            "Train loss 131/1000: 2.345134973526001\n",
            "Train loss 132/1000: 2.3441684246063232\n",
            "Train loss 133/1000: 2.3432137966156006\n",
            "Train loss 134/1000: 2.342271327972412\n",
            "Train loss 135/1000: 2.3413400650024414\n",
            "Train loss 136/1000: 2.3404202461242676\n",
            "Train loss 137/1000: 2.3395113945007324\n",
            "Train loss 138/1000: 2.338613748550415\n",
            "Train loss 139/1000: 2.3377270698547363\n",
            "Train loss 140/1000: 2.336850643157959\n",
            "Train loss 141/1000: 2.335984945297241\n",
            "Train loss 142/1000: 2.3351292610168457\n",
            "Train loss 143/1000: 2.3342835903167725\n",
            "Train loss 144/1000: 2.3334474563598633\n",
            "Train loss 145/1000: 2.3326210975646973\n",
            "Train loss 146/1000: 2.3318045139312744\n",
            "Train loss 147/1000: 2.3309972286224365\n",
            "Train loss 148/1000: 2.3301992416381836\n",
            "Train loss 149/1000: 2.3294098377227783\n",
            "Train loss 150/1000: 2.328629732131958\n",
            "Train loss 151/1000: 2.3278582096099854\n",
            "Train loss 152/1000: 2.3270952701568604\n",
            "Train loss 153/1000: 2.326340675354004\n",
            "Train loss 154/1000: 2.325594902038574\n",
            "Train loss 155/1000: 2.3248562812805176\n",
            "Train loss 156/1000: 2.324126958847046\n",
            "Train loss 157/1000: 2.323404550552368\n",
            "Train loss 158/1000: 2.32269024848938\n",
            "Train loss 159/1000: 2.321983575820923\n",
            "Train loss 160/1000: 2.321284294128418\n",
            "Train loss 161/1000: 2.3205926418304443\n",
            "Train loss 162/1000: 2.319908380508423\n",
            "Train loss 163/1000: 2.3192312717437744\n",
            "Train loss 164/1000: 2.318560838699341\n",
            "Train loss 165/1000: 2.3178977966308594\n",
            "Train loss 166/1000: 2.3172411918640137\n",
            "Train loss 167/1000: 2.316591739654541\n",
            "Train loss 168/1000: 2.315948724746704\n",
            "Train loss 169/1000: 2.315312385559082\n",
            "Train loss 170/1000: 2.3146822452545166\n",
            "Train loss 171/1000: 2.314058542251587\n",
            "Train loss 172/1000: 2.313441276550293\n",
            "Train loss 173/1000: 2.3128302097320557\n",
            "Train loss 174/1000: 2.312224864959717\n",
            "Train loss 175/1000: 2.3116257190704346\n",
            "Train loss 176/1000: 2.31103253364563\n",
            "Train loss 177/1000: 2.3104450702667236\n",
            "Train loss 178/1000: 2.309863328933716\n",
            "Train loss 179/1000: 2.3092873096466064\n",
            "Train loss 180/1000: 2.3087167739868164\n",
            "Train loss 181/1000: 2.308151960372925\n",
            "Train loss 182/1000: 2.3075921535491943\n",
            "Train loss 183/1000: 2.3070380687713623\n",
            "Train loss 184/1000: 2.3064887523651123\n",
            "Train loss 185/1000: 2.3059449195861816\n",
            "Train loss 186/1000: 2.305406093597412\n",
            "Train loss 187/1000: 2.304872512817383\n",
            "Train loss 188/1000: 2.3043434619903564\n",
            "Train loss 189/1000: 2.3038196563720703\n",
            "Train loss 190/1000: 2.303300619125366\n",
            "Train loss 191/1000: 2.302786111831665\n",
            "Train loss 192/1000: 2.302276611328125\n",
            "Train loss 193/1000: 2.301771402359009\n",
            "Train loss 194/1000: 2.3012712001800537\n",
            "Train loss 195/1000: 2.3007750511169434\n",
            "Train loss 196/1000: 2.300283670425415\n",
            "Train loss 197/1000: 2.2997965812683105\n",
            "Train loss 198/1000: 2.299314022064209\n",
            "Train loss 199/1000: 2.298835277557373\n",
            "Train loss 200/1000: 2.29836106300354\n",
            "Train loss 201/1000: 2.2978909015655518\n",
            "Train loss 202/1000: 2.297424793243408\n",
            "Train loss 203/1000: 2.2969627380371094\n",
            "Train loss 204/1000: 2.2965047359466553\n",
            "Train loss 205/1000: 2.296050548553467\n",
            "Train loss 206/1000: 2.295600652694702\n",
            "Train loss 207/1000: 2.295154333114624\n",
            "Train loss 208/1000: 2.2947118282318115\n",
            "Train loss 209/1000: 2.2942731380462646\n",
            "Train loss 210/1000: 2.2938380241394043\n",
            "Train loss 211/1000: 2.2934062480926514\n",
            "Train loss 212/1000: 2.2929787635803223\n",
            "Train loss 213/1000: 2.2925546169281006\n",
            "Train loss 214/1000: 2.2921335697174072\n",
            "Train loss 215/1000: 2.2917165756225586\n",
            "Train loss 216/1000: 2.2913029193878174\n",
            "Train loss 217/1000: 2.2908923625946045\n",
            "Train loss 218/1000: 2.290485382080078\n",
            "Train loss 219/1000: 2.290081739425659\n",
            "Train loss 220/1000: 2.2896811962127686\n",
            "Train loss 221/1000: 2.2892839908599854\n",
            "Train loss 222/1000: 2.2888898849487305\n",
            "Train loss 223/1000: 2.288499116897583\n",
            "Train loss 224/1000: 2.2881112098693848\n",
            "Train loss 225/1000: 2.287726640701294\n",
            "Train loss 226/1000: 2.2873446941375732\n",
            "Train loss 227/1000: 2.28696608543396\n",
            "Train loss 228/1000: 2.286590576171875\n",
            "Train loss 229/1000: 2.2862179279327393\n",
            "Train loss 230/1000: 2.2858479022979736\n",
            "Train loss 231/1000: 2.2854807376861572\n",
            "Train loss 232/1000: 2.285116672515869\n",
            "Train loss 233/1000: 2.2847554683685303\n",
            "Train loss 234/1000: 2.2843968868255615\n",
            "Train loss 235/1000: 2.284040927886963\n",
            "Train loss 236/1000: 2.2836878299713135\n",
            "Train loss 237/1000: 2.283337116241455\n",
            "Train loss 238/1000: 2.282989501953125\n",
            "Train loss 239/1000: 2.282644271850586\n",
            "Train loss 240/1000: 2.282301902770996\n",
            "Train loss 241/1000: 2.281961679458618\n",
            "Train loss 242/1000: 2.2816240787506104\n",
            "Train loss 243/1000: 2.2812888622283936\n",
            "Train loss 244/1000: 2.280956268310547\n",
            "Train loss 245/1000: 2.280626058578491\n",
            "Train loss 246/1000: 2.2802982330322266\n",
            "Train loss 247/1000: 2.279973030090332\n",
            "Train loss 248/1000: 2.2796499729156494\n",
            "Train loss 249/1000: 2.279329538345337\n",
            "Train loss 250/1000: 2.2790112495422363\n",
            "Train loss 251/1000: 2.2786951065063477\n",
            "Train loss 252/1000: 2.278381109237671\n",
            "Train loss 253/1000: 2.2780699729919434\n",
            "Train loss 254/1000: 2.2777605056762695\n",
            "Train loss 255/1000: 2.2774531841278076\n",
            "Train loss 256/1000: 2.277148485183716\n",
            "Train loss 257/1000: 2.2768454551696777\n",
            "Train loss 258/1000: 2.2765450477600098\n",
            "Train loss 259/1000: 2.2762465476989746\n",
            "Train loss 260/1000: 2.275949716567993\n",
            "Train loss 261/1000: 2.2756552696228027\n",
            "Train loss 262/1000: 2.275362968444824\n",
            "Train loss 263/1000: 2.2750725746154785\n",
            "Train loss 264/1000: 2.2747840881347656\n",
            "Train loss 265/1000: 2.2744977474212646\n",
            "Train loss 266/1000: 2.2742133140563965\n",
            "Train loss 267/1000: 2.273930788040161\n",
            "Train loss 268/1000: 2.2736499309539795\n",
            "Train loss 269/1000: 2.273371458053589\n",
            "Train loss 270/1000: 2.273094415664673\n",
            "Train loss 271/1000: 2.2728192806243896\n",
            "Train loss 272/1000: 2.2725460529327393\n",
            "Train loss 273/1000: 2.2722747325897217\n",
            "Train loss 274/1000: 2.272005319595337\n",
            "Train loss 275/1000: 2.2717373371124268\n",
            "Train loss 276/1000: 2.2714712619781494\n",
            "Train loss 277/1000: 2.271207332611084\n",
            "Train loss 278/1000: 2.270944356918335\n",
            "Train loss 279/1000: 2.270683526992798\n",
            "Train loss 280/1000: 2.2704243659973145\n",
            "Train loss 281/1000: 2.270167112350464\n",
            "Train loss 282/1000: 2.269911289215088\n",
            "Train loss 283/1000: 2.2696568965911865\n",
            "Train loss 284/1000: 2.269404411315918\n",
            "Train loss 285/1000: 2.2691538333892822\n",
            "Train loss 286/1000: 2.268904209136963\n",
            "Train loss 287/1000: 2.2686562538146973\n",
            "Train loss 288/1000: 2.2684102058410645\n",
            "Train loss 289/1000: 2.268165349960327\n",
            "Train loss 290/1000: 2.2679226398468018\n",
            "Train loss 291/1000: 2.2676808834075928\n",
            "Train loss 292/1000: 2.2674407958984375\n",
            "Train loss 293/1000: 2.267202138900757\n",
            "Train loss 294/1000: 2.266965389251709\n",
            "Train loss 295/1000: 2.2667295932769775\n",
            "Train loss 296/1000: 2.2664952278137207\n",
            "Train loss 297/1000: 2.2662625312805176\n",
            "Train loss 298/1000: 2.266031265258789\n",
            "Train loss 299/1000: 2.265801191329956\n",
            "Train loss 300/1000: 2.265573024749756\n",
            "Train loss 301/1000: 2.265345573425293\n",
            "Train loss 302/1000: 2.265119791030884\n",
            "Train loss 303/1000: 2.26489520072937\n",
            "Train loss 304/1000: 2.26467227935791\n",
            "Train loss 305/1000: 2.264450788497925\n",
            "Train loss 306/1000: 2.264230489730835\n",
            "Train loss 307/1000: 2.2640111446380615\n",
            "Train loss 308/1000: 2.263793468475342\n",
            "Train loss 309/1000: 2.2635772228240967\n",
            "Train loss 310/1000: 2.263361692428589\n",
            "Train loss 311/1000: 2.2631475925445557\n",
            "Train loss 312/1000: 2.262935161590576\n",
            "Train loss 313/1000: 2.262723445892334\n",
            "Train loss 314/1000: 2.2625133991241455\n",
            "Train loss 315/1000: 2.2623043060302734\n",
            "Train loss 316/1000: 2.262096405029297\n",
            "Train loss 317/1000: 2.261889934539795\n",
            "Train loss 318/1000: 2.2616844177246094\n",
            "Train loss 319/1000: 2.2614798545837402\n",
            "Train loss 320/1000: 2.261276960372925\n",
            "Train loss 321/1000: 2.261075019836426\n",
            "Train loss 322/1000: 2.2608745098114014\n",
            "Train loss 323/1000: 2.2606747150421143\n",
            "Train loss 324/1000: 2.2604763507843018\n",
            "Train loss 325/1000: 2.2602787017822266\n",
            "Train loss 326/1000: 2.260082483291626\n",
            "Train loss 327/1000: 2.259887218475342\n",
            "Train loss 328/1000: 2.259693145751953\n",
            "Train loss 329/1000: 2.259500026702881\n",
            "Train loss 330/1000: 2.259308099746704\n",
            "Train loss 331/1000: 2.2591171264648438\n",
            "Train loss 332/1000: 2.258927345275879\n",
            "Train loss 333/1000: 2.2587385177612305\n",
            "Train loss 334/1000: 2.2585506439208984\n",
            "Train loss 335/1000: 2.258364200592041\n",
            "Train loss 336/1000: 2.258178472518921\n",
            "Train loss 337/1000: 2.257993698120117\n",
            "Train loss 338/1000: 2.25780987739563\n",
            "Train loss 339/1000: 2.257627010345459\n",
            "Train loss 340/1000: 2.2574450969696045\n",
            "Train loss 341/1000: 2.2572646141052246\n",
            "Train loss 342/1000: 2.257084608078003\n",
            "Train loss 343/1000: 2.256906032562256\n",
            "Train loss 344/1000: 2.256727933883667\n",
            "Train loss 345/1000: 2.2565507888793945\n",
            "Train loss 346/1000: 2.2563750743865967\n",
            "Train loss 347/1000: 2.256200075149536\n",
            "Train loss 348/1000: 2.256025791168213\n",
            "Train loss 349/1000: 2.255852460861206\n",
            "Train loss 350/1000: 2.2556800842285156\n",
            "Train loss 351/1000: 2.2555086612701416\n",
            "Train loss 352/1000: 2.255338191986084\n",
            "Train loss 353/1000: 2.2551684379577637\n",
            "Train loss 354/1000: 2.2549996376037598\n",
            "Train loss 355/1000: 2.2548317909240723\n",
            "Train loss 356/1000: 2.254664659500122\n",
            "Train loss 357/1000: 2.254498243331909\n",
            "Train loss 358/1000: 2.254333257675171\n",
            "Train loss 359/1000: 2.2541685104370117\n",
            "Train loss 360/1000: 2.254004955291748\n",
            "Train loss 361/1000: 2.253842353820801\n",
            "Train loss 362/1000: 2.2536799907684326\n",
            "Train loss 363/1000: 2.253519058227539\n",
            "Train loss 364/1000: 2.2533583641052246\n",
            "Train loss 365/1000: 2.2531991004943848\n",
            "Train loss 366/1000: 2.253040075302124\n",
            "Train loss 367/1000: 2.252882242202759\n",
            "Train loss 368/1000: 2.2527248859405518\n",
            "Train loss 369/1000: 2.252568244934082\n",
            "Train loss 370/1000: 2.252412796020508\n",
            "Train loss 371/1000: 2.252257823944092\n",
            "Train loss 372/1000: 2.252103805541992\n",
            "Train loss 373/1000: 2.25195050239563\n",
            "Train loss 374/1000: 2.251797914505005\n",
            "Train loss 375/1000: 2.251645803451538\n",
            "Train loss 376/1000: 2.251494884490967\n",
            "Train loss 377/1000: 2.2513444423675537\n",
            "Train loss 378/1000: 2.251194715499878\n",
            "Train loss 379/1000: 2.2510457038879395\n",
            "Train loss 380/1000: 2.2508976459503174\n",
            "Train loss 381/1000: 2.2507500648498535\n",
            "Train loss 382/1000: 2.250603437423706\n",
            "Train loss 383/1000: 2.250457286834717\n",
            "Train loss 384/1000: 2.2503116130828857\n",
            "Train loss 385/1000: 2.25016713142395\n",
            "Train loss 386/1000: 2.250023126602173\n",
            "Train loss 387/1000: 2.2498795986175537\n",
            "Train loss 388/1000: 2.249737024307251\n",
            "Train loss 389/1000: 2.2495949268341064\n",
            "Train loss 390/1000: 2.2494540214538574\n",
            "Train loss 391/1000: 2.2493131160736084\n",
            "Train loss 392/1000: 2.249173164367676\n",
            "Train loss 393/1000: 2.2490339279174805\n",
            "Train loss 394/1000: 2.2488951683044434\n",
            "Train loss 395/1000: 2.2487571239471436\n",
            "Train loss 396/1000: 2.248619794845581\n",
            "Train loss 397/1000: 2.2484829425811768\n",
            "Train loss 398/1000: 2.2483468055725098\n",
            "Train loss 399/1000: 2.248211145401001\n",
            "Train loss 400/1000: 2.2480764389038086\n",
            "Train loss 401/1000: 2.2479422092437744\n",
            "Train loss 402/1000: 2.2478082180023193\n",
            "Train loss 403/1000: 2.2476754188537598\n",
            "Train loss 404/1000: 2.2475430965423584\n",
            "Train loss 405/1000: 2.247411012649536\n",
            "Train loss 406/1000: 2.2472798824310303\n",
            "Train loss 407/1000: 2.2471492290496826\n",
            "Train loss 408/1000: 2.2470192909240723\n",
            "Train loss 409/1000: 2.246889352798462\n",
            "Train loss 410/1000: 2.246760606765747\n",
            "Train loss 411/1000: 2.2466323375701904\n",
            "Train loss 412/1000: 2.246504545211792\n",
            "Train loss 413/1000: 2.246377468109131\n",
            "Train loss 414/1000: 2.246250629425049\n",
            "Train loss 415/1000: 2.246124744415283\n",
            "Train loss 416/1000: 2.2459990978240967\n",
            "Train loss 417/1000: 2.2458744049072266\n",
            "Train loss 418/1000: 2.2457494735717773\n",
            "Train loss 419/1000: 2.245626211166382\n",
            "Train loss 420/1000: 2.245502471923828\n",
            "Train loss 421/1000: 2.2453794479370117\n",
            "Train loss 422/1000: 2.245257616043091\n",
            "Train loss 423/1000: 2.245136022567749\n",
            "Train loss 424/1000: 2.2450146675109863\n",
            "Train loss 425/1000: 2.244894027709961\n",
            "Train loss 426/1000: 2.2447738647460938\n",
            "Train loss 427/1000: 2.2446539402008057\n",
            "Train loss 428/1000: 2.244535207748413\n",
            "Train loss 429/1000: 2.2444164752960205\n",
            "Train loss 430/1000: 2.2442984580993652\n",
            "Train loss 431/1000: 2.244180679321289\n",
            "Train loss 432/1000: 2.24406361579895\n",
            "Train loss 433/1000: 2.2439470291137695\n",
            "Train loss 434/1000: 2.243831157684326\n",
            "Train loss 435/1000: 2.243715524673462\n",
            "Train loss 436/1000: 2.2436001300811768\n",
            "Train loss 437/1000: 2.243485689163208\n",
            "Train loss 438/1000: 2.2433714866638184\n",
            "Train loss 439/1000: 2.243257761001587\n",
            "Train loss 440/1000: 2.2431445121765137\n",
            "Train loss 441/1000: 2.2430317401885986\n",
            "Train loss 442/1000: 2.242919445037842\n",
            "Train loss 443/1000: 2.2428078651428223\n",
            "Train loss 444/1000: 2.242696523666382\n",
            "Train loss 445/1000: 2.2425854206085205\n",
            "Train loss 446/1000: 2.2424752712249756\n",
            "Train loss 447/1000: 2.2423651218414307\n",
            "Train loss 448/1000: 2.242255926132202\n",
            "Train loss 449/1000: 2.2421464920043945\n",
            "Train loss 450/1000: 2.2420380115509033\n",
            "Train loss 451/1000: 2.2419300079345703\n",
            "Train loss 452/1000: 2.2418222427368164\n",
            "Train loss 453/1000: 2.2417149543762207\n",
            "Train loss 454/1000: 2.241608142852783\n",
            "Train loss 455/1000: 2.241501569747925\n",
            "Train loss 456/1000: 2.2413954734802246\n",
            "Train loss 457/1000: 2.2412900924682617\n",
            "Train loss 458/1000: 2.241184949874878\n",
            "Train loss 459/1000: 2.2410802841186523\n",
            "Train loss 460/1000: 2.240976095199585\n",
            "Train loss 461/1000: 2.2408719062805176\n",
            "Train loss 462/1000: 2.2407684326171875\n",
            "Train loss 463/1000: 2.2406654357910156\n",
            "Train loss 464/1000: 2.240562677383423\n",
            "Train loss 465/1000: 2.2404606342315674\n",
            "Train loss 466/1000: 2.240358829498291\n",
            "Train loss 467/1000: 2.2402572631835938\n",
            "Train loss 468/1000: 2.240156412124634\n",
            "Train loss 469/1000: 2.240055799484253\n",
            "Train loss 470/1000: 2.239955425262451\n",
            "Train loss 471/1000: 2.2398555278778076\n",
            "Train loss 472/1000: 2.2397561073303223\n",
            "Train loss 473/1000: 2.239657163619995\n",
            "Train loss 474/1000: 2.239558458328247\n",
            "Train loss 475/1000: 2.239459991455078\n",
            "Train loss 476/1000: 2.2393622398376465\n",
            "Train loss 477/1000: 2.239264488220215\n",
            "Train loss 478/1000: 2.2391674518585205\n",
            "Train loss 479/1000: 2.2390708923339844\n",
            "Train loss 480/1000: 2.2389743328094482\n",
            "Train loss 481/1000: 2.2388782501220703\n",
            "Train loss 482/1000: 2.2387826442718506\n",
            "Train loss 483/1000: 2.23868727684021\n",
            "Train loss 484/1000: 2.2385923862457275\n",
            "Train loss 485/1000: 2.238497734069824\n",
            "Train loss 486/1000: 2.2384033203125\n",
            "Train loss 487/1000: 2.238309383392334\n",
            "Train loss 488/1000: 2.2382161617279053\n",
            "Train loss 489/1000: 2.2381229400634766\n",
            "Train loss 490/1000: 2.238029956817627\n",
            "Train loss 491/1000: 2.2379374504089355\n",
            "Train loss 492/1000: 2.2378456592559814\n",
            "Train loss 493/1000: 2.2377536296844482\n",
            "Train loss 494/1000: 2.2376623153686523\n",
            "Train loss 495/1000: 2.2375712394714355\n",
            "Train loss 496/1000: 2.237480401992798\n",
            "Train loss 497/1000: 2.2373900413513184\n",
            "Train loss 498/1000: 2.237300157546997\n",
            "Train loss 499/1000: 2.2372100353240967\n",
            "Train loss 500/1000: 2.237121105194092\n",
            "Train loss 501/1000: 2.2370316982269287\n",
            "Train loss 502/1000: 2.236943006515503\n",
            "Train loss 503/1000: 2.2368545532226562\n",
            "Train loss 504/1000: 2.2367665767669678\n",
            "Train loss 505/1000: 2.2366786003112793\n",
            "Train loss 506/1000: 2.2365915775299072\n",
            "Train loss 507/1000: 2.236504316329956\n",
            "Train loss 508/1000: 2.236417531967163\n",
            "Train loss 509/1000: 2.23633074760437\n",
            "Train loss 510/1000: 2.2362446784973145\n",
            "Train loss 511/1000: 2.236158609390259\n",
            "Train loss 512/1000: 2.2360732555389404\n",
            "Train loss 513/1000: 2.235987901687622\n",
            "Train loss 514/1000: 2.235903024673462\n",
            "Train loss 515/1000: 2.23581862449646\n",
            "Train loss 516/1000: 2.235733985900879\n",
            "Train loss 517/1000: 2.235650062561035\n",
            "Train loss 518/1000: 2.2355663776397705\n",
            "Train loss 519/1000: 2.235482931137085\n",
            "Train loss 520/1000: 2.2353994846343994\n",
            "Train loss 521/1000: 2.2353169918060303\n",
            "Train loss 522/1000: 2.235234260559082\n",
            "Train loss 523/1000: 2.235152006149292\n",
            "Train loss 524/1000: 2.23507022857666\n",
            "Train loss 525/1000: 2.2349884510040283\n",
            "Train loss 526/1000: 2.2349069118499756\n",
            "Train loss 527/1000: 2.23482608795166\n",
            "Train loss 528/1000: 2.2347450256347656\n",
            "Train loss 529/1000: 2.2346644401550293\n",
            "Train loss 530/1000: 2.234584331512451\n",
            "Train loss 531/1000: 2.234504222869873\n",
            "Train loss 532/1000: 2.2344248294830322\n",
            "Train loss 533/1000: 2.2343451976776123\n",
            "Train loss 534/1000: 2.2342660427093506\n",
            "Train loss 535/1000: 2.234187364578247\n",
            "Train loss 536/1000: 2.2341086864471436\n",
            "Train loss 537/1000: 2.2340304851531982\n",
            "Train loss 538/1000: 2.233952522277832\n",
            "Train loss 539/1000: 2.233874797821045\n",
            "Train loss 540/1000: 2.233797073364258\n",
            "Train loss 541/1000: 2.233720064163208\n",
            "Train loss 542/1000: 2.233642816543579\n",
            "Train loss 543/1000: 2.2335660457611084\n",
            "Train loss 544/1000: 2.233489990234375\n",
            "Train loss 545/1000: 2.2334134578704834\n",
            "Train loss 546/1000: 2.233337640762329\n",
            "Train loss 547/1000: 2.233262062072754\n",
            "Train loss 548/1000: 2.2331864833831787\n",
            "Train loss 549/1000: 2.2331113815307617\n",
            "Train loss 550/1000: 2.233036518096924\n",
            "Train loss 551/1000: 2.232961654663086\n",
            "Train loss 552/1000: 2.2328872680664062\n",
            "Train loss 553/1000: 2.2328128814697266\n",
            "Train loss 554/1000: 2.232739210128784\n",
            "Train loss 555/1000: 2.232665538787842\n",
            "Train loss 556/1000: 2.2325921058654785\n",
            "Train loss 557/1000: 2.2325189113616943\n",
            "Train loss 558/1000: 2.2324459552764893\n",
            "Train loss 559/1000: 2.2323734760284424\n",
            "Train loss 560/1000: 2.2323007583618164\n",
            "Train loss 561/1000: 2.2322285175323486\n",
            "Train loss 562/1000: 2.232156991958618\n",
            "Train loss 563/1000: 2.2320847511291504\n",
            "Train loss 564/1000: 2.232013463973999\n",
            "Train loss 565/1000: 2.2319424152374268\n",
            "Train loss 566/1000: 2.2318713665008545\n",
            "Train loss 567/1000: 2.2318005561828613\n",
            "Train loss 568/1000: 2.2317299842834473\n",
            "Train loss 569/1000: 2.231659412384033\n",
            "Train loss 570/1000: 2.2315895557403564\n",
            "Train loss 571/1000: 2.2315196990966797\n",
            "Train loss 572/1000: 2.231450080871582\n",
            "Train loss 573/1000: 2.2313804626464844\n",
            "Train loss 574/1000: 2.231311559677124\n",
            "Train loss 575/1000: 2.2312424182891846\n",
            "Train loss 576/1000: 2.2311737537384033\n",
            "Train loss 577/1000: 2.231105327606201\n",
            "Train loss 578/1000: 2.231036901473999\n",
            "Train loss 579/1000: 2.230968952178955\n",
            "Train loss 580/1000: 2.230900764465332\n",
            "Train loss 581/1000: 2.2308332920074463\n",
            "Train loss 582/1000: 2.2307658195495605\n",
            "Train loss 583/1000: 2.230698585510254\n",
            "Train loss 584/1000: 2.2306318283081055\n",
            "Train loss 585/1000: 2.230564832687378\n",
            "Train loss 586/1000: 2.2304983139038086\n",
            "Train loss 587/1000: 2.2304320335388184\n",
            "Train loss 588/1000: 2.2303659915924072\n",
            "Train loss 589/1000: 2.230299711227417\n",
            "Train loss 590/1000: 2.230233907699585\n",
            "Train loss 591/1000: 2.230168581008911\n",
            "Train loss 592/1000: 2.2301032543182373\n",
            "Train loss 593/1000: 2.2300381660461426\n",
            "Train loss 594/1000: 2.229973554611206\n",
            "Train loss 595/1000: 2.2299087047576904\n",
            "Train loss 596/1000: 2.229844093322754\n",
            "Train loss 597/1000: 2.2297797203063965\n",
            "Train loss 598/1000: 2.229715585708618\n",
            "Train loss 599/1000: 2.229651689529419\n",
            "Train loss 600/1000: 2.229588031768799\n",
            "Train loss 601/1000: 2.229524612426758\n",
            "Train loss 602/1000: 2.229461431503296\n",
            "Train loss 603/1000: 2.229398250579834\n",
            "Train loss 604/1000: 2.2293355464935303\n",
            "Train loss 605/1000: 2.2292723655700684\n",
            "Train loss 606/1000: 2.229210376739502\n",
            "Train loss 607/1000: 2.2291479110717773\n",
            "Train loss 608/1000: 2.229085683822632\n",
            "Train loss 609/1000: 2.2290239334106445\n",
            "Train loss 610/1000: 2.228961944580078\n",
            "Train loss 611/1000: 2.228900194168091\n",
            "Train loss 612/1000: 2.2288389205932617\n",
            "Train loss 613/1000: 2.2287776470184326\n",
            "Train loss 614/1000: 2.2287168502807617\n",
            "Train loss 615/1000: 2.228656053543091\n",
            "Train loss 616/1000: 2.22859525680542\n",
            "Train loss 617/1000: 2.228534698486328\n",
            "Train loss 618/1000: 2.2284743785858154\n",
            "Train loss 619/1000: 2.228414535522461\n",
            "Train loss 620/1000: 2.2283546924591064\n",
            "Train loss 621/1000: 2.228294849395752\n",
            "Train loss 622/1000: 2.2282352447509766\n",
            "Train loss 623/1000: 2.228175640106201\n",
            "Train loss 624/1000: 2.228116512298584\n",
            "Train loss 625/1000: 2.228057861328125\n",
            "Train loss 626/1000: 2.227998971939087\n",
            "Train loss 627/1000: 2.227940082550049\n",
            "Train loss 628/1000: 2.227881669998169\n",
            "Train loss 629/1000: 2.227823257446289\n",
            "Train loss 630/1000: 2.2277650833129883\n",
            "Train loss 631/1000: 2.2277071475982666\n",
            "Train loss 632/1000: 2.227649450302124\n",
            "Train loss 633/1000: 2.2275917530059814\n",
            "Train loss 634/1000: 2.227534532546997\n",
            "Train loss 635/1000: 2.2274773120880127\n",
            "Train loss 636/1000: 2.227419853210449\n",
            "Train loss 637/1000: 2.227363109588623\n",
            "Train loss 638/1000: 2.2273061275482178\n",
            "Train loss 639/1000: 2.2272493839263916\n",
            "Train loss 640/1000: 2.2271931171417236\n",
            "Train loss 641/1000: 2.2271368503570557\n",
            "Train loss 642/1000: 2.2270805835723877\n",
            "Train loss 643/1000: 2.227024555206299\n",
            "Train loss 644/1000: 2.226969003677368\n",
            "Train loss 645/1000: 2.2269134521484375\n",
            "Train loss 646/1000: 2.2268576622009277\n",
            "Train loss 647/1000: 2.2268025875091553\n",
            "Train loss 648/1000: 2.2267472743988037\n",
            "Train loss 649/1000: 2.2266924381256104\n",
            "Train loss 650/1000: 2.226637363433838\n",
            "Train loss 651/1000: 2.2265827655792236\n",
            "Train loss 652/1000: 2.2265281677246094\n",
            "Train loss 653/1000: 2.2264740467071533\n",
            "Train loss 654/1000: 2.226419448852539\n",
            "Train loss 655/1000: 2.226365566253662\n",
            "Train loss 656/1000: 2.226311683654785\n",
            "Train loss 657/1000: 2.2262582778930664\n",
            "Train loss 658/1000: 2.2262043952941895\n",
            "Train loss 659/1000: 2.2261509895324707\n",
            "Train loss 660/1000: 2.226097583770752\n",
            "Train loss 661/1000: 2.226044178009033\n",
            "Train loss 662/1000: 2.225991725921631\n",
            "Train loss 663/1000: 2.225938558578491\n",
            "Train loss 664/1000: 2.2258858680725098\n",
            "Train loss 665/1000: 2.2258334159851074\n",
            "Train loss 666/1000: 2.225780963897705\n",
            "Train loss 667/1000: 2.225728750228882\n",
            "Train loss 668/1000: 2.2256765365600586\n",
            "Train loss 669/1000: 2.2256247997283936\n",
            "Train loss 670/1000: 2.2255728244781494\n",
            "Train loss 671/1000: 2.2255213260650635\n",
            "Train loss 672/1000: 2.2254695892333984\n",
            "Train loss 673/1000: 2.2254183292388916\n",
            "Train loss 674/1000: 2.225367307662964\n",
            "Train loss 675/1000: 2.225315809249878\n",
            "Train loss 676/1000: 2.2252650260925293\n",
            "Train loss 677/1000: 2.2252140045166016\n",
            "Train loss 678/1000: 2.225163459777832\n",
            "Train loss 679/1000: 2.2251129150390625\n",
            "Train loss 680/1000: 2.225062608718872\n",
            "Train loss 681/1000: 2.2250123023986816\n",
            "Train loss 682/1000: 2.2249622344970703\n",
            "Train loss 683/1000: 2.224912405014038\n",
            "Train loss 684/1000: 2.2248623371124268\n",
            "Train loss 685/1000: 2.2248127460479736\n",
            "Train loss 686/1000: 2.2247629165649414\n",
            "Train loss 687/1000: 2.2247138023376465\n",
            "Train loss 688/1000: 2.2246642112731934\n",
            "Train loss 689/1000: 2.2246153354644775\n",
            "Train loss 690/1000: 2.2245659828186035\n",
            "Train loss 691/1000: 2.224517345428467\n",
            "Train loss 692/1000: 2.22446870803833\n",
            "Train loss 693/1000: 2.2244200706481934\n",
            "Train loss 694/1000: 2.2243714332580566\n",
            "Train loss 695/1000: 2.224323272705078\n",
            "Train loss 696/1000: 2.2242748737335205\n",
            "Train loss 697/1000: 2.224226951599121\n",
            "Train loss 698/1000: 2.2241787910461426\n",
            "Train loss 699/1000: 2.224130868911743\n",
            "Train loss 700/1000: 2.224083423614502\n",
            "Train loss 701/1000: 2.2240355014801025\n",
            "Train loss 702/1000: 2.2239880561828613\n",
            "Train loss 703/1000: 2.22394061088562\n",
            "Train loss 704/1000: 2.223893404006958\n",
            "Train loss 705/1000: 2.223846435546875\n",
            "Train loss 706/1000: 2.223799228668213\n",
            "Train loss 707/1000: 2.223752498626709\n",
            "Train loss 708/1000: 2.223705768585205\n",
            "Train loss 709/1000: 2.2236592769622803\n",
            "Train loss 710/1000: 2.2236127853393555\n",
            "Train loss 711/1000: 2.2235662937164307\n",
            "Train loss 712/1000: 2.223520278930664\n",
            "Train loss 713/1000: 2.2234740257263184\n",
            "Train loss 714/1000: 2.223428249359131\n",
            "Train loss 715/1000: 2.2233822345733643\n",
            "Train loss 716/1000: 2.2233364582061768\n",
            "Train loss 717/1000: 2.2232909202575684\n",
            "Train loss 718/1000: 2.223245620727539\n",
            "Train loss 719/1000: 2.2232000827789307\n",
            "Train loss 720/1000: 2.2231547832489014\n",
            "Train loss 721/1000: 2.223109722137451\n",
            "Train loss 722/1000: 2.223064422607422\n",
            "Train loss 723/1000: 2.22301983833313\n",
            "Train loss 724/1000: 2.222975015640259\n",
            "Train loss 725/1000: 2.2229301929473877\n",
            "Train loss 726/1000: 2.2228856086730957\n",
            "Train loss 727/1000: 2.2228410243988037\n",
            "Train loss 728/1000: 2.22279691696167\n",
            "Train loss 729/1000: 2.222752571105957\n",
            "Train loss 730/1000: 2.2227084636688232\n",
            "Train loss 731/1000: 2.2226645946502686\n",
            "Train loss 732/1000: 2.222620725631714\n",
            "Train loss 733/1000: 2.222576856613159\n",
            "Train loss 734/1000: 2.2225332260131836\n",
            "Train loss 735/1000: 2.222489595413208\n",
            "Train loss 736/1000: 2.2224464416503906\n",
            "Train loss 737/1000: 2.222403049468994\n",
            "Train loss 738/1000: 2.2223594188690186\n",
            "Train loss 739/1000: 2.2223167419433594\n",
            "Train loss 740/1000: 2.222273588180542\n",
            "Train loss 741/1000: 2.2222306728363037\n",
            "Train loss 742/1000: 2.2221879959106445\n",
            "Train loss 743/1000: 2.2221453189849854\n",
            "Train loss 744/1000: 2.222102642059326\n",
            "Train loss 745/1000: 2.222060203552246\n",
            "Train loss 746/1000: 2.222018003463745\n",
            "Train loss 747/1000: 2.221975564956665\n",
            "Train loss 748/1000: 2.221933364868164\n",
            "Train loss 749/1000: 2.221891403198242\n",
            "Train loss 750/1000: 2.2218496799468994\n",
            "Train loss 751/1000: 2.2218077182769775\n",
            "Train loss 752/1000: 2.2217659950256348\n",
            "Train loss 753/1000: 2.221724271774292\n",
            "Train loss 754/1000: 2.2216827869415283\n",
            "Train loss 755/1000: 2.2216415405273438\n",
            "Train loss 756/1000: 2.22160005569458\n",
            "Train loss 757/1000: 2.2215588092803955\n",
            "Train loss 758/1000: 2.22151780128479\n",
            "Train loss 759/1000: 2.2214770317077637\n",
            "Train loss 760/1000: 2.221436023712158\n",
            "Train loss 761/1000: 2.2213950157165527\n",
            "Train loss 762/1000: 2.2213544845581055\n",
            "Train loss 763/1000: 2.221313953399658\n",
            "Train loss 764/1000: 2.221273422241211\n",
            "Train loss 765/1000: 2.2212328910827637\n",
            "Train loss 766/1000: 2.2211928367614746\n",
            "Train loss 767/1000: 2.2211525440216064\n",
            "Train loss 768/1000: 2.2211124897003174\n",
            "Train loss 769/1000: 2.2210724353790283\n",
            "Train loss 770/1000: 2.2210326194763184\n",
            "Train loss 771/1000: 2.2209925651550293\n",
            "Train loss 772/1000: 2.2209529876708984\n",
            "Train loss 773/1000: 2.2209131717681885\n",
            "Train loss 774/1000: 2.2208738327026367\n",
            "Train loss 775/1000: 2.220834493637085\n",
            "Train loss 776/1000: 2.220795154571533\n",
            "Train loss 777/1000: 2.2207558155059814\n",
            "Train loss 778/1000: 2.220716714859009\n",
            "Train loss 779/1000: 2.220677375793457\n",
            "Train loss 780/1000: 2.2206387519836426\n",
            "Train loss 781/1000: 2.220599889755249\n",
            "Train loss 782/1000: 2.2205610275268555\n",
            "Train loss 783/1000: 2.220522403717041\n",
            "Train loss 784/1000: 2.2204837799072266\n",
            "Train loss 785/1000: 2.220445394515991\n",
            "Train loss 786/1000: 2.220407009124756\n",
            "Train loss 787/1000: 2.2203688621520996\n",
            "Train loss 788/1000: 2.2203304767608643\n",
            "Train loss 789/1000: 2.220292091369629\n",
            "Train loss 790/1000: 2.220254421234131\n",
            "Train loss 791/1000: 2.2202162742614746\n",
            "Train loss 792/1000: 2.2201786041259766\n",
            "Train loss 793/1000: 2.2201406955718994\n",
            "Train loss 794/1000: 2.2201030254364014\n",
            "Train loss 795/1000: 2.2200655937194824\n",
            "Train loss 796/1000: 2.2200281620025635\n",
            "Train loss 797/1000: 2.2199904918670654\n",
            "Train loss 798/1000: 2.2199535369873047\n",
            "Train loss 799/1000: 2.2199158668518066\n",
            "Train loss 800/1000: 2.219878911972046\n",
            "Train loss 801/1000: 2.219841718673706\n",
            "Train loss 802/1000: 2.2198050022125244\n",
            "Train loss 803/1000: 2.2197680473327637\n",
            "Train loss 804/1000: 2.219731092453003\n",
            "Train loss 805/1000: 2.2196943759918213\n",
            "Train loss 806/1000: 2.2196578979492188\n",
            "Train loss 807/1000: 2.219621181488037\n",
            "Train loss 808/1000: 2.2195849418640137\n",
            "Train loss 809/1000: 2.219548225402832\n",
            "Train loss 810/1000: 2.2195122241973877\n",
            "Train loss 811/1000: 2.2194759845733643\n",
            "Train loss 812/1000: 2.21943998336792\n",
            "Train loss 813/1000: 2.2194037437438965\n",
            "Train loss 814/1000: 2.2193679809570312\n",
            "Train loss 815/1000: 2.219331979751587\n",
            "Train loss 816/1000: 2.2192962169647217\n",
            "Train loss 817/1000: 2.2192606925964355\n",
            "Train loss 818/1000: 2.2192249298095703\n",
            "Train loss 819/1000: 2.219189405441284\n",
            "Train loss 820/1000: 2.219154119491577\n",
            "Train loss 821/1000: 2.219118595123291\n",
            "Train loss 822/1000: 2.219083309173584\n",
            "Train loss 823/1000: 2.219048023223877\n",
            "Train loss 824/1000: 2.21901273727417\n",
            "Train loss 825/1000: 2.218977689743042\n",
            "Train loss 826/1000: 2.218942880630493\n",
            "Train loss 827/1000: 2.2189078330993652\n",
            "Train loss 828/1000: 2.2188730239868164\n",
            "Train loss 829/1000: 2.2188384532928467\n",
            "Train loss 830/1000: 2.218803644180298\n",
            "Train loss 831/1000: 2.218769073486328\n",
            "Train loss 832/1000: 2.2187347412109375\n",
            "Train loss 833/1000: 2.2187001705169678\n",
            "Train loss 834/1000: 2.218665838241577\n",
            "Train loss 835/1000: 2.2186317443847656\n",
            "Train loss 836/1000: 2.218597173690796\n",
            "Train loss 837/1000: 2.2185633182525635\n",
            "Train loss 838/1000: 2.218529224395752\n",
            "Train loss 839/1000: 2.2184953689575195\n",
            "Train loss 840/1000: 2.218461513519287\n",
            "Train loss 841/1000: 2.2184276580810547\n",
            "Train loss 842/1000: 2.2183938026428223\n",
            "Train loss 843/1000: 2.21835994720459\n",
            "Train loss 844/1000: 2.2183265686035156\n",
            "Train loss 845/1000: 2.2182929515838623\n",
            "Train loss 846/1000: 2.218259572982788\n",
            "Train loss 847/1000: 2.218226194381714\n",
            "Train loss 848/1000: 2.2181928157806396\n",
            "Train loss 849/1000: 2.2181596755981445\n",
            "Train loss 850/1000: 2.2181265354156494\n",
            "Train loss 851/1000: 2.2180933952331543\n",
            "Train loss 852/1000: 2.2180604934692383\n",
            "Train loss 853/1000: 2.218027353286743\n",
            "Train loss 854/1000: 2.2179946899414062\n",
            "Train loss 855/1000: 2.2179617881774902\n",
            "Train loss 856/1000: 2.2179293632507324\n",
            "Train loss 857/1000: 2.2178966999053955\n",
            "Train loss 858/1000: 2.2178640365600586\n",
            "Train loss 859/1000: 2.2178313732147217\n",
            "Train loss 860/1000: 2.217799186706543\n",
            "Train loss 861/1000: 2.217766761779785\n",
            "Train loss 862/1000: 2.2177343368530273\n",
            "Train loss 863/1000: 2.2177021503448486\n",
            "Train loss 864/1000: 2.21766996383667\n",
            "Train loss 865/1000: 2.2176380157470703\n",
            "Train loss 866/1000: 2.2176058292388916\n",
            "Train loss 867/1000: 2.217573881149292\n",
            "Train loss 868/1000: 2.2175419330596924\n",
            "Train loss 869/1000: 2.217510223388672\n",
            "Train loss 870/1000: 2.2174785137176514\n",
            "Train loss 871/1000: 2.21744704246521\n",
            "Train loss 872/1000: 2.2174150943756104\n",
            "Train loss 873/1000: 2.217383623123169\n",
            "Train loss 874/1000: 2.2173523902893066\n",
            "Train loss 875/1000: 2.2173209190368652\n",
            "Train loss 876/1000: 2.217289447784424\n",
            "Train loss 877/1000: 2.2172584533691406\n",
            "Train loss 878/1000: 2.2172272205352783\n",
            "Train loss 879/1000: 2.217195987701416\n",
            "Train loss 880/1000: 2.217164993286133\n",
            "Train loss 881/1000: 2.2171337604522705\n",
            "Train loss 882/1000: 2.2171032428741455\n",
            "Train loss 883/1000: 2.217072010040283\n",
            "Train loss 884/1000: 2.217041492462158\n",
            "Train loss 885/1000: 2.217010498046875\n",
            "Train loss 886/1000: 2.216979742050171\n",
            "Train loss 887/1000: 2.216949462890625\n",
            "Train loss 888/1000: 2.216918706893921\n",
            "Train loss 889/1000: 2.216888189315796\n",
            "Train loss 890/1000: 2.21685791015625\n",
            "Train loss 891/1000: 2.216827392578125\n",
            "Train loss 892/1000: 2.216797113418579\n",
            "Train loss 893/1000: 2.2167670726776123\n",
            "Train loss 894/1000: 2.2167367935180664\n",
            "Train loss 895/1000: 2.2167067527770996\n",
            "Train loss 896/1000: 2.216676712036133\n",
            "Train loss 897/1000: 2.216646671295166\n",
            "Train loss 898/1000: 2.2166168689727783\n",
            "Train loss 899/1000: 2.2165868282318115\n",
            "Train loss 900/1000: 2.216557264328003\n",
            "Train loss 901/1000: 2.216527223587036\n",
            "Train loss 902/1000: 2.2164976596832275\n",
            "Train loss 903/1000: 2.216468334197998\n",
            "Train loss 904/1000: 2.2164385318756104\n",
            "Train loss 905/1000: 2.216409206390381\n",
            "Train loss 906/1000: 2.216379404067993\n",
            "Train loss 907/1000: 2.2163503170013428\n",
            "Train loss 908/1000: 2.2163212299346924\n",
            "Train loss 909/1000: 2.216291904449463\n",
            "Train loss 910/1000: 2.2162625789642334\n",
            "Train loss 911/1000: 2.216233730316162\n",
            "Train loss 912/1000: 2.2162041664123535\n",
            "Train loss 913/1000: 2.2161755561828613\n",
            "Train loss 914/1000: 2.216146469116211\n",
            "Train loss 915/1000: 2.2161176204681396\n",
            "Train loss 916/1000: 2.2160887718200684\n",
            "Train loss 917/1000: 2.216059923171997\n",
            "Train loss 918/1000: 2.216031551361084\n",
            "Train loss 919/1000: 2.2160027027130127\n",
            "Train loss 920/1000: 2.2159743309020996\n",
            "Train loss 921/1000: 2.2159454822540283\n",
            "Train loss 922/1000: 2.2159173488616943\n",
            "Train loss 923/1000: 2.2158889770507812\n",
            "Train loss 924/1000: 2.215860366821289\n",
            "Train loss 925/1000: 2.215832233428955\n",
            "Train loss 926/1000: 2.215803861618042\n",
            "Train loss 927/1000: 2.215775728225708\n",
            "Train loss 928/1000: 2.215747594833374\n",
            "Train loss 929/1000: 2.21571946144104\n",
            "Train loss 930/1000: 2.215691566467285\n",
            "Train loss 931/1000: 2.215663194656372\n",
            "Train loss 932/1000: 2.2156355381011963\n",
            "Train loss 933/1000: 2.2156076431274414\n",
            "Train loss 934/1000: 2.2155799865722656\n",
            "Train loss 935/1000: 2.2155520915985107\n",
            "Train loss 936/1000: 2.215524435043335\n",
            "Train loss 937/1000: 2.215496778488159\n",
            "Train loss 938/1000: 2.2154693603515625\n",
            "Train loss 939/1000: 2.215441942214966\n",
            "Train loss 940/1000: 2.21541428565979\n",
            "Train loss 941/1000: 2.2153871059417725\n",
            "Train loss 942/1000: 2.2153594493865967\n",
            "Train loss 943/1000: 2.21533203125\n",
            "Train loss 944/1000: 2.2153050899505615\n",
            "Train loss 945/1000: 2.215278148651123\n",
            "Train loss 946/1000: 2.2152507305145264\n",
            "Train loss 947/1000: 2.215223550796509\n",
            "Train loss 948/1000: 2.2151966094970703\n",
            "Train loss 949/1000: 2.215169668197632\n",
            "Train loss 950/1000: 2.2151427268981934\n",
            "Train loss 951/1000: 2.215116024017334\n",
            "Train loss 952/1000: 2.2150890827178955\n",
            "Train loss 953/1000: 2.215062141418457\n",
            "Train loss 954/1000: 2.2150354385375977\n",
            "Train loss 955/1000: 2.2150087356567383\n",
            "Train loss 956/1000: 2.214982032775879\n",
            "Train loss 957/1000: 2.2149558067321777\n",
            "Train loss 958/1000: 2.2149293422698975\n",
            "Train loss 959/1000: 2.214902639389038\n",
            "Train loss 960/1000: 2.214876413345337\n",
            "Train loss 961/1000: 2.2148499488830566\n",
            "Train loss 962/1000: 2.2148234844207764\n",
            "Train loss 963/1000: 2.2147977352142334\n",
            "Train loss 964/1000: 2.214771032333374\n",
            "Train loss 965/1000: 2.214745044708252\n",
            "Train loss 966/1000: 2.21471905708313\n",
            "Train loss 967/1000: 2.214693069458008\n",
            "Train loss 968/1000: 2.2146668434143066\n",
            "Train loss 969/1000: 2.2146408557891846\n",
            "Train loss 970/1000: 2.2146148681640625\n",
            "Train loss 971/1000: 2.2145891189575195\n",
            "Train loss 972/1000: 2.2145633697509766\n",
            "Train loss 973/1000: 2.2145376205444336\n",
            "Train loss 974/1000: 2.2145118713378906\n",
            "Train loss 975/1000: 2.2144861221313477\n",
            "Train loss 976/1000: 2.214460611343384\n",
            "Train loss 977/1000: 2.214434862136841\n",
            "Train loss 978/1000: 2.214409589767456\n",
            "Train loss 979/1000: 2.2143843173980713\n",
            "Train loss 980/1000: 2.2143588066101074\n",
            "Train loss 981/1000: 2.2143335342407227\n",
            "Train loss 982/1000: 2.214308261871338\n",
            "Train loss 983/1000: 2.214282989501953\n",
            "Train loss 984/1000: 2.2142574787139893\n",
            "Train loss 985/1000: 2.2142324447631836\n",
            "Train loss 986/1000: 2.214207410812378\n",
            "Train loss 987/1000: 2.214182138442993\n",
            "Train loss 988/1000: 2.2141571044921875\n",
            "Train loss 989/1000: 2.214132308959961\n",
            "Train loss 990/1000: 2.2141075134277344\n",
            "Train loss 991/1000: 2.2140822410583496\n",
            "Train loss 992/1000: 2.214057683944702\n",
            "Train loss 993/1000: 2.2140326499938965\n",
            "Train loss 994/1000: 2.214008092880249\n",
            "Train loss 995/1000: 2.2139832973480225\n",
            "Train loss 996/1000: 2.213958740234375\n",
            "Train loss 997/1000: 2.2139341831207275\n",
            "Train loss 998/1000: 2.213909387588501\n",
            "Train loss 999/1000: 2.2138850688934326\n",
            "Train loss 1000/1000: 2.213860511779785\n",
            "junide\n",
            "janasid\n",
            "prelay\n",
            "adin\n",
            "kairritoper\n",
            "sathen\n",
            "sameia\n",
            "yanileniassibduinewin\n",
            "lessiyanayla\n",
            "te\n",
            "farmanthyfortumj\n",
            "ponn\n",
            "lena\n",
            "jaylicore\n",
            "ya\n",
            "jocken\n",
            "jamilyn\n",
            "korin\n",
            "wyn\n",
            "ne\n",
            "gaasnhavi\n",
            "monszxhxdgor\n",
            "mathani\n",
            "zie\n",
            "paun\n",
            "ty\n",
            "tin\n",
            "sreli\n",
            "ish\n",
            "dyn\n",
            "rumikujcmkhaubwyla\n",
            "kha\n",
            "cra\n",
            "raydnhwadorta\n",
            "malyn\n",
            "brey\n",
            "aur\n",
            "lavarock\n",
            "themiraya\n",
            "ath\n",
            "basely\n",
            "tavisonikiyaalee\n",
            "marlen\n",
            "em\n",
            "fabethellianten\n",
            "chan\n",
            "jazaody\n",
            "drd\n",
            "johialiypjrgia\n",
            "tezrwaylia\n",
            "vywhqelvani\n",
            "sahimah\n",
            "kellette\n",
            "braceodon\n",
            "ali\n",
            "alian\n",
            "denn\n",
            "jayannyah\n",
            "kennelyn\n",
            "marianner\n",
            "samotan\n",
            "kyroderihana\n",
            "shday\n",
            "ta\n",
            "olleah\n",
            "tertte\n",
            "keus\n",
            "dasia\n",
            "na\n",
            "chlynevini\n",
            "aspie\n",
            "lealilondral\n",
            "fanmari\n",
            "mishama\n",
            "verykengeon\n",
            "resynivion\n",
            "uzien\n",
            "jalivyah\n",
            "alto\n",
            "marafelanaylian\n",
            "ohanorrisyli\n",
            "mson\n",
            "jorayiangkxdvjypnfria\n",
            "maulil\n",
            "cacvjveer\n",
            "ael\n",
            "shirla\n",
            "sa\n",
            "pre\n",
            "azaikeri\n",
            "khanna\n",
            "abbiha\n",
            "isa\n",
            "brailyond\n",
            "bradeelyn\n",
            "zyarierri\n",
            "chamadayda\n",
            "fbduqayceeyton\n",
            "za\n",
            "sabdin\n",
            "han\n",
            "kennasslen\n",
            "conik\n",
            "ny\n",
            "naaston\n",
            "tisona\n",
            "isemrtellwes\n",
            "aleissarekam\n",
            "ljaykjmhiffaresiyah\n",
            "lula\n",
            "jus\n",
            "b\n",
            "den\n",
            "hawyqsum\n",
            "ozie\n",
            "hasiam\n",
            "na\n",
            "adiamordelamiquadilah\n",
            "an\n",
            "keika\n",
            "viyah\n",
            "ax\n",
            "mandt\n",
            "tyjamanalingh\n",
            "jepekala\n",
            "koberishurays\n",
            "khin\n",
            "haud\n",
            "wcalipqkwel\n",
            "chieann\n",
            "madiah\n",
            "pasdtianancs\n",
            "cobzmarodiva\n",
            "myah\n",
            "kaws\n",
            "eva\n",
            "rie\n",
            "laydeni\n",
            "dasiradinie\n",
            "cora\n",
            "aulynnah\n",
            "dever\n",
            "shen\n",
            "jo\n",
            "esten\n",
            "die\n",
            "alaciordeliskyann\n",
            "ala\n",
            "der\n",
            "hambenda\n",
            "dale\n",
            "aansharalarynehmiybrashozirael\n",
            "ta\n",
            "ix\n",
            "na\n",
            "ek\n",
            "maryena\n",
            "kemerytle\n",
            "kacqueelqexen\n",
            "caliavan\n",
            "ann\n",
            "inyohasyn\n",
            "frey\n",
            "an\n",
            "ya\n",
            "laydena\n",
            "bronidavicopkrniella\n",
            "evkyreigen\n",
            "amaylee\n",
            "tala\n",
            "laton\n",
            "gialiah\n",
            "joyarisen\n",
            "kamela\n",
            "la\n",
            "shoselynna\n",
            "ib\n",
            "sundrenn\n",
            "qsia\n",
            "mince\n",
            "marmileyah\n",
            "anus\n",
            "redon\n",
            "kilapxzdfvdriell\n",
            "luozel\n",
            "hanady\n",
            "woordi\n",
            "iksh\n",
            "delaitonnah\n",
            "zylah\n",
            "trayn\n",
            "ha\n",
            "ron\n",
            "alayly\n",
            "gvkvekhtaerri\n",
            "eloveriel\n",
            "chnu\n",
            "brah\n",
            "sia\n",
            "aann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_weights = train_model(inputs, targets)\n",
        "sample_names(model_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OPmmcIwK4o28",
        "outputId": "a4e50fa0-4f4b-4384-9d31-c1456c7ae916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 1/1000: 3.792776346206665\n",
            "Train loss 2/1000: 3.538167953491211\n",
            "Train loss 3/1000: 3.417757511138916\n",
            "Train loss 4/1000: 3.3257200717926025\n",
            "Train loss 5/1000: 3.2489397525787354\n",
            "Train loss 6/1000: 3.1840076446533203\n",
            "Train loss 7/1000: 3.1282403469085693\n",
            "Train loss 8/1000: 3.0796451568603516\n",
            "Train loss 9/1000: 3.036648750305176\n",
            "Train loss 10/1000: 2.998178005218506\n",
            "Train loss 11/1000: 2.9634721279144287\n",
            "Train loss 12/1000: 2.931981086730957\n",
            "Train loss 13/1000: 2.9032769203186035\n",
            "Train loss 14/1000: 2.8770182132720947\n",
            "Train loss 15/1000: 2.8529207706451416\n",
            "Train loss 16/1000: 2.8307430744171143\n",
            "Train loss 17/1000: 2.810276985168457\n",
            "Train loss 18/1000: 2.791339159011841\n",
            "Train loss 19/1000: 2.7737646102905273\n",
            "Train loss 20/1000: 2.7574081420898438\n",
            "Train loss 21/1000: 2.7421388626098633\n",
            "Train loss 22/1000: 2.7278409004211426\n",
            "Train loss 23/1000: 2.7144129276275635\n",
            "Train loss 24/1000: 2.7017667293548584\n",
            "Train loss 25/1000: 2.689824104309082\n",
            "Train loss 26/1000: 2.6785190105438232\n",
            "Train loss 27/1000: 2.667792558670044\n",
            "Train loss 28/1000: 2.657594919204712\n",
            "Train loss 29/1000: 2.647880792617798\n",
            "Train loss 30/1000: 2.638612747192383\n",
            "Train loss 31/1000: 2.6297545433044434\n",
            "Train loss 32/1000: 2.621277093887329\n",
            "Train loss 33/1000: 2.613152265548706\n",
            "Train loss 34/1000: 2.6053566932678223\n",
            "Train loss 35/1000: 2.597867488861084\n",
            "Train loss 36/1000: 2.590665578842163\n",
            "Train loss 37/1000: 2.5837326049804688\n",
            "Train loss 38/1000: 2.577052593231201\n",
            "Train loss 39/1000: 2.570610523223877\n",
            "Train loss 40/1000: 2.5643930435180664\n",
            "Train loss 41/1000: 2.5583877563476562\n",
            "Train loss 42/1000: 2.5525834560394287\n",
            "Train loss 43/1000: 2.546968936920166\n",
            "Train loss 44/1000: 2.5415351390838623\n",
            "Train loss 45/1000: 2.5362725257873535\n",
            "Train loss 46/1000: 2.53117299079895\n",
            "Train loss 47/1000: 2.5262293815612793\n",
            "Train loss 48/1000: 2.5214338302612305\n",
            "Train loss 49/1000: 2.5167791843414307\n",
            "Train loss 50/1000: 2.5122599601745605\n",
            "Train loss 51/1000: 2.5078697204589844\n",
            "Train loss 52/1000: 2.5036027431488037\n",
            "Train loss 53/1000: 2.4994540214538574\n",
            "Train loss 54/1000: 2.4954185485839844\n",
            "Train loss 55/1000: 2.4914917945861816\n",
            "Train loss 56/1000: 2.487668991088867\n",
            "Train loss 57/1000: 2.4839463233947754\n",
            "Train loss 58/1000: 2.4803197383880615\n",
            "Train loss 59/1000: 2.476785659790039\n",
            "Train loss 60/1000: 2.4733402729034424\n",
            "Train loss 61/1000: 2.469980239868164\n",
            "Train loss 62/1000: 2.466702699661255\n",
            "Train loss 63/1000: 2.4635047912597656\n",
            "Train loss 64/1000: 2.460383176803589\n",
            "Train loss 65/1000: 2.4573352336883545\n",
            "Train loss 66/1000: 2.4543588161468506\n",
            "Train loss 67/1000: 2.451450824737549\n",
            "Train loss 68/1000: 2.448608875274658\n",
            "Train loss 69/1000: 2.445831298828125\n",
            "Train loss 70/1000: 2.4431159496307373\n",
            "Train loss 71/1000: 2.440460443496704\n",
            "Train loss 72/1000: 2.4378628730773926\n",
            "Train loss 73/1000: 2.435321569442749\n",
            "Train loss 74/1000: 2.4328343868255615\n",
            "Train loss 75/1000: 2.4303998947143555\n",
            "Train loss 76/1000: 2.428015947341919\n",
            "Train loss 77/1000: 2.4256813526153564\n",
            "Train loss 78/1000: 2.4233949184417725\n",
            "Train loss 79/1000: 2.421154737472534\n",
            "Train loss 80/1000: 2.418959140777588\n",
            "Train loss 81/1000: 2.4168074131011963\n",
            "Train loss 82/1000: 2.4146976470947266\n",
            "Train loss 83/1000: 2.4126288890838623\n",
            "Train loss 84/1000: 2.410599946975708\n",
            "Train loss 85/1000: 2.4086098670959473\n",
            "Train loss 86/1000: 2.4066569805145264\n",
            "Train loss 87/1000: 2.404740571975708\n",
            "Train loss 88/1000: 2.4028594493865967\n",
            "Train loss 89/1000: 2.401012659072876\n",
            "Train loss 90/1000: 2.3991992473602295\n",
            "Train loss 91/1000: 2.3974180221557617\n",
            "Train loss 92/1000: 2.3956687450408936\n",
            "Train loss 93/1000: 2.393949508666992\n",
            "Train loss 94/1000: 2.3922603130340576\n",
            "Train loss 95/1000: 2.3906004428863525\n",
            "Train loss 96/1000: 2.3889682292938232\n",
            "Train loss 97/1000: 2.387363910675049\n",
            "Train loss 98/1000: 2.3857860565185547\n",
            "Train loss 99/1000: 2.3842344284057617\n",
            "Train loss 100/1000: 2.3827080726623535\n",
            "Train loss 101/1000: 2.3812062740325928\n",
            "Train loss 102/1000: 2.379728317260742\n",
            "Train loss 103/1000: 2.3782742023468018\n",
            "Train loss 104/1000: 2.376842498779297\n",
            "Train loss 105/1000: 2.3754334449768066\n",
            "Train loss 106/1000: 2.3740460872650146\n",
            "Train loss 107/1000: 2.3726799488067627\n",
            "Train loss 108/1000: 2.3713345527648926\n",
            "Train loss 109/1000: 2.370009183883667\n",
            "Train loss 110/1000: 2.3687033653259277\n",
            "Train loss 111/1000: 2.367417097091675\n",
            "Train loss 112/1000: 2.366149425506592\n",
            "Train loss 113/1000: 2.3649001121520996\n",
            "Train loss 114/1000: 2.36366868019104\n",
            "Train loss 115/1000: 2.362455129623413\n",
            "Train loss 116/1000: 2.3612582683563232\n",
            "Train loss 117/1000: 2.3600783348083496\n",
            "Train loss 118/1000: 2.358915090560913\n",
            "Train loss 119/1000: 2.3577680587768555\n",
            "Train loss 120/1000: 2.3566362857818604\n",
            "Train loss 121/1000: 2.3555195331573486\n",
            "Train loss 122/1000: 2.3544185161590576\n",
            "Train loss 123/1000: 2.3533318042755127\n",
            "Train loss 124/1000: 2.352259635925293\n",
            "Train loss 125/1000: 2.3512020111083984\n",
            "Train loss 126/1000: 2.3501579761505127\n",
            "Train loss 127/1000: 2.3491270542144775\n",
            "Train loss 128/1000: 2.3481099605560303\n",
            "Train loss 129/1000: 2.3471055030822754\n",
            "Train loss 130/1000: 2.346113920211792\n",
            "Train loss 131/1000: 2.345134973526001\n",
            "Train loss 132/1000: 2.3441684246063232\n",
            "Train loss 133/1000: 2.3432137966156006\n",
            "Train loss 134/1000: 2.342270851135254\n",
            "Train loss 135/1000: 2.3413398265838623\n",
            "Train loss 136/1000: 2.3404202461242676\n",
            "Train loss 137/1000: 2.3395113945007324\n",
            "Train loss 138/1000: 2.338613748550415\n",
            "Train loss 139/1000: 2.3377268314361572\n",
            "Train loss 140/1000: 2.336850881576538\n",
            "Train loss 141/1000: 2.335984706878662\n",
            "Train loss 142/1000: 2.3351287841796875\n",
            "Train loss 143/1000: 2.3342835903167725\n",
            "Train loss 144/1000: 2.3334474563598633\n",
            "Train loss 145/1000: 2.3326213359832764\n",
            "Train loss 146/1000: 2.3318047523498535\n",
            "Train loss 147/1000: 2.3309972286224365\n",
            "Train loss 148/1000: 2.3301992416381836\n",
            "Train loss 149/1000: 2.3294098377227783\n",
            "Train loss 150/1000: 2.328629732131958\n",
            "Train loss 151/1000: 2.3278579711914062\n",
            "Train loss 152/1000: 2.3270950317382812\n",
            "Train loss 153/1000: 2.326340913772583\n",
            "Train loss 154/1000: 2.325594663619995\n",
            "Train loss 155/1000: 2.324856758117676\n",
            "Train loss 156/1000: 2.3241262435913086\n",
            "Train loss 157/1000: 2.3234047889709473\n",
            "Train loss 158/1000: 2.32269024848938\n",
            "Train loss 159/1000: 2.3219833374023438\n",
            "Train loss 160/1000: 2.321284294128418\n",
            "Train loss 161/1000: 2.3205926418304443\n",
            "Train loss 162/1000: 2.319908380508423\n",
            "Train loss 163/1000: 2.319230794906616\n",
            "Train loss 164/1000: 2.318560838699341\n",
            "Train loss 165/1000: 2.317897319793701\n",
            "Train loss 166/1000: 2.3172411918640137\n",
            "Train loss 167/1000: 2.316591501235962\n",
            "Train loss 168/1000: 2.315948963165283\n",
            "Train loss 169/1000: 2.315312385559082\n",
            "Train loss 170/1000: 2.3146822452545166\n",
            "Train loss 171/1000: 2.314058780670166\n",
            "Train loss 172/1000: 2.313441038131714\n",
            "Train loss 173/1000: 2.3128302097320557\n",
            "Train loss 174/1000: 2.312224864959717\n",
            "Train loss 175/1000: 2.3116257190704346\n",
            "Train loss 176/1000: 2.31103253364563\n",
            "Train loss 177/1000: 2.3104450702667236\n",
            "Train loss 178/1000: 2.309863328933716\n",
            "Train loss 179/1000: 2.3092873096466064\n",
            "Train loss 180/1000: 2.3087167739868164\n",
            "Train loss 181/1000: 2.308151960372925\n",
            "Train loss 182/1000: 2.3075921535491943\n",
            "Train loss 183/1000: 2.307037591934204\n",
            "Train loss 184/1000: 2.3064887523651123\n",
            "Train loss 185/1000: 2.3059446811676025\n",
            "Train loss 186/1000: 2.305405855178833\n",
            "Train loss 187/1000: 2.3048720359802246\n",
            "Train loss 188/1000: 2.3043434619903564\n",
            "Train loss 189/1000: 2.303819417953491\n",
            "Train loss 190/1000: 2.303300380706787\n",
            "Train loss 191/1000: 2.302786111831665\n",
            "Train loss 192/1000: 2.302276372909546\n",
            "Train loss 193/1000: 2.301771640777588\n",
            "Train loss 194/1000: 2.3012712001800537\n",
            "Train loss 195/1000: 2.3007750511169434\n",
            "Train loss 196/1000: 2.300283670425415\n",
            "Train loss 197/1000: 2.2997963428497314\n",
            "Train loss 198/1000: 2.299313545227051\n",
            "Train loss 199/1000: 2.298835277557373\n",
            "Train loss 200/1000: 2.29836106300354\n",
            "Train loss 201/1000: 2.2978906631469727\n",
            "Train loss 202/1000: 2.297424793243408\n",
            "Train loss 203/1000: 2.2969627380371094\n",
            "Train loss 204/1000: 2.2965047359466553\n",
            "Train loss 205/1000: 2.296050786972046\n",
            "Train loss 206/1000: 2.295600414276123\n",
            "Train loss 207/1000: 2.295154571533203\n",
            "Train loss 208/1000: 2.2947118282318115\n",
            "Train loss 209/1000: 2.2942731380462646\n",
            "Train loss 210/1000: 2.2938382625579834\n",
            "Train loss 211/1000: 2.2934064865112305\n",
            "Train loss 212/1000: 2.2929787635803223\n",
            "Train loss 213/1000: 2.2925546169281006\n",
            "Train loss 214/1000: 2.2921340465545654\n",
            "Train loss 215/1000: 2.2917168140411377\n",
            "Train loss 216/1000: 2.2913029193878174\n",
            "Train loss 217/1000: 2.2908923625946045\n",
            "Train loss 218/1000: 2.2904856204986572\n",
            "Train loss 219/1000: 2.2900819778442383\n",
            "Train loss 220/1000: 2.2896811962127686\n",
            "Train loss 221/1000: 2.2892839908599854\n",
            "Train loss 222/1000: 2.2888898849487305\n",
            "Train loss 223/1000: 2.288499116897583\n",
            "Train loss 224/1000: 2.2881112098693848\n",
            "Train loss 225/1000: 2.287726640701294\n",
            "Train loss 226/1000: 2.2873449325561523\n",
            "Train loss 227/1000: 2.286966323852539\n",
            "Train loss 228/1000: 2.286590576171875\n",
            "Train loss 229/1000: 2.28621768951416\n",
            "Train loss 230/1000: 2.2858479022979736\n",
            "Train loss 231/1000: 2.2854809761047363\n",
            "Train loss 232/1000: 2.2851169109344482\n",
            "Train loss 233/1000: 2.284755229949951\n",
            "Train loss 234/1000: 2.2843968868255615\n",
            "Train loss 235/1000: 2.284040927886963\n",
            "Train loss 236/1000: 2.2836878299713135\n",
            "Train loss 237/1000: 2.283337354660034\n",
            "Train loss 238/1000: 2.282989263534546\n",
            "Train loss 239/1000: 2.282644033432007\n",
            "Train loss 240/1000: 2.282301664352417\n",
            "Train loss 241/1000: 2.28196120262146\n",
            "Train loss 242/1000: 2.2816238403320312\n",
            "Train loss 243/1000: 2.2812888622283936\n",
            "Train loss 244/1000: 2.280956268310547\n",
            "Train loss 245/1000: 2.280626058578491\n",
            "Train loss 246/1000: 2.2802982330322266\n",
            "Train loss 247/1000: 2.279973268508911\n",
            "Train loss 248/1000: 2.2796499729156494\n",
            "Train loss 249/1000: 2.279329538345337\n",
            "Train loss 250/1000: 2.2790110111236572\n",
            "Train loss 251/1000: 2.2786953449249268\n",
            "Train loss 252/1000: 2.27838134765625\n",
            "Train loss 253/1000: 2.2780697345733643\n",
            "Train loss 254/1000: 2.2777605056762695\n",
            "Train loss 255/1000: 2.2774534225463867\n",
            "Train loss 256/1000: 2.277148485183716\n",
            "Train loss 257/1000: 2.276845693588257\n",
            "Train loss 258/1000: 2.2765450477600098\n",
            "Train loss 259/1000: 2.2762463092803955\n",
            "Train loss 260/1000: 2.2759499549865723\n",
            "Train loss 261/1000: 2.275655508041382\n",
            "Train loss 262/1000: 2.275362968444824\n",
            "Train loss 263/1000: 2.2750725746154785\n",
            "Train loss 264/1000: 2.2747840881347656\n",
            "Train loss 265/1000: 2.2744975090026855\n",
            "Train loss 266/1000: 2.2742133140563965\n",
            "Train loss 267/1000: 2.273930549621582\n",
            "Train loss 268/1000: 2.2736499309539795\n",
            "Train loss 269/1000: 2.2733712196350098\n",
            "Train loss 270/1000: 2.273094654083252\n",
            "Train loss 271/1000: 2.2728192806243896\n",
            "Train loss 272/1000: 2.2725460529327393\n",
            "Train loss 273/1000: 2.272274971008301\n",
            "Train loss 274/1000: 2.272005319595337\n",
            "Train loss 275/1000: 2.2717373371124268\n",
            "Train loss 276/1000: 2.2714715003967285\n",
            "Train loss 277/1000: 2.271207094192505\n",
            "Train loss 278/1000: 2.270944595336914\n",
            "Train loss 279/1000: 2.270683765411377\n",
            "Train loss 280/1000: 2.2704246044158936\n",
            "Train loss 281/1000: 2.2701668739318848\n",
            "Train loss 282/1000: 2.269911289215088\n",
            "Train loss 283/1000: 2.2696571350097656\n",
            "Train loss 284/1000: 2.269404411315918\n",
            "Train loss 285/1000: 2.269153594970703\n",
            "Train loss 286/1000: 2.268904209136963\n",
            "Train loss 287/1000: 2.2686562538146973\n",
            "Train loss 288/1000: 2.2684102058410645\n",
            "Train loss 289/1000: 2.2681655883789062\n",
            "Train loss 290/1000: 2.2679226398468018\n",
            "Train loss 291/1000: 2.2676808834075928\n",
            "Train loss 292/1000: 2.2674407958984375\n",
            "Train loss 293/1000: 2.267202138900757\n",
            "Train loss 294/1000: 2.26696515083313\n",
            "Train loss 295/1000: 2.2667295932769775\n",
            "Train loss 296/1000: 2.2664952278137207\n",
            "Train loss 297/1000: 2.2662625312805176\n",
            "Train loss 298/1000: 2.266031265258789\n",
            "Train loss 299/1000: 2.265801429748535\n",
            "Train loss 300/1000: 2.2655727863311768\n",
            "Train loss 301/1000: 2.265345811843872\n",
            "Train loss 302/1000: 2.265120029449463\n",
            "Train loss 303/1000: 2.264895439147949\n",
            "Train loss 304/1000: 2.2646725177764893\n",
            "Train loss 305/1000: 2.264450788497925\n",
            "Train loss 306/1000: 2.264230251312256\n",
            "Train loss 307/1000: 2.2640111446380615\n",
            "Train loss 308/1000: 2.263793468475342\n",
            "Train loss 309/1000: 2.2635767459869385\n",
            "Train loss 310/1000: 2.263361692428589\n",
            "Train loss 311/1000: 2.2631478309631348\n",
            "Train loss 312/1000: 2.262935161590576\n",
            "Train loss 313/1000: 2.262723684310913\n",
            "Train loss 314/1000: 2.2625131607055664\n",
            "Train loss 315/1000: 2.2623043060302734\n",
            "Train loss 316/1000: 2.262096643447876\n",
            "Train loss 317/1000: 2.261889934539795\n",
            "Train loss 318/1000: 2.2616844177246094\n",
            "Train loss 319/1000: 2.2614803314208984\n",
            "Train loss 320/1000: 2.261277198791504\n",
            "Train loss 321/1000: 2.261075258255005\n",
            "Train loss 322/1000: 2.2608742713928223\n",
            "Train loss 323/1000: 2.2606747150421143\n",
            "Train loss 324/1000: 2.2604761123657227\n",
            "Train loss 325/1000: 2.2602787017822266\n",
            "Train loss 326/1000: 2.260082483291626\n",
            "Train loss 327/1000: 2.259887218475342\n",
            "Train loss 328/1000: 2.259693145751953\n",
            "Train loss 329/1000: 2.259500026702881\n",
            "Train loss 330/1000: 2.259308099746704\n",
            "Train loss 331/1000: 2.2591171264648438\n",
            "Train loss 332/1000: 2.258927345275879\n",
            "Train loss 333/1000: 2.2587385177612305\n",
            "Train loss 334/1000: 2.2585508823394775\n",
            "Train loss 335/1000: 2.258363962173462\n",
            "Train loss 336/1000: 2.258178234100342\n",
            "Train loss 337/1000: 2.257993459701538\n",
            "Train loss 338/1000: 2.25780987739563\n",
            "Train loss 339/1000: 2.257627010345459\n",
            "Train loss 340/1000: 2.2574455738067627\n",
            "Train loss 341/1000: 2.2572646141052246\n",
            "Train loss 342/1000: 2.257084846496582\n",
            "Train loss 343/1000: 2.2569057941436768\n",
            "Train loss 344/1000: 2.256727933883667\n",
            "Train loss 345/1000: 2.2565512657165527\n",
            "Train loss 346/1000: 2.2563750743865967\n",
            "Train loss 347/1000: 2.256199836730957\n",
            "Train loss 348/1000: 2.256025791168213\n",
            "Train loss 349/1000: 2.255852699279785\n",
            "Train loss 350/1000: 2.2556803226470947\n",
            "Train loss 351/1000: 2.2555086612701416\n",
            "Train loss 352/1000: 2.255338191986084\n",
            "Train loss 353/1000: 2.2551686763763428\n",
            "Train loss 354/1000: 2.2549996376037598\n",
            "Train loss 355/1000: 2.254831552505493\n",
            "Train loss 356/1000: 2.254664659500122\n",
            "Train loss 357/1000: 2.2544984817504883\n",
            "Train loss 358/1000: 2.254333257675171\n",
            "Train loss 359/1000: 2.254168748855591\n",
            "Train loss 360/1000: 2.254004955291748\n",
            "Train loss 361/1000: 2.2538421154022217\n",
            "Train loss 362/1000: 2.2536799907684326\n",
            "Train loss 363/1000: 2.25351881980896\n",
            "Train loss 364/1000: 2.2533586025238037\n",
            "Train loss 365/1000: 2.2531988620758057\n",
            "Train loss 366/1000: 2.253040313720703\n",
            "Train loss 367/1000: 2.252882242202759\n",
            "Train loss 368/1000: 2.2527248859405518\n",
            "Train loss 369/1000: 2.252568483352661\n",
            "Train loss 370/1000: 2.252412796020508\n",
            "Train loss 371/1000: 2.252258062362671\n",
            "Train loss 372/1000: 2.252103805541992\n",
            "Train loss 373/1000: 2.251950263977051\n",
            "Train loss 374/1000: 2.251797676086426\n",
            "Train loss 375/1000: 2.251646041870117\n",
            "Train loss 376/1000: 2.251494884490967\n",
            "Train loss 377/1000: 2.2513444423675537\n",
            "Train loss 378/1000: 2.251194715499878\n",
            "Train loss 379/1000: 2.2510459423065186\n",
            "Train loss 380/1000: 2.2508976459503174\n",
            "Train loss 381/1000: 2.2507503032684326\n",
            "Train loss 382/1000: 2.250603437423706\n",
            "Train loss 383/1000: 2.250457286834717\n",
            "Train loss 384/1000: 2.250311851501465\n",
            "Train loss 385/1000: 2.25016713142395\n",
            "Train loss 386/1000: 2.250023126602173\n",
            "Train loss 387/1000: 2.249879837036133\n",
            "Train loss 388/1000: 2.24973726272583\n",
            "Train loss 389/1000: 2.2495951652526855\n",
            "Train loss 390/1000: 2.2494537830352783\n",
            "Train loss 391/1000: 2.2493131160736084\n",
            "Train loss 392/1000: 2.249173164367676\n",
            "Train loss 393/1000: 2.2490341663360596\n",
            "Train loss 394/1000: 2.2488954067230225\n",
            "Train loss 395/1000: 2.2487568855285645\n",
            "Train loss 396/1000: 2.248619556427002\n",
            "Train loss 397/1000: 2.2484829425811768\n",
            "Train loss 398/1000: 2.248347043991089\n",
            "Train loss 399/1000: 2.248211145401001\n",
            "Train loss 400/1000: 2.2480764389038086\n",
            "Train loss 401/1000: 2.2479422092437744\n",
            "Train loss 402/1000: 2.2478086948394775\n",
            "Train loss 403/1000: 2.2476754188537598\n",
            "Train loss 404/1000: 2.2475428581237793\n",
            "Train loss 405/1000: 2.247411012649536\n",
            "Train loss 406/1000: 2.2472798824310303\n",
            "Train loss 407/1000: 2.2471492290496826\n",
            "Train loss 408/1000: 2.2470192909240723\n",
            "Train loss 409/1000: 2.246889591217041\n",
            "Train loss 410/1000: 2.246760845184326\n",
            "Train loss 411/1000: 2.2466323375701904\n",
            "Train loss 412/1000: 2.246504783630371\n",
            "Train loss 413/1000: 2.246377468109131\n",
            "Train loss 414/1000: 2.246250867843628\n",
            "Train loss 415/1000: 2.246124744415283\n",
            "Train loss 416/1000: 2.2459990978240967\n",
            "Train loss 417/1000: 2.2458744049072266\n",
            "Train loss 418/1000: 2.2457499504089355\n",
            "Train loss 419/1000: 2.2456259727478027\n",
            "Train loss 420/1000: 2.2455027103424072\n",
            "Train loss 421/1000: 2.24537992477417\n",
            "Train loss 422/1000: 2.245257616043091\n",
            "Train loss 423/1000: 2.24513578414917\n",
            "Train loss 424/1000: 2.2450146675109863\n",
            "Train loss 425/1000: 2.24489426612854\n",
            "Train loss 426/1000: 2.2447738647460938\n",
            "Train loss 427/1000: 2.2446541786193848\n",
            "Train loss 428/1000: 2.244535207748413\n",
            "Train loss 429/1000: 2.2444167137145996\n",
            "Train loss 430/1000: 2.244298219680786\n",
            "Train loss 431/1000: 2.244180917739868\n",
            "Train loss 432/1000: 2.24406361579895\n",
            "Train loss 433/1000: 2.2439470291137695\n",
            "Train loss 434/1000: 2.243831157684326\n",
            "Train loss 435/1000: 2.243715524673462\n",
            "Train loss 436/1000: 2.2436001300811768\n",
            "Train loss 437/1000: 2.243485689163208\n",
            "Train loss 438/1000: 2.2433714866638184\n",
            "Train loss 439/1000: 2.243257999420166\n",
            "Train loss 440/1000: 2.2431445121765137\n",
            "Train loss 441/1000: 2.2430317401885986\n",
            "Train loss 442/1000: 2.242919921875\n",
            "Train loss 443/1000: 2.2428078651428223\n",
            "Train loss 444/1000: 2.242696523666382\n",
            "Train loss 445/1000: 2.2425856590270996\n",
            "Train loss 446/1000: 2.2424752712249756\n",
            "Train loss 447/1000: 2.2423651218414307\n",
            "Train loss 448/1000: 2.242255687713623\n",
            "Train loss 449/1000: 2.2421467304229736\n",
            "Train loss 450/1000: 2.2420382499694824\n",
            "Train loss 451/1000: 2.2419300079345703\n",
            "Train loss 452/1000: 2.2418222427368164\n",
            "Train loss 453/1000: 2.2417149543762207\n",
            "Train loss 454/1000: 2.241608142852783\n",
            "Train loss 455/1000: 2.241501569747925\n",
            "Train loss 456/1000: 2.2413957118988037\n",
            "Train loss 457/1000: 2.2412900924682617\n",
            "Train loss 458/1000: 2.241184949874878\n",
            "Train loss 459/1000: 2.2410802841186523\n",
            "Train loss 460/1000: 2.240975856781006\n",
            "Train loss 461/1000: 2.2408721446990967\n",
            "Train loss 462/1000: 2.2407686710357666\n",
            "Train loss 463/1000: 2.2406654357910156\n",
            "Train loss 464/1000: 2.240562915802002\n",
            "Train loss 465/1000: 2.2404606342315674\n",
            "Train loss 466/1000: 2.240358591079712\n",
            "Train loss 467/1000: 2.2402572631835938\n",
            "Train loss 468/1000: 2.2401561737060547\n",
            "Train loss 469/1000: 2.240055799484253\n",
            "Train loss 470/1000: 2.239955425262451\n",
            "Train loss 471/1000: 2.2398555278778076\n",
            "Train loss 472/1000: 2.2397561073303223\n",
            "Train loss 473/1000: 2.239656925201416\n",
            "Train loss 474/1000: 2.239558458328247\n",
            "Train loss 475/1000: 2.239459991455078\n",
            "Train loss 476/1000: 2.2393622398376465\n",
            "Train loss 477/1000: 2.239264726638794\n",
            "Train loss 478/1000: 2.2391674518585205\n",
            "Train loss 479/1000: 2.2390706539154053\n",
            "Train loss 480/1000: 2.238974094390869\n",
            "Train loss 481/1000: 2.2388782501220703\n",
            "Train loss 482/1000: 2.2387826442718506\n",
            "Train loss 483/1000: 2.23868727684021\n",
            "Train loss 484/1000: 2.2385923862457275\n",
            "Train loss 485/1000: 2.238497734069824\n",
            "Train loss 486/1000: 2.238403558731079\n",
            "Train loss 487/1000: 2.238309621810913\n",
            "Train loss 488/1000: 2.2382164001464844\n",
            "Train loss 489/1000: 2.2381229400634766\n",
            "Train loss 490/1000: 2.238030195236206\n",
            "Train loss 491/1000: 2.2379374504089355\n",
            "Train loss 492/1000: 2.2378456592559814\n",
            "Train loss 493/1000: 2.2377536296844482\n",
            "Train loss 494/1000: 2.2376625537872314\n",
            "Train loss 495/1000: 2.2375712394714355\n",
            "Train loss 496/1000: 2.237480401992798\n",
            "Train loss 497/1000: 2.2373900413513184\n",
            "Train loss 498/1000: 2.237300157546997\n",
            "Train loss 499/1000: 2.237210512161255\n",
            "Train loss 500/1000: 2.2371208667755127\n",
            "Train loss 501/1000: 2.2370316982269287\n",
            "Train loss 502/1000: 2.236943006515503\n",
            "Train loss 503/1000: 2.2368547916412354\n",
            "Train loss 504/1000: 2.236766815185547\n",
            "Train loss 505/1000: 2.2366788387298584\n",
            "Train loss 506/1000: 2.236591339111328\n",
            "Train loss 507/1000: 2.236504077911377\n",
            "Train loss 508/1000: 2.236417531967163\n",
            "Train loss 509/1000: 2.23633074760437\n",
            "Train loss 510/1000: 2.2362446784973145\n",
            "Train loss 511/1000: 2.236158609390259\n",
            "Train loss 512/1000: 2.2360732555389404\n",
            "Train loss 513/1000: 2.235987901687622\n",
            "Train loss 514/1000: 2.235903263092041\n",
            "Train loss 515/1000: 2.235818386077881\n",
            "Train loss 516/1000: 2.235734224319458\n",
            "Train loss 517/1000: 2.235650062561035\n",
            "Train loss 518/1000: 2.2355663776397705\n",
            "Train loss 519/1000: 2.235482931137085\n",
            "Train loss 520/1000: 2.2353997230529785\n",
            "Train loss 521/1000: 2.2353169918060303\n",
            "Train loss 522/1000: 2.235234260559082\n",
            "Train loss 523/1000: 2.235152244567871\n",
            "Train loss 524/1000: 2.23507022857666\n",
            "Train loss 525/1000: 2.2349886894226074\n",
            "Train loss 526/1000: 2.2349071502685547\n",
            "Train loss 527/1000: 2.234825849533081\n",
            "Train loss 528/1000: 2.2347452640533447\n",
            "Train loss 529/1000: 2.2346646785736084\n",
            "Train loss 530/1000: 2.234584331512451\n",
            "Train loss 531/1000: 2.234504222869873\n",
            "Train loss 532/1000: 2.2344248294830322\n",
            "Train loss 533/1000: 2.2343454360961914\n",
            "Train loss 534/1000: 2.2342662811279297\n",
            "Train loss 535/1000: 2.234187126159668\n",
            "Train loss 536/1000: 2.2341089248657227\n",
            "Train loss 537/1000: 2.2340304851531982\n",
            "Train loss 538/1000: 2.233952522277832\n",
            "Train loss 539/1000: 2.233874559402466\n",
            "Train loss 540/1000: 2.233797073364258\n",
            "Train loss 541/1000: 2.233720064163208\n",
            "Train loss 542/1000: 2.233642816543579\n",
            "Train loss 543/1000: 2.2335660457611084\n",
            "Train loss 544/1000: 2.233489990234375\n",
            "Train loss 545/1000: 2.2334136962890625\n",
            "Train loss 546/1000: 2.233337640762329\n",
            "Train loss 547/1000: 2.233262062072754\n",
            "Train loss 548/1000: 2.2331864833831787\n",
            "Train loss 549/1000: 2.2331113815307617\n",
            "Train loss 550/1000: 2.2330362796783447\n",
            "Train loss 551/1000: 2.232961654663086\n",
            "Train loss 552/1000: 2.2328875064849854\n",
            "Train loss 553/1000: 2.2328131198883057\n",
            "Train loss 554/1000: 2.232739210128784\n",
            "Train loss 555/1000: 2.232665777206421\n",
            "Train loss 556/1000: 2.2325921058654785\n",
            "Train loss 557/1000: 2.2325191497802734\n",
            "Train loss 558/1000: 2.2324459552764893\n",
            "Train loss 559/1000: 2.2323734760284424\n",
            "Train loss 560/1000: 2.2323009967803955\n",
            "Train loss 561/1000: 2.2322287559509277\n",
            "Train loss 562/1000: 2.232156753540039\n",
            "Train loss 563/1000: 2.2320849895477295\n",
            "Train loss 564/1000: 2.232013463973999\n",
            "Train loss 565/1000: 2.2319424152374268\n",
            "Train loss 566/1000: 2.2318713665008545\n",
            "Train loss 567/1000: 2.2318005561828613\n",
            "Train loss 568/1000: 2.2317299842834473\n",
            "Train loss 569/1000: 2.2316596508026123\n",
            "Train loss 570/1000: 2.2315895557403564\n",
            "Train loss 571/1000: 2.2315194606781006\n",
            "Train loss 572/1000: 2.231449842453003\n",
            "Train loss 573/1000: 2.2313804626464844\n",
            "Train loss 574/1000: 2.231311559677124\n",
            "Train loss 575/1000: 2.2312426567077637\n",
            "Train loss 576/1000: 2.2311737537384033\n",
            "Train loss 577/1000: 2.231105089187622\n",
            "Train loss 578/1000: 2.231037139892578\n",
            "Train loss 579/1000: 2.230968952178955\n",
            "Train loss 580/1000: 2.230901002883911\n",
            "Train loss 581/1000: 2.2308332920074463\n",
            "Train loss 582/1000: 2.2307658195495605\n",
            "Train loss 583/1000: 2.230698823928833\n",
            "Train loss 584/1000: 2.2306315898895264\n",
            "Train loss 585/1000: 2.230565071105957\n",
            "Train loss 586/1000: 2.2304983139038086\n",
            "Train loss 587/1000: 2.2304320335388184\n",
            "Train loss 588/1000: 2.2303659915924072\n",
            "Train loss 589/1000: 2.230299949645996\n",
            "Train loss 590/1000: 2.230234384536743\n",
            "Train loss 591/1000: 2.230168581008911\n",
            "Train loss 592/1000: 2.2301032543182373\n",
            "Train loss 593/1000: 2.2300381660461426\n",
            "Train loss 594/1000: 2.229973316192627\n",
            "Train loss 595/1000: 2.2299087047576904\n",
            "Train loss 596/1000: 2.229844093322754\n",
            "Train loss 597/1000: 2.2297797203063965\n",
            "Train loss 598/1000: 2.229715585708618\n",
            "Train loss 599/1000: 2.229651927947998\n",
            "Train loss 600/1000: 2.229588031768799\n",
            "Train loss 601/1000: 2.229524850845337\n",
            "Train loss 602/1000: 2.229461431503296\n",
            "Train loss 603/1000: 2.229398250579834\n",
            "Train loss 604/1000: 2.2293355464935303\n",
            "Train loss 605/1000: 2.2292728424072266\n",
            "Train loss 606/1000: 2.229210138320923\n",
            "Train loss 607/1000: 2.2291476726531982\n",
            "Train loss 608/1000: 2.229085683822632\n",
            "Train loss 609/1000: 2.2290236949920654\n",
            "Train loss 610/1000: 2.228961944580078\n",
            "Train loss 611/1000: 2.228900194168091\n",
            "Train loss 612/1000: 2.2288389205932617\n",
            "Train loss 613/1000: 2.2287776470184326\n",
            "Train loss 614/1000: 2.2287168502807617\n",
            "Train loss 615/1000: 2.228656053543091\n",
            "Train loss 616/1000: 2.228595495223999\n",
            "Train loss 617/1000: 2.2285349369049072\n",
            "Train loss 618/1000: 2.2284746170043945\n",
            "Train loss 619/1000: 2.228414535522461\n",
            "Train loss 620/1000: 2.2283546924591064\n",
            "Train loss 621/1000: 2.228294849395752\n",
            "Train loss 622/1000: 2.2282352447509766\n",
            "Train loss 623/1000: 2.2281758785247803\n",
            "Train loss 624/1000: 2.228116750717163\n",
            "Train loss 625/1000: 2.228057622909546\n",
            "Train loss 626/1000: 2.227998971939087\n",
            "Train loss 627/1000: 2.227940082550049\n",
            "Train loss 628/1000: 2.227881669998169\n",
            "Train loss 629/1000: 2.227823495864868\n",
            "Train loss 630/1000: 2.2277653217315674\n",
            "Train loss 631/1000: 2.2277073860168457\n",
            "Train loss 632/1000: 2.227649688720703\n",
            "Train loss 633/1000: 2.2275917530059814\n",
            "Train loss 634/1000: 2.227534294128418\n",
            "Train loss 635/1000: 2.2274770736694336\n",
            "Train loss 636/1000: 2.2274200916290283\n",
            "Train loss 637/1000: 2.227363109588623\n",
            "Train loss 638/1000: 2.2273061275482178\n",
            "Train loss 639/1000: 2.2272493839263916\n",
            "Train loss 640/1000: 2.2271931171417236\n",
            "Train loss 641/1000: 2.2271368503570557\n",
            "Train loss 642/1000: 2.227080821990967\n",
            "Train loss 643/1000: 2.227024793624878\n",
            "Train loss 644/1000: 2.226969003677368\n",
            "Train loss 645/1000: 2.2269132137298584\n",
            "Train loss 646/1000: 2.226857900619507\n",
            "Train loss 647/1000: 2.226802349090576\n",
            "Train loss 648/1000: 2.2267472743988037\n",
            "Train loss 649/1000: 2.2266921997070312\n",
            "Train loss 650/1000: 2.226637601852417\n",
            "Train loss 651/1000: 2.2265827655792236\n",
            "Train loss 652/1000: 2.2265284061431885\n",
            "Train loss 653/1000: 2.2264740467071533\n",
            "Train loss 654/1000: 2.2264199256896973\n",
            "Train loss 655/1000: 2.226365804672241\n",
            "Train loss 656/1000: 2.226311683654785\n",
            "Train loss 657/1000: 2.2262580394744873\n",
            "Train loss 658/1000: 2.2262046337127686\n",
            "Train loss 659/1000: 2.2261507511138916\n",
            "Train loss 660/1000: 2.226097583770752\n",
            "Train loss 661/1000: 2.2260444164276123\n",
            "Train loss 662/1000: 2.225991725921631\n",
            "Train loss 663/1000: 2.225938558578491\n",
            "Train loss 664/1000: 2.225886106491089\n",
            "Train loss 665/1000: 2.2258334159851074\n",
            "Train loss 666/1000: 2.225781202316284\n",
            "Train loss 667/1000: 2.225728988647461\n",
            "Train loss 668/1000: 2.2256767749786377\n",
            "Train loss 669/1000: 2.2256247997283936\n",
            "Train loss 670/1000: 2.2255728244781494\n",
            "Train loss 671/1000: 2.2255213260650635\n",
            "Train loss 672/1000: 2.2254695892333984\n",
            "Train loss 673/1000: 2.2254183292388916\n",
            "Train loss 674/1000: 2.225367307662964\n",
            "Train loss 675/1000: 2.225316047668457\n",
            "Train loss 676/1000: 2.2252652645111084\n",
            "Train loss 677/1000: 2.2252142429351807\n",
            "Train loss 678/1000: 2.225163698196411\n",
            "Train loss 679/1000: 2.2251131534576416\n",
            "Train loss 680/1000: 2.225062608718872\n",
            "Train loss 681/1000: 2.2250125408172607\n",
            "Train loss 682/1000: 2.2249622344970703\n",
            "Train loss 683/1000: 2.224912405014038\n",
            "Train loss 684/1000: 2.2248623371124268\n",
            "Train loss 685/1000: 2.2248129844665527\n",
            "Train loss 686/1000: 2.2247633934020996\n",
            "Train loss 687/1000: 2.2247138023376465\n",
            "Train loss 688/1000: 2.2246644496917725\n",
            "Train loss 689/1000: 2.2246153354644775\n",
            "Train loss 690/1000: 2.2245662212371826\n",
            "Train loss 691/1000: 2.224517345428467\n",
            "Train loss 692/1000: 2.22446870803833\n",
            "Train loss 693/1000: 2.2244200706481934\n",
            "Train loss 694/1000: 2.2243714332580566\n",
            "Train loss 695/1000: 2.224323034286499\n",
            "Train loss 696/1000: 2.2242746353149414\n",
            "Train loss 697/1000: 2.224226713180542\n",
            "Train loss 698/1000: 2.2241787910461426\n",
            "Train loss 699/1000: 2.2241311073303223\n",
            "Train loss 700/1000: 2.224083185195923\n",
            "Train loss 701/1000: 2.2240355014801025\n",
            "Train loss 702/1000: 2.2239880561828613\n",
            "Train loss 703/1000: 2.223940849304199\n",
            "Train loss 704/1000: 2.223893642425537\n",
            "Train loss 705/1000: 2.223846435546875\n",
            "Train loss 706/1000: 2.223799467086792\n",
            "Train loss 707/1000: 2.223752737045288\n",
            "Train loss 708/1000: 2.223706007003784\n",
            "Train loss 709/1000: 2.2236595153808594\n",
            "Train loss 710/1000: 2.2236127853393555\n",
            "Train loss 711/1000: 2.2235665321350098\n",
            "Train loss 712/1000: 2.223520278930664\n",
            "Train loss 713/1000: 2.2234740257263184\n",
            "Train loss 714/1000: 2.223428249359131\n",
            "Train loss 715/1000: 2.2233822345733643\n",
            "Train loss 716/1000: 2.2233364582061768\n",
            "Train loss 717/1000: 2.2232909202575684\n",
            "Train loss 718/1000: 2.22324538230896\n",
            "Train loss 719/1000: 2.2232003211975098\n",
            "Train loss 720/1000: 2.2231547832489014\n",
            "Train loss 721/1000: 2.223109483718872\n",
            "Train loss 722/1000: 2.223064661026001\n",
            "Train loss 723/1000: 2.22301983833313\n",
            "Train loss 724/1000: 2.222975015640259\n",
            "Train loss 725/1000: 2.2229301929473877\n",
            "Train loss 726/1000: 2.2228856086730957\n",
            "Train loss 727/1000: 2.222841262817383\n",
            "Train loss 728/1000: 2.22279691696167\n",
            "Train loss 729/1000: 2.222752571105957\n",
            "Train loss 730/1000: 2.2227084636688232\n",
            "Train loss 731/1000: 2.2226643562316895\n",
            "Train loss 732/1000: 2.2226204872131348\n",
            "Train loss 733/1000: 2.222576856613159\n",
            "Train loss 734/1000: 2.2225332260131836\n",
            "Train loss 735/1000: 2.222489595413208\n",
            "Train loss 736/1000: 2.2224464416503906\n",
            "Train loss 737/1000: 2.222403049468994\n",
            "Train loss 738/1000: 2.2223596572875977\n",
            "Train loss 739/1000: 2.2223167419433594\n",
            "Train loss 740/1000: 2.222273588180542\n",
            "Train loss 741/1000: 2.2222306728363037\n",
            "Train loss 742/1000: 2.2221879959106445\n",
            "Train loss 743/1000: 2.2221453189849854\n",
            "Train loss 744/1000: 2.222102642059326\n",
            "Train loss 745/1000: 2.222060203552246\n",
            "Train loss 746/1000: 2.222018003463745\n",
            "Train loss 747/1000: 2.221975803375244\n",
            "Train loss 748/1000: 2.221933603286743\n",
            "Train loss 749/1000: 2.221891403198242\n",
            "Train loss 750/1000: 2.2218494415283203\n",
            "Train loss 751/1000: 2.2218077182769775\n",
            "Train loss 752/1000: 2.221766233444214\n",
            "Train loss 753/1000: 2.221724271774292\n",
            "Train loss 754/1000: 2.2216827869415283\n",
            "Train loss 755/1000: 2.2216415405273438\n",
            "Train loss 756/1000: 2.22160005569458\n",
            "Train loss 757/1000: 2.2215590476989746\n",
            "Train loss 758/1000: 2.221518039703369\n",
            "Train loss 759/1000: 2.2214770317077637\n",
            "Train loss 760/1000: 2.221436023712158\n",
            "Train loss 761/1000: 2.2213950157165527\n",
            "Train loss 762/1000: 2.2213544845581055\n",
            "Train loss 763/1000: 2.221313953399658\n",
            "Train loss 764/1000: 2.221273422241211\n",
            "Train loss 765/1000: 2.2212331295013428\n",
            "Train loss 766/1000: 2.2211928367614746\n",
            "Train loss 767/1000: 2.2211525440216064\n",
            "Train loss 768/1000: 2.2211124897003174\n",
            "Train loss 769/1000: 2.2210724353790283\n",
            "Train loss 770/1000: 2.22103214263916\n",
            "Train loss 771/1000: 2.2209928035736084\n",
            "Train loss 772/1000: 2.2209527492523193\n",
            "Train loss 773/1000: 2.2209134101867676\n",
            "Train loss 774/1000: 2.2208738327026367\n",
            "Train loss 775/1000: 2.220834493637085\n",
            "Train loss 776/1000: 2.220794916152954\n",
            "Train loss 777/1000: 2.2207560539245605\n",
            "Train loss 778/1000: 2.220716714859009\n",
            "Train loss 779/1000: 2.220677614212036\n",
            "Train loss 780/1000: 2.2206387519836426\n",
            "Train loss 781/1000: 2.220599889755249\n",
            "Train loss 782/1000: 2.2205610275268555\n",
            "Train loss 783/1000: 2.22052264213562\n",
            "Train loss 784/1000: 2.2204837799072266\n",
            "Train loss 785/1000: 2.220445394515991\n",
            "Train loss 786/1000: 2.2204067707061768\n",
            "Train loss 787/1000: 2.2203688621520996\n",
            "Train loss 788/1000: 2.2203304767608643\n",
            "Train loss 789/1000: 2.220292091369629\n",
            "Train loss 790/1000: 2.2202541828155518\n",
            "Train loss 791/1000: 2.2202162742614746\n",
            "Train loss 792/1000: 2.2201786041259766\n",
            "Train loss 793/1000: 2.2201409339904785\n",
            "Train loss 794/1000: 2.2201030254364014\n",
            "Train loss 795/1000: 2.2200655937194824\n",
            "Train loss 796/1000: 2.2200279235839844\n",
            "Train loss 797/1000: 2.2199904918670654\n",
            "Train loss 798/1000: 2.2199535369873047\n",
            "Train loss 799/1000: 2.2199161052703857\n",
            "Train loss 800/1000: 2.219878911972046\n",
            "Train loss 801/1000: 2.219841957092285\n",
            "Train loss 802/1000: 2.2198050022125244\n",
            "Train loss 803/1000: 2.2197680473327637\n",
            "Train loss 804/1000: 2.219731330871582\n",
            "Train loss 805/1000: 2.2196943759918213\n",
            "Train loss 806/1000: 2.2196578979492188\n",
            "Train loss 807/1000: 2.219621419906616\n",
            "Train loss 808/1000: 2.2195849418640137\n",
            "Train loss 809/1000: 2.219548463821411\n",
            "Train loss 810/1000: 2.219512462615967\n",
            "Train loss 811/1000: 2.2194762229919434\n",
            "Train loss 812/1000: 2.219439744949341\n",
            "Train loss 813/1000: 2.2194039821624756\n",
            "Train loss 814/1000: 2.219367742538452\n",
            "Train loss 815/1000: 2.219332218170166\n",
            "Train loss 816/1000: 2.2192962169647217\n",
            "Train loss 817/1000: 2.2192606925964355\n",
            "Train loss 818/1000: 2.2192249298095703\n",
            "Train loss 819/1000: 2.219189405441284\n",
            "Train loss 820/1000: 2.219153881072998\n",
            "Train loss 821/1000: 2.21911883354187\n",
            "Train loss 822/1000: 2.219083309173584\n",
            "Train loss 823/1000: 2.219048023223877\n",
            "Train loss 824/1000: 2.219012975692749\n",
            "Train loss 825/1000: 2.218977928161621\n",
            "Train loss 826/1000: 2.218942880630493\n",
            "Train loss 827/1000: 2.2189080715179443\n",
            "Train loss 828/1000: 2.2188730239868164\n",
            "Train loss 829/1000: 2.2188382148742676\n",
            "Train loss 830/1000: 2.218803644180298\n",
            "Train loss 831/1000: 2.2187693119049072\n",
            "Train loss 832/1000: 2.2187347412109375\n",
            "Train loss 833/1000: 2.2187001705169678\n",
            "Train loss 834/1000: 2.218665838241577\n",
            "Train loss 835/1000: 2.2186317443847656\n",
            "Train loss 836/1000: 2.218597412109375\n",
            "Train loss 837/1000: 2.2185635566711426\n",
            "Train loss 838/1000: 2.218529224395752\n",
            "Train loss 839/1000: 2.2184953689575195\n",
            "Train loss 840/1000: 2.218461513519287\n",
            "Train loss 841/1000: 2.2184276580810547\n",
            "Train loss 842/1000: 2.2183938026428223\n",
            "Train loss 843/1000: 2.218360424041748\n",
            "Train loss 844/1000: 2.2183265686035156\n",
            "Train loss 845/1000: 2.2182929515838623\n",
            "Train loss 846/1000: 2.218259572982788\n",
            "Train loss 847/1000: 2.218226194381714\n",
            "Train loss 848/1000: 2.2181928157806396\n",
            "Train loss 849/1000: 2.2181594371795654\n",
            "Train loss 850/1000: 2.2181265354156494\n",
            "Train loss 851/1000: 2.2180936336517334\n",
            "Train loss 852/1000: 2.2180604934692383\n",
            "Train loss 853/1000: 2.2180275917053223\n",
            "Train loss 854/1000: 2.2179946899414062\n",
            "Train loss 855/1000: 2.2179617881774902\n",
            "Train loss 856/1000: 2.2179293632507324\n",
            "Train loss 857/1000: 2.2178966999053955\n",
            "Train loss 858/1000: 2.2178640365600586\n",
            "Train loss 859/1000: 2.2178313732147217\n",
            "Train loss 860/1000: 2.217799186706543\n",
            "Train loss 861/1000: 2.217766523361206\n",
            "Train loss 862/1000: 2.2177345752716064\n",
            "Train loss 863/1000: 2.2177021503448486\n",
            "Train loss 864/1000: 2.21766996383667\n",
            "Train loss 865/1000: 2.2176380157470703\n",
            "Train loss 866/1000: 2.2176060676574707\n",
            "Train loss 867/1000: 2.217574119567871\n",
            "Train loss 868/1000: 2.2175424098968506\n",
            "Train loss 869/1000: 2.217510223388672\n",
            "Train loss 870/1000: 2.2174785137176514\n",
            "Train loss 871/1000: 2.217446804046631\n",
            "Train loss 872/1000: 2.2174153327941895\n",
            "Train loss 873/1000: 2.217383623123169\n",
            "Train loss 874/1000: 2.2173523902893066\n",
            "Train loss 875/1000: 2.2173211574554443\n",
            "Train loss 876/1000: 2.217289686203003\n",
            "Train loss 877/1000: 2.2172584533691406\n",
            "Train loss 878/1000: 2.2172272205352783\n",
            "Train loss 879/1000: 2.217195749282837\n",
            "Train loss 880/1000: 2.2171647548675537\n",
            "Train loss 881/1000: 2.2171337604522705\n",
            "Train loss 882/1000: 2.2171030044555664\n",
            "Train loss 883/1000: 2.217072010040283\n",
            "Train loss 884/1000: 2.217041015625\n",
            "Train loss 885/1000: 2.217010736465454\n",
            "Train loss 886/1000: 2.21697998046875\n",
            "Train loss 887/1000: 2.216949462890625\n",
            "Train loss 888/1000: 2.216918706893921\n",
            "Train loss 889/1000: 2.216888427734375\n",
            "Train loss 890/1000: 2.21685791015625\n",
            "Train loss 891/1000: 2.216827392578125\n",
            "Train loss 892/1000: 2.216797351837158\n",
            "Train loss 893/1000: 2.2167670726776123\n",
            "Train loss 894/1000: 2.2167367935180664\n",
            "Train loss 895/1000: 2.2167069911956787\n",
            "Train loss 896/1000: 2.216676950454712\n",
            "Train loss 897/1000: 2.216646671295166\n",
            "Train loss 898/1000: 2.2166168689727783\n",
            "Train loss 899/1000: 2.2165868282318115\n",
            "Train loss 900/1000: 2.216557025909424\n",
            "Train loss 901/1000: 2.216527223587036\n",
            "Train loss 902/1000: 2.2164978981018066\n",
            "Train loss 903/1000: 2.216468334197998\n",
            "Train loss 904/1000: 2.2164385318756104\n",
            "Train loss 905/1000: 2.216409206390381\n",
            "Train loss 906/1000: 2.2163798809051514\n",
            "Train loss 907/1000: 2.2163503170013428\n",
            "Train loss 908/1000: 2.2163212299346924\n",
            "Train loss 909/1000: 2.216291904449463\n",
            "Train loss 910/1000: 2.2162625789642334\n",
            "Train loss 911/1000: 2.216233730316162\n",
            "Train loss 912/1000: 2.2162044048309326\n",
            "Train loss 913/1000: 2.2161755561828613\n",
            "Train loss 914/1000: 2.21614670753479\n",
            "Train loss 915/1000: 2.2161176204681396\n",
            "Train loss 916/1000: 2.2160887718200684\n",
            "Train loss 917/1000: 2.216060161590576\n",
            "Train loss 918/1000: 2.216031312942505\n",
            "Train loss 919/1000: 2.2160027027130127\n",
            "Train loss 920/1000: 2.2159743309020996\n",
            "Train loss 921/1000: 2.2159454822540283\n",
            "Train loss 922/1000: 2.2159173488616943\n",
            "Train loss 923/1000: 2.215888738632202\n",
            "Train loss 924/1000: 2.215860366821289\n",
            "Train loss 925/1000: 2.215832233428955\n",
            "Train loss 926/1000: 2.215803861618042\n",
            "Train loss 927/1000: 2.215775728225708\n",
            "Train loss 928/1000: 2.215747594833374\n",
            "Train loss 929/1000: 2.21571946144104\n",
            "Train loss 930/1000: 2.215691328048706\n",
            "Train loss 931/1000: 2.2156636714935303\n",
            "Train loss 932/1000: 2.2156355381011963\n",
            "Train loss 933/1000: 2.2156078815460205\n",
            "Train loss 934/1000: 2.2155799865722656\n",
            "Train loss 935/1000: 2.21555233001709\n",
            "Train loss 936/1000: 2.215524435043335\n",
            "Train loss 937/1000: 2.215496778488159\n",
            "Train loss 938/1000: 2.2154693603515625\n",
            "Train loss 939/1000: 2.2154417037963867\n",
            "Train loss 940/1000: 2.215414524078369\n",
            "Train loss 941/1000: 2.2153866291046143\n",
            "Train loss 942/1000: 2.215359687805176\n",
            "Train loss 943/1000: 2.215332269668579\n",
            "Train loss 944/1000: 2.2153050899505615\n",
            "Train loss 945/1000: 2.215277910232544\n",
            "Train loss 946/1000: 2.2152507305145264\n",
            "Train loss 947/1000: 2.215223550796509\n",
            "Train loss 948/1000: 2.2151966094970703\n",
            "Train loss 949/1000: 2.215169668197632\n",
            "Train loss 950/1000: 2.2151429653167725\n",
            "Train loss 951/1000: 2.215115785598755\n",
            "Train loss 952/1000: 2.2150890827178955\n",
            "Train loss 953/1000: 2.215062379837036\n",
            "Train loss 954/1000: 2.2150354385375977\n",
            "Train loss 955/1000: 2.2150089740753174\n",
            "Train loss 956/1000: 2.214982509613037\n",
            "Train loss 957/1000: 2.2149558067321777\n",
            "Train loss 958/1000: 2.2149288654327393\n",
            "Train loss 959/1000: 2.214902639389038\n",
            "Train loss 960/1000: 2.214876413345337\n",
            "Train loss 961/1000: 2.2148501873016357\n",
            "Train loss 962/1000: 2.2148237228393555\n",
            "Train loss 963/1000: 2.2147974967956543\n",
            "Train loss 964/1000: 2.214771270751953\n",
            "Train loss 965/1000: 2.214745044708252\n",
            "Train loss 966/1000: 2.21471905708313\n",
            "Train loss 967/1000: 2.214693069458008\n",
            "Train loss 968/1000: 2.2146668434143066\n",
            "Train loss 969/1000: 2.2146410942077637\n",
            "Train loss 970/1000: 2.2146151065826416\n",
            "Train loss 971/1000: 2.2145893573760986\n",
            "Train loss 972/1000: 2.2145633697509766\n",
            "Train loss 973/1000: 2.2145378589630127\n",
            "Train loss 974/1000: 2.2145118713378906\n",
            "Train loss 975/1000: 2.2144863605499268\n",
            "Train loss 976/1000: 2.214460849761963\n",
            "Train loss 977/1000: 2.21443510055542\n",
            "Train loss 978/1000: 2.214409589767456\n",
            "Train loss 979/1000: 2.2143843173980713\n",
            "Train loss 980/1000: 2.2143588066101074\n",
            "Train loss 981/1000: 2.2143335342407227\n",
            "Train loss 982/1000: 2.214308261871338\n",
            "Train loss 983/1000: 2.214282989501953\n",
            "Train loss 984/1000: 2.2142577171325684\n",
            "Train loss 985/1000: 2.2142324447631836\n",
            "Train loss 986/1000: 2.214207410812378\n",
            "Train loss 987/1000: 2.214182138442993\n",
            "Train loss 988/1000: 2.2141573429107666\n",
            "Train loss 989/1000: 2.214132308959961\n",
            "Train loss 990/1000: 2.2141072750091553\n",
            "Train loss 991/1000: 2.2140822410583496\n",
            "Train loss 992/1000: 2.214057683944702\n",
            "Train loss 993/1000: 2.2140326499938965\n",
            "Train loss 994/1000: 2.214008092880249\n",
            "Train loss 995/1000: 2.2139832973480225\n",
            "Train loss 996/1000: 2.213958740234375\n",
            "Train loss 997/1000: 2.2139339447021484\n",
            "Train loss 998/1000: 2.213909387588501\n",
            "Train loss 999/1000: 2.2138850688934326\n",
            "Train loss 1000/1000: 2.213860511779785\n",
            "junide\n",
            "janasid\n",
            "prelay\n",
            "adin\n",
            "kairritoper\n",
            "sathen\n",
            "sameia\n",
            "yanileniassibduinewin\n",
            "lessiyanayla\n",
            "te\n",
            "farmanthyfortumj\n",
            "ponn\n",
            "lena\n",
            "jaylicore\n",
            "ya\n",
            "jocken\n",
            "jamilyn\n",
            "korin\n",
            "wyn\n",
            "ne\n",
            "gaasnhavi\n",
            "monszxhxdgor\n",
            "mathani\n",
            "zie\n",
            "paun\n",
            "ty\n",
            "tin\n",
            "sreli\n",
            "ish\n",
            "dyn\n",
            "rumikujcmkhaubwyla\n",
            "kha\n",
            "cra\n",
            "raydnhwadorta\n",
            "malyn\n",
            "brey\n",
            "aur\n",
            "lavarock\n",
            "themiraya\n",
            "ath\n",
            "basely\n",
            "tavisonikiyaalee\n",
            "marlen\n",
            "em\n",
            "fabethellianten\n",
            "chan\n",
            "jazaody\n",
            "drd\n",
            "johialiypjrgia\n",
            "tezrwaylia\n",
            "vywhqelvani\n",
            "sahimah\n",
            "kellette\n",
            "braceodon\n",
            "ali\n",
            "alian\n",
            "denn\n",
            "jayannyah\n",
            "kennelyn\n",
            "marianner\n",
            "samotan\n",
            "kyroderihana\n",
            "shday\n",
            "ta\n",
            "olleah\n",
            "tertte\n",
            "keus\n",
            "dasia\n",
            "na\n",
            "chlynevini\n",
            "aspie\n",
            "lealilondral\n",
            "fanmari\n",
            "mishama\n",
            "verykengeon\n",
            "resynivion\n",
            "uzien\n",
            "jalivyah\n",
            "alto\n",
            "marafelanaylian\n",
            "ohanorrisyli\n",
            "mson\n",
            "jorayiangkxdvjypnfria\n",
            "maulil\n",
            "cacvjveer\n",
            "ael\n",
            "shirla\n",
            "sa\n",
            "pre\n",
            "azaikeri\n",
            "khanna\n",
            "abbiha\n",
            "isa\n",
            "brailyond\n",
            "bradeelyn\n",
            "zyarierri\n",
            "chamadayda\n",
            "fbduqayceeyton\n",
            "za\n",
            "sabdin\n",
            "han\n",
            "kennasslen\n",
            "conik\n",
            "ny\n",
            "naaston\n",
            "tisona\n",
            "isemrtellwes\n",
            "aleissarekam\n",
            "ljaykjmhiffaresiyah\n",
            "lula\n",
            "jus\n",
            "b\n",
            "den\n",
            "hawyqsum\n",
            "ozie\n",
            "hasiam\n",
            "na\n",
            "adiamordelamiquadilah\n",
            "an\n",
            "keika\n",
            "viyah\n",
            "ax\n",
            "mandt\n",
            "tyjamanalingh\n",
            "jepekala\n",
            "koberishurays\n",
            "khin\n",
            "haud\n",
            "wcalipqkwel\n",
            "chieann\n",
            "madiah\n",
            "pasdtianancs\n",
            "cobzmarodiva\n",
            "myah\n",
            "kaws\n",
            "eva\n",
            "rie\n",
            "laydeni\n",
            "dasiradinie\n",
            "cora\n",
            "aulynnah\n",
            "dever\n",
            "shen\n",
            "jo\n",
            "esten\n",
            "die\n",
            "alaciordeliskyann\n",
            "ala\n",
            "der\n",
            "hambenda\n",
            "dale\n",
            "aansharalarynehmiybrashozirael\n",
            "ta\n",
            "ix\n",
            "na\n",
            "ek\n",
            "maryena\n",
            "kemerytle\n",
            "kacqueelqexen\n",
            "caliavan\n",
            "ann\n",
            "inyohasyn\n",
            "frey\n",
            "an\n",
            "ya\n",
            "laydena\n",
            "bronidavicopkrniella\n",
            "evkyreigen\n",
            "amaylee\n",
            "tala\n",
            "laton\n",
            "gialiah\n",
            "joyarisen\n",
            "kamela\n",
            "la\n",
            "shoselynna\n",
            "ib\n",
            "sundrenn\n",
            "qsia\n",
            "mince\n",
            "marmileyah\n",
            "anus\n",
            "redon\n",
            "kilapxzdfvdriell\n",
            "luozel\n",
            "hanady\n",
            "woordi\n",
            "iksh\n",
            "delaitonnah\n",
            "zylah\n",
            "trayn\n",
            "ha\n",
            "ron\n",
            "alayly\n",
            "gvkvekhtaerri\n",
            "eloveriel\n",
            "chnu\n",
            "brah\n",
            "sia\n",
            "aann\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Divide the dataset into smallet sets & train only on the train set"
      ],
      "metadata": {
        "id": "-YKsd1211-i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the inputs and target into train set, dev set, and test set\n",
        "train_split = int(0.8 * count)\n",
        "dev_split = int(0.9 * count)\n",
        "train_inputs, train_targets = inputs[:train_split], targets[:train_split]\n",
        "dev_inputs, dev_targets = inputs[train_split : dev_split], targets[train_split : dev_split]\n",
        "test_inputs, test_targets = inputs[dev_split :], targets[dev_split :]\n",
        "\n",
        "# test the division\n",
        "train_inputs.shape[0] + dev_inputs.shape[0] + test_inputs.shape[0] == inputs.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBxSgD2Y2Itt",
        "outputId": "c64942c2-a2d1-4de3-9f3a-7af0defa4e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model on the train set, see how it goes with the dev set too\n",
        "train_weights = train_model(train_inputs, train_targets)\n",
        "sample_names(train_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vbH9GQvn4LR2",
        "outputId": "35b9e374-7206-4bec-9fb5-0750c1dcd1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 1/1000: 3.7902348041534424\n",
            "Train loss 2/1000: 3.522183895111084\n",
            "Train loss 3/1000: 3.3927693367004395\n",
            "Train loss 4/1000: 3.29388689994812\n",
            "Train loss 5/1000: 3.212381362915039\n",
            "Train loss 6/1000: 3.1441524028778076\n",
            "Train loss 7/1000: 3.085684299468994\n",
            "Train loss 8/1000: 3.0348353385925293\n",
            "Train loss 9/1000: 2.990046739578247\n",
            "Train loss 10/1000: 2.950261116027832\n",
            "Train loss 11/1000: 2.9146273136138916\n",
            "Train loss 12/1000: 2.8825061321258545\n",
            "Train loss 13/1000: 2.853383779525757\n",
            "Train loss 14/1000: 2.8268635272979736\n",
            "Train loss 15/1000: 2.8026211261749268\n",
            "Train loss 16/1000: 2.7803895473480225\n",
            "Train loss 17/1000: 2.7599384784698486\n",
            "Train loss 18/1000: 2.7410669326782227\n",
            "Train loss 19/1000: 2.723595142364502\n",
            "Train loss 20/1000: 2.707364797592163\n",
            "Train loss 21/1000: 2.692234516143799\n",
            "Train loss 22/1000: 2.6780827045440674\n",
            "Train loss 23/1000: 2.6648027896881104\n",
            "Train loss 24/1000: 2.652304172515869\n",
            "Train loss 25/1000: 2.640507936477661\n",
            "Train loss 26/1000: 2.629347085952759\n",
            "Train loss 27/1000: 2.6187641620635986\n",
            "Train loss 28/1000: 2.608708381652832\n",
            "Train loss 29/1000: 2.599135160446167\n",
            "Train loss 30/1000: 2.5900065898895264\n",
            "Train loss 31/1000: 2.5812880992889404\n",
            "Train loss 32/1000: 2.572949171066284\n",
            "Train loss 33/1000: 2.5649616718292236\n",
            "Train loss 34/1000: 2.557302236557007\n",
            "Train loss 35/1000: 2.5499472618103027\n",
            "Train loss 36/1000: 2.542877674102783\n",
            "Train loss 37/1000: 2.5360751152038574\n",
            "Train loss 38/1000: 2.5295231342315674\n",
            "Train loss 39/1000: 2.523206949234009\n",
            "Train loss 40/1000: 2.5171127319335938\n",
            "Train loss 41/1000: 2.511228084564209\n",
            "Train loss 42/1000: 2.5055415630340576\n",
            "Train loss 43/1000: 2.5000431537628174\n",
            "Train loss 44/1000: 2.4947221279144287\n",
            "Train loss 45/1000: 2.489570379257202\n",
            "Train loss 46/1000: 2.48457932472229\n",
            "Train loss 47/1000: 2.479741096496582\n",
            "Train loss 48/1000: 2.475048780441284\n",
            "Train loss 49/1000: 2.4704957008361816\n",
            "Train loss 50/1000: 2.4660754203796387\n",
            "Train loss 51/1000: 2.4617817401885986\n",
            "Train loss 52/1000: 2.4576096534729004\n",
            "Train loss 53/1000: 2.4535536766052246\n",
            "Train loss 54/1000: 2.4496090412139893\n",
            "Train loss 55/1000: 2.445770740509033\n",
            "Train loss 56/1000: 2.44203519821167\n",
            "Train loss 57/1000: 2.4383976459503174\n",
            "Train loss 58/1000: 2.434854745864868\n",
            "Train loss 59/1000: 2.4314022064208984\n",
            "Train loss 60/1000: 2.428036689758301\n",
            "Train loss 61/1000: 2.424755573272705\n",
            "Train loss 62/1000: 2.4215548038482666\n",
            "Train loss 63/1000: 2.4184322357177734\n",
            "Train loss 64/1000: 2.415384531021118\n",
            "Train loss 65/1000: 2.4124093055725098\n",
            "Train loss 66/1000: 2.40950345993042\n",
            "Train loss 67/1000: 2.406665086746216\n",
            "Train loss 68/1000: 2.4038915634155273\n",
            "Train loss 69/1000: 2.4011809825897217\n",
            "Train loss 70/1000: 2.398531436920166\n",
            "Train loss 71/1000: 2.395939826965332\n",
            "Train loss 72/1000: 2.3934051990509033\n",
            "Train loss 73/1000: 2.390925407409668\n",
            "Train loss 74/1000: 2.388498306274414\n",
            "Train loss 75/1000: 2.386122941970825\n",
            "Train loss 76/1000: 2.3837969303131104\n",
            "Train loss 77/1000: 2.381519317626953\n",
            "Train loss 78/1000: 2.3792881965637207\n",
            "Train loss 79/1000: 2.3771021366119385\n",
            "Train loss 80/1000: 2.374959707260132\n",
            "Train loss 81/1000: 2.3728597164154053\n",
            "Train loss 82/1000: 2.3708012104034424\n",
            "Train loss 83/1000: 2.3687822818756104\n",
            "Train loss 84/1000: 2.3668019771575928\n",
            "Train loss 85/1000: 2.3648593425750732\n",
            "Train loss 86/1000: 2.3629531860351562\n",
            "Train loss 87/1000: 2.3610823154449463\n",
            "Train loss 88/1000: 2.359245777130127\n",
            "Train loss 89/1000: 2.3574423789978027\n",
            "Train loss 90/1000: 2.3556716442108154\n",
            "Train loss 91/1000: 2.3539323806762695\n",
            "Train loss 92/1000: 2.3522236347198486\n",
            "Train loss 93/1000: 2.3505444526672363\n",
            "Train loss 94/1000: 2.3488945960998535\n",
            "Train loss 95/1000: 2.3472723960876465\n",
            "Train loss 96/1000: 2.345677614212036\n",
            "Train loss 97/1000: 2.3441097736358643\n",
            "Train loss 98/1000: 2.3425676822662354\n",
            "Train loss 99/1000: 2.341050863265991\n",
            "Train loss 100/1000: 2.3395583629608154\n",
            "Train loss 101/1000: 2.33808970451355\n",
            "Train loss 102/1000: 2.3366448879241943\n",
            "Train loss 103/1000: 2.3352227210998535\n",
            "Train loss 104/1000: 2.333822011947632\n",
            "Train loss 105/1000: 2.3324434757232666\n",
            "Train loss 106/1000: 2.3310859203338623\n",
            "Train loss 107/1000: 2.3297488689422607\n",
            "Train loss 108/1000: 2.3284318447113037\n",
            "Train loss 109/1000: 2.327134609222412\n",
            "Train loss 110/1000: 2.3258559703826904\n",
            "Train loss 111/1000: 2.324596405029297\n",
            "Train loss 112/1000: 2.323354721069336\n",
            "Train loss 113/1000: 2.3221309185028076\n",
            "Train loss 114/1000: 2.320924758911133\n",
            "Train loss 115/1000: 2.319735527038574\n",
            "Train loss 116/1000: 2.3185627460479736\n",
            "Train loss 117/1000: 2.317406177520752\n",
            "Train loss 118/1000: 2.316265344619751\n",
            "Train loss 119/1000: 2.3151400089263916\n",
            "Train loss 120/1000: 2.314030408859253\n",
            "Train loss 121/1000: 2.3129351139068604\n",
            "Train loss 122/1000: 2.311854600906372\n",
            "Train loss 123/1000: 2.310788154602051\n",
            "Train loss 124/1000: 2.3097357749938965\n",
            "Train loss 125/1000: 2.308696985244751\n",
            "Train loss 126/1000: 2.307671546936035\n",
            "Train loss 127/1000: 2.306659460067749\n",
            "Train loss 128/1000: 2.3056602478027344\n",
            "Train loss 129/1000: 2.304673671722412\n",
            "Train loss 130/1000: 2.303698778152466\n",
            "Train loss 131/1000: 2.302736759185791\n",
            "Train loss 132/1000: 2.301785945892334\n",
            "Train loss 133/1000: 2.300847291946411\n",
            "Train loss 134/1000: 2.299919843673706\n",
            "Train loss 135/1000: 2.2990033626556396\n",
            "Train loss 136/1000: 2.298098087310791\n",
            "Train loss 137/1000: 2.297203779220581\n",
            "Train loss 138/1000: 2.2963197231292725\n",
            "Train loss 139/1000: 2.2954459190368652\n",
            "Train loss 140/1000: 2.2945828437805176\n",
            "Train loss 141/1000: 2.293729782104492\n",
            "Train loss 142/1000: 2.2928860187530518\n",
            "Train loss 143/1000: 2.2920522689819336\n",
            "Train loss 144/1000: 2.2912282943725586\n",
            "Train loss 145/1000: 2.2904131412506104\n",
            "Train loss 146/1000: 2.289607524871826\n",
            "Train loss 147/1000: 2.2888107299804688\n",
            "Train loss 148/1000: 2.288022756576538\n",
            "Train loss 149/1000: 2.2872438430786133\n",
            "Train loss 150/1000: 2.286472797393799\n",
            "Train loss 151/1000: 2.2857108116149902\n",
            "Train loss 152/1000: 2.284956932067871\n",
            "Train loss 153/1000: 2.2842111587524414\n",
            "Train loss 154/1000: 2.283473491668701\n",
            "Train loss 155/1000: 2.2827439308166504\n",
            "Train loss 156/1000: 2.2820215225219727\n",
            "Train loss 157/1000: 2.2813069820404053\n",
            "Train loss 158/1000: 2.2806003093719482\n",
            "Train loss 159/1000: 2.2799007892608643\n",
            "Train loss 160/1000: 2.2792086601257324\n",
            "Train loss 161/1000: 2.2785234451293945\n",
            "Train loss 162/1000: 2.2778453826904297\n",
            "Train loss 163/1000: 2.277174472808838\n",
            "Train loss 164/1000: 2.276510238647461\n",
            "Train loss 165/1000: 2.275852918624878\n",
            "Train loss 166/1000: 2.2752022743225098\n",
            "Train loss 167/1000: 2.2745580673217773\n",
            "Train loss 168/1000: 2.2739200592041016\n",
            "Train loss 169/1000: 2.2732884883880615\n",
            "Train loss 170/1000: 2.2726633548736572\n",
            "Train loss 171/1000: 2.2720444202423096\n",
            "Train loss 172/1000: 2.2714314460754395\n",
            "Train loss 173/1000: 2.270824670791626\n",
            "Train loss 174/1000: 2.27022385597229\n",
            "Train loss 175/1000: 2.2696285247802734\n",
            "Train loss 176/1000: 2.2690389156341553\n",
            "Train loss 177/1000: 2.2684550285339355\n",
            "Train loss 178/1000: 2.2678768634796143\n",
            "Train loss 179/1000: 2.2673041820526123\n",
            "Train loss 180/1000: 2.2667369842529297\n",
            "Train loss 181/1000: 2.2661752700805664\n",
            "Train loss 182/1000: 2.2656185626983643\n",
            "Train loss 183/1000: 2.2650671005249023\n",
            "Train loss 184/1000: 2.2645206451416016\n",
            "Train loss 185/1000: 2.26397967338562\n",
            "Train loss 186/1000: 2.2634432315826416\n",
            "Train loss 187/1000: 2.2629120349884033\n",
            "Train loss 188/1000: 2.262385606765747\n",
            "Train loss 189/1000: 2.2618637084960938\n",
            "Train loss 190/1000: 2.2613468170166016\n",
            "Train loss 191/1000: 2.2608346939086914\n",
            "Train loss 192/1000: 2.260327100753784\n",
            "Train loss 193/1000: 2.259823799133301\n",
            "Train loss 194/1000: 2.2593252658843994\n",
            "Train loss 195/1000: 2.258831024169922\n",
            "Train loss 196/1000: 2.2583413124084473\n",
            "Train loss 197/1000: 2.2578558921813965\n",
            "Train loss 198/1000: 2.2573745250701904\n",
            "Train loss 199/1000: 2.2568976879119873\n",
            "Train loss 200/1000: 2.25642466545105\n",
            "Train loss 201/1000: 2.255955696105957\n",
            "Train loss 202/1000: 2.255491256713867\n",
            "Train loss 203/1000: 2.255030632019043\n",
            "Train loss 204/1000: 2.2545738220214844\n",
            "Train loss 205/1000: 2.2541210651397705\n",
            "Train loss 206/1000: 2.253671884536743\n",
            "Train loss 207/1000: 2.2532267570495605\n",
            "Train loss 208/1000: 2.2527849674224854\n",
            "Train loss 209/1000: 2.252347469329834\n",
            "Train loss 210/1000: 2.251913547515869\n",
            "Train loss 211/1000: 2.251483201980591\n",
            "Train loss 212/1000: 2.251055955886841\n",
            "Train loss 213/1000: 2.2506327629089355\n",
            "Train loss 214/1000: 2.2502129077911377\n",
            "Train loss 215/1000: 2.2497963905334473\n",
            "Train loss 216/1000: 2.2493834495544434\n",
            "Train loss 217/1000: 2.2489736080169678\n",
            "Train loss 218/1000: 2.2485673427581787\n",
            "Train loss 219/1000: 2.248164653778076\n",
            "Train loss 220/1000: 2.2477643489837646\n",
            "Train loss 221/1000: 2.247368097305298\n",
            "Train loss 222/1000: 2.246974468231201\n",
            "Train loss 223/1000: 2.246584177017212\n",
            "Train loss 224/1000: 2.246196985244751\n",
            "Train loss 225/1000: 2.2458128929138184\n",
            "Train loss 226/1000: 2.245431661605835\n",
            "Train loss 227/1000: 2.24505352973938\n",
            "Train loss 228/1000: 2.244678497314453\n",
            "Train loss 229/1000: 2.2443060874938965\n",
            "Train loss 230/1000: 2.243936538696289\n",
            "Train loss 231/1000: 2.24357008934021\n",
            "Train loss 232/1000: 2.24320650100708\n",
            "Train loss 233/1000: 2.2428455352783203\n",
            "Train loss 234/1000: 2.2424874305725098\n",
            "Train loss 235/1000: 2.2421319484710693\n",
            "Train loss 236/1000: 2.241779088973999\n",
            "Train loss 237/1000: 2.241429090499878\n",
            "Train loss 238/1000: 2.241081476211548\n",
            "Train loss 239/1000: 2.240736484527588\n",
            "Train loss 240/1000: 2.240394353866577\n",
            "Train loss 241/1000: 2.2400543689727783\n",
            "Train loss 242/1000: 2.2397172451019287\n",
            "Train loss 243/1000: 2.23938250541687\n",
            "Train loss 244/1000: 2.2390501499176025\n",
            "Train loss 245/1000: 2.238720417022705\n",
            "Train loss 246/1000: 2.2383928298950195\n",
            "Train loss 247/1000: 2.238067865371704\n",
            "Train loss 248/1000: 2.2377450466156006\n",
            "Train loss 249/1000: 2.237424612045288\n",
            "Train loss 250/1000: 2.2371065616607666\n",
            "Train loss 251/1000: 2.236790657043457\n",
            "Train loss 252/1000: 2.2364771366119385\n",
            "Train loss 253/1000: 2.236166000366211\n",
            "Train loss 254/1000: 2.235856771469116\n",
            "Train loss 255/1000: 2.2355496883392334\n",
            "Train loss 256/1000: 2.2352447509765625\n",
            "Train loss 257/1000: 2.2349421977996826\n",
            "Train loss 258/1000: 2.2346420288085938\n",
            "Train loss 259/1000: 2.2343432903289795\n",
            "Train loss 260/1000: 2.2340469360351562\n",
            "Train loss 261/1000: 2.233752727508545\n",
            "Train loss 262/1000: 2.2334604263305664\n",
            "Train loss 263/1000: 2.2331700325012207\n",
            "Train loss 264/1000: 2.232881546020508\n",
            "Train loss 265/1000: 2.232595205307007\n",
            "Train loss 266/1000: 2.2323107719421387\n",
            "Train loss 267/1000: 2.2320282459259033\n",
            "Train loss 268/1000: 2.23174786567688\n",
            "Train loss 269/1000: 2.23146915435791\n",
            "Train loss 270/1000: 2.231192111968994\n",
            "Train loss 271/1000: 2.230917453765869\n",
            "Train loss 272/1000: 2.2306442260742188\n",
            "Train loss 273/1000: 2.230372905731201\n",
            "Train loss 274/1000: 2.2301032543182373\n",
            "Train loss 275/1000: 2.2298355102539062\n",
            "Train loss 276/1000: 2.229569435119629\n",
            "Train loss 277/1000: 2.2293052673339844\n",
            "Train loss 278/1000: 2.2290425300598145\n",
            "Train loss 279/1000: 2.2287819385528564\n",
            "Train loss 280/1000: 2.228522539138794\n",
            "Train loss 281/1000: 2.228264808654785\n",
            "Train loss 282/1000: 2.2280092239379883\n",
            "Train loss 283/1000: 2.227755069732666\n",
            "Train loss 284/1000: 2.2275025844573975\n",
            "Train loss 285/1000: 2.2272512912750244\n",
            "Train loss 286/1000: 2.2270021438598633\n",
            "Train loss 287/1000: 2.2267541885375977\n",
            "Train loss 288/1000: 2.2265079021453857\n",
            "Train loss 289/1000: 2.2262632846832275\n",
            "Train loss 290/1000: 2.226020336151123\n",
            "Train loss 291/1000: 2.225778579711914\n",
            "Train loss 292/1000: 2.2255380153656006\n",
            "Train loss 293/1000: 2.225299835205078\n",
            "Train loss 294/1000: 2.225062370300293\n",
            "Train loss 295/1000: 2.2248265743255615\n",
            "Train loss 296/1000: 2.2245922088623047\n",
            "Train loss 297/1000: 2.2243595123291016\n",
            "Train loss 298/1000: 2.224128007888794\n",
            "Train loss 299/1000: 2.223897933959961\n",
            "Train loss 300/1000: 2.2236695289611816\n",
            "Train loss 301/1000: 2.2234420776367188\n",
            "Train loss 302/1000: 2.2232162952423096\n",
            "Train loss 303/1000: 2.222991466522217\n",
            "Train loss 304/1000: 2.2227683067321777\n",
            "Train loss 305/1000: 2.2225465774536133\n",
            "Train loss 306/1000: 2.2223258018493652\n",
            "Train loss 307/1000: 2.222106456756592\n",
            "Train loss 308/1000: 2.221888542175293\n",
            "Train loss 309/1000: 2.2216720581054688\n",
            "Train loss 310/1000: 2.221456527709961\n",
            "Train loss 311/1000: 2.2212424278259277\n",
            "Train loss 312/1000: 2.22102952003479\n",
            "Train loss 313/1000: 2.220818042755127\n",
            "Train loss 314/1000: 2.2206075191497803\n",
            "Train loss 315/1000: 2.220398426055908\n",
            "Train loss 316/1000: 2.2201902866363525\n",
            "Train loss 317/1000: 2.2199833393096924\n",
            "Train loss 318/1000: 2.2197775840759277\n",
            "Train loss 319/1000: 2.2195732593536377\n",
            "Train loss 320/1000: 2.219369888305664\n",
            "Train loss 321/1000: 2.219167709350586\n",
            "Train loss 322/1000: 2.2189669609069824\n",
            "Train loss 323/1000: 2.2187671661376953\n",
            "Train loss 324/1000: 2.2185680866241455\n",
            "Train loss 325/1000: 2.2183706760406494\n",
            "Train loss 326/1000: 2.2181739807128906\n",
            "Train loss 327/1000: 2.2179784774780273\n",
            "Train loss 328/1000: 2.2177841663360596\n",
            "Train loss 329/1000: 2.2175910472869873\n",
            "Train loss 330/1000: 2.2173986434936523\n",
            "Train loss 331/1000: 2.217207431793213\n",
            "Train loss 332/1000: 2.21701717376709\n",
            "Train loss 333/1000: 2.2168283462524414\n",
            "Train loss 334/1000: 2.2166402339935303\n",
            "Train loss 335/1000: 2.2164533138275146\n",
            "Train loss 336/1000: 2.2162673473358154\n",
            "Train loss 337/1000: 2.2160823345184326\n",
            "Train loss 338/1000: 2.215898036956787\n",
            "Train loss 339/1000: 2.215715169906616\n",
            "Train loss 340/1000: 2.2155332565307617\n",
            "Train loss 341/1000: 2.2153520584106445\n",
            "Train loss 342/1000: 2.215172052383423\n",
            "Train loss 343/1000: 2.2149925231933594\n",
            "Train loss 344/1000: 2.2148146629333496\n",
            "Train loss 345/1000: 2.214637279510498\n",
            "Train loss 346/1000: 2.214461088180542\n",
            "Train loss 347/1000: 2.2142856121063232\n",
            "Train loss 348/1000: 2.214111089706421\n",
            "Train loss 349/1000: 2.213937520980835\n",
            "Train loss 350/1000: 2.2137649059295654\n",
            "Train loss 351/1000: 2.2135932445526123\n",
            "Train loss 352/1000: 2.2134222984313965\n",
            "Train loss 353/1000: 2.213252305984497\n",
            "Train loss 354/1000: 2.213083028793335\n",
            "Train loss 355/1000: 2.2129149436950684\n",
            "Train loss 356/1000: 2.21274733543396\n",
            "Train loss 357/1000: 2.212580919265747\n",
            "Train loss 358/1000: 2.2124152183532715\n",
            "Train loss 359/1000: 2.212250232696533\n",
            "Train loss 360/1000: 2.2120862007141113\n",
            "Train loss 361/1000: 2.211923122406006\n",
            "Train loss 362/1000: 2.211760997772217\n",
            "Train loss 363/1000: 2.211599349975586\n",
            "Train loss 364/1000: 2.2114384174346924\n",
            "Train loss 365/1000: 2.2112786769866943\n",
            "Train loss 366/1000: 2.2111191749572754\n",
            "Train loss 367/1000: 2.210960865020752\n",
            "Train loss 368/1000: 2.210803270339966\n",
            "Train loss 369/1000: 2.210646629333496\n",
            "Train loss 370/1000: 2.2104904651641846\n",
            "Train loss 371/1000: 2.2103352546691895\n",
            "Train loss 372/1000: 2.2101809978485107\n",
            "Train loss 373/1000: 2.210026979446411\n",
            "Train loss 374/1000: 2.209874153137207\n",
            "Train loss 375/1000: 2.209721803665161\n",
            "Train loss 376/1000: 2.2095704078674316\n",
            "Train loss 377/1000: 2.2094194889068604\n",
            "Train loss 378/1000: 2.2092697620391846\n",
            "Train loss 379/1000: 2.209120273590088\n",
            "Train loss 380/1000: 2.2089717388153076\n",
            "Train loss 381/1000: 2.2088239192962646\n",
            "Train loss 382/1000: 2.208676338195801\n",
            "Train loss 383/1000: 2.2085299491882324\n",
            "Train loss 384/1000: 2.2083842754364014\n",
            "Train loss 385/1000: 2.2082390785217285\n",
            "Train loss 386/1000: 2.208094596862793\n",
            "Train loss 387/1000: 2.2079508304595947\n",
            "Train loss 388/1000: 2.207807779312134\n",
            "Train loss 389/1000: 2.20766544342041\n",
            "Train loss 390/1000: 2.2075235843658447\n",
            "Train loss 391/1000: 2.2073826789855957\n",
            "Train loss 392/1000: 2.207242250442505\n",
            "Train loss 393/1000: 2.2071025371551514\n",
            "Train loss 394/1000: 2.206963300704956\n",
            "Train loss 395/1000: 2.206824779510498\n",
            "Train loss 396/1000: 2.2066869735717773\n",
            "Train loss 397/1000: 2.206549644470215\n",
            "Train loss 398/1000: 2.2064132690429688\n",
            "Train loss 399/1000: 2.206277370452881\n",
            "Train loss 400/1000: 2.206141948699951\n",
            "Train loss 401/1000: 2.206007242202759\n",
            "Train loss 402/1000: 2.2058730125427246\n",
            "Train loss 403/1000: 2.2057394981384277\n",
            "Train loss 404/1000: 2.205606698989868\n",
            "Train loss 405/1000: 2.205474376678467\n",
            "Train loss 406/1000: 2.2053427696228027\n",
            "Train loss 407/1000: 2.205211877822876\n",
            "Train loss 408/1000: 2.20508074760437\n",
            "Train loss 409/1000: 2.204951286315918\n",
            "Train loss 410/1000: 2.204821825027466\n",
            "Train loss 411/1000: 2.204692840576172\n",
            "Train loss 412/1000: 2.2045645713806152\n",
            "Train loss 413/1000: 2.204437017440796\n",
            "Train loss 414/1000: 2.2043099403381348\n",
            "Train loss 415/1000: 2.204183578491211\n",
            "Train loss 416/1000: 2.204057455062866\n",
            "Train loss 417/1000: 2.2039318084716797\n",
            "Train loss 418/1000: 2.2038068771362305\n",
            "Train loss 419/1000: 2.2036828994750977\n",
            "Train loss 420/1000: 2.203558921813965\n",
            "Train loss 421/1000: 2.2034356594085693\n",
            "Train loss 422/1000: 2.203313112258911\n",
            "Train loss 423/1000: 2.203190803527832\n",
            "Train loss 424/1000: 2.203068971633911\n",
            "Train loss 425/1000: 2.2029478549957275\n",
            "Train loss 426/1000: 2.2028274536132812\n",
            "Train loss 427/1000: 2.202707290649414\n",
            "Train loss 428/1000: 2.202587604522705\n",
            "Train loss 429/1000: 2.2024686336517334\n",
            "Train loss 430/1000: 2.202349901199341\n",
            "Train loss 431/1000: 2.2022316455841064\n",
            "Train loss 432/1000: 2.2021141052246094\n",
            "Train loss 433/1000: 2.2019970417022705\n",
            "Train loss 434/1000: 2.20188045501709\n",
            "Train loss 435/1000: 2.2017641067504883\n",
            "Train loss 436/1000: 2.201648712158203\n",
            "Train loss 437/1000: 2.201533794403076\n",
            "Train loss 438/1000: 2.201418876647949\n",
            "Train loss 439/1000: 2.2013046741485596\n",
            "Train loss 440/1000: 2.201190948486328\n",
            "Train loss 441/1000: 2.201077699661255\n",
            "Train loss 442/1000: 2.200965166091919\n",
            "Train loss 443/1000: 2.200852632522583\n",
            "Train loss 444/1000: 2.2007408142089844\n",
            "Train loss 445/1000: 2.200629472732544\n",
            "Train loss 446/1000: 2.2005181312561035\n",
            "Train loss 447/1000: 2.2004079818725586\n",
            "Train loss 448/1000: 2.2002975940704346\n",
            "Train loss 449/1000: 2.200188159942627\n",
            "Train loss 450/1000: 2.2000792026519775\n",
            "Train loss 451/1000: 2.1999707221984863\n",
            "Train loss 452/1000: 2.199862003326416\n",
            "Train loss 453/1000: 2.199754476547241\n",
            "Train loss 454/1000: 2.1996469497680664\n",
            "Train loss 455/1000: 2.199540138244629\n",
            "Train loss 456/1000: 2.1994335651397705\n",
            "Train loss 457/1000: 2.1993274688720703\n",
            "Train loss 458/1000: 2.199221611022949\n",
            "Train loss 459/1000: 2.1991164684295654\n",
            "Train loss 460/1000: 2.1990113258361816\n",
            "Train loss 461/1000: 2.198906898498535\n",
            "Train loss 462/1000: 2.198802947998047\n",
            "Train loss 463/1000: 2.198699474334717\n",
            "Train loss 464/1000: 2.1985960006713867\n",
            "Train loss 465/1000: 2.198493480682373\n",
            "Train loss 466/1000: 2.1983909606933594\n",
            "Train loss 467/1000: 2.198289155960083\n",
            "Train loss 468/1000: 2.1981873512268066\n",
            "Train loss 469/1000: 2.1980862617492676\n",
            "Train loss 470/1000: 2.1979854106903076\n",
            "Train loss 471/1000: 2.1978847980499268\n",
            "Train loss 472/1000: 2.1977851390838623\n",
            "Train loss 473/1000: 2.1976852416992188\n",
            "Train loss 474/1000: 2.1975860595703125\n",
            "Train loss 475/1000: 2.1974871158599854\n",
            "Train loss 476/1000: 2.1973886489868164\n",
            "Train loss 477/1000: 2.1972906589508057\n",
            "Train loss 478/1000: 2.197192907333374\n",
            "Train loss 479/1000: 2.1970951557159424\n",
            "Train loss 480/1000: 2.196998119354248\n",
            "Train loss 481/1000: 2.196901559829712\n",
            "Train loss 482/1000: 2.196805238723755\n",
            "Train loss 483/1000: 2.196709394454956\n",
            "Train loss 484/1000: 2.1966137886047363\n",
            "Train loss 485/1000: 2.196518659591675\n",
            "Train loss 486/1000: 2.1964237689971924\n",
            "Train loss 487/1000: 2.196329116821289\n",
            "Train loss 488/1000: 2.196235179901123\n",
            "Train loss 489/1000: 2.196141481399536\n",
            "Train loss 490/1000: 2.1960480213165283\n",
            "Train loss 491/1000: 2.1959547996520996\n",
            "Train loss 492/1000: 2.195862054824829\n",
            "Train loss 493/1000: 2.195769786834717\n",
            "Train loss 494/1000: 2.1956777572631836\n",
            "Train loss 495/1000: 2.1955862045288086\n",
            "Train loss 496/1000: 2.1954946517944336\n",
            "Train loss 497/1000: 2.195403814315796\n",
            "Train loss 498/1000: 2.195312738418579\n",
            "Train loss 499/1000: 2.1952223777770996\n",
            "Train loss 500/1000: 2.1951324939727783\n",
            "Train loss 501/1000: 2.195042371749878\n",
            "Train loss 502/1000: 2.194953203201294\n",
            "Train loss 503/1000: 2.194864273071289\n",
            "Train loss 504/1000: 2.1947755813598633\n",
            "Train loss 505/1000: 2.1946871280670166\n",
            "Train loss 506/1000: 2.194598913192749\n",
            "Train loss 507/1000: 2.1945114135742188\n",
            "Train loss 508/1000: 2.1944239139556885\n",
            "Train loss 509/1000: 2.1943366527557373\n",
            "Train loss 510/1000: 2.1942498683929443\n",
            "Train loss 511/1000: 2.1941633224487305\n",
            "Train loss 512/1000: 2.1940770149230957\n",
            "Train loss 513/1000: 2.19399094581604\n",
            "Train loss 514/1000: 2.1939053535461426\n",
            "Train loss 515/1000: 2.1938202381134033\n",
            "Train loss 516/1000: 2.193735122680664\n",
            "Train loss 517/1000: 2.193650484085083\n",
            "Train loss 518/1000: 2.19356632232666\n",
            "Train loss 519/1000: 2.193481922149658\n",
            "Train loss 520/1000: 2.1933979988098145\n",
            "Train loss 521/1000: 2.193314790725708\n",
            "Train loss 522/1000: 2.1932315826416016\n",
            "Train loss 523/1000: 2.193148374557495\n",
            "Train loss 524/1000: 2.193065643310547\n",
            "Train loss 525/1000: 2.192983388900757\n",
            "Train loss 526/1000: 2.192901134490967\n",
            "Train loss 527/1000: 2.192819833755493\n",
            "Train loss 528/1000: 2.1927382946014404\n",
            "Train loss 529/1000: 2.1926567554473877\n",
            "Train loss 530/1000: 2.192575693130493\n",
            "Train loss 531/1000: 2.192495346069336\n",
            "Train loss 532/1000: 2.1924147605895996\n",
            "Train loss 533/1000: 2.1923348903656006\n",
            "Train loss 534/1000: 2.1922547817230225\n",
            "Train loss 535/1000: 2.1921753883361816\n",
            "Train loss 536/1000: 2.192095994949341\n",
            "Train loss 537/1000: 2.192017078399658\n",
            "Train loss 538/1000: 2.1919384002685547\n",
            "Train loss 539/1000: 2.191859722137451\n",
            "Train loss 540/1000: 2.191781759262085\n",
            "Train loss 541/1000: 2.1917033195495605\n",
            "Train loss 542/1000: 2.1916258335113525\n",
            "Train loss 543/1000: 2.1915483474731445\n",
            "Train loss 544/1000: 2.1914710998535156\n",
            "Train loss 545/1000: 2.191394329071045\n",
            "Train loss 546/1000: 2.191317558288574\n",
            "Train loss 547/1000: 2.1912412643432617\n",
            "Train loss 548/1000: 2.1911652088165283\n",
            "Train loss 549/1000: 2.191089153289795\n",
            "Train loss 550/1000: 2.1910133361816406\n",
            "Train loss 551/1000: 2.1909379959106445\n",
            "Train loss 552/1000: 2.1908631324768066\n",
            "Train loss 553/1000: 2.1907880306243896\n",
            "Train loss 554/1000: 2.1907131671905518\n",
            "Train loss 555/1000: 2.190639019012451\n",
            "Train loss 556/1000: 2.1905648708343506\n",
            "Train loss 557/1000: 2.190490961074829\n",
            "Train loss 558/1000: 2.1904172897338867\n",
            "Train loss 559/1000: 2.1903438568115234\n",
            "Train loss 560/1000: 2.1902706623077393\n",
            "Train loss 561/1000: 2.190197706222534\n",
            "Train loss 562/1000: 2.190124988555908\n",
            "Train loss 563/1000: 2.1900522708892822\n",
            "Train loss 564/1000: 2.1899805068969727\n",
            "Train loss 565/1000: 2.189908266067505\n",
            "Train loss 566/1000: 2.1898365020751953\n",
            "Train loss 567/1000: 2.189764976501465\n",
            "Train loss 568/1000: 2.1896936893463135\n",
            "Train loss 569/1000: 2.189622640609741\n",
            "Train loss 570/1000: 2.189551591873169\n",
            "Train loss 571/1000: 2.189481258392334\n",
            "Train loss 572/1000: 2.18941068649292\n",
            "Train loss 573/1000: 2.189340591430664\n",
            "Train loss 574/1000: 2.1892707347869873\n",
            "Train loss 575/1000: 2.1892008781433105\n",
            "Train loss 576/1000: 2.189131259918213\n",
            "Train loss 577/1000: 2.1890621185302734\n",
            "Train loss 578/1000: 2.188992977142334\n",
            "Train loss 579/1000: 2.1889240741729736\n",
            "Train loss 580/1000: 2.1888556480407715\n",
            "Train loss 581/1000: 2.1887869834899902\n",
            "Train loss 582/1000: 2.188718795776367\n",
            "Train loss 583/1000: 2.1886508464813232\n",
            "Train loss 584/1000: 2.1885828971862793\n",
            "Train loss 585/1000: 2.1885154247283936\n",
            "Train loss 586/1000: 2.188448190689087\n",
            "Train loss 587/1000: 2.1883811950683594\n",
            "Train loss 588/1000: 2.188314199447632\n",
            "Train loss 589/1000: 2.1882472038269043\n",
            "Train loss 590/1000: 2.188180685043335\n",
            "Train loss 591/1000: 2.1881144046783447\n",
            "Train loss 592/1000: 2.1880483627319336\n",
            "Train loss 593/1000: 2.1879825592041016\n",
            "Train loss 594/1000: 2.1879167556762695\n",
            "Train loss 595/1000: 2.1878514289855957\n",
            "Train loss 596/1000: 2.187786102294922\n",
            "Train loss 597/1000: 2.187720775604248\n",
            "Train loss 598/1000: 2.1876559257507324\n",
            "Train loss 599/1000: 2.187591314315796\n",
            "Train loss 600/1000: 2.1875269412994385\n",
            "Train loss 601/1000: 2.187462568283081\n",
            "Train loss 602/1000: 2.1873981952667236\n",
            "Train loss 603/1000: 2.1873345375061035\n",
            "Train loss 604/1000: 2.1872706413269043\n",
            "Train loss 605/1000: 2.1872074604034424\n",
            "Train loss 606/1000: 2.1871440410614014\n",
            "Train loss 607/1000: 2.1870808601379395\n",
            "Train loss 608/1000: 2.1870179176330566\n",
            "Train loss 609/1000: 2.186954975128174\n",
            "Train loss 610/1000: 2.1868927478790283\n",
            "Train loss 611/1000: 2.1868298053741455\n",
            "Train loss 612/1000: 2.186768054962158\n",
            "Train loss 613/1000: 2.1867058277130127\n",
            "Train loss 614/1000: 2.1866443157196045\n",
            "Train loss 615/1000: 2.186582326889038\n",
            "Train loss 616/1000: 2.186521053314209\n",
            "Train loss 617/1000: 2.18645977973938\n",
            "Train loss 618/1000: 2.186398506164551\n",
            "Train loss 619/1000: 2.18633770942688\n",
            "Train loss 620/1000: 2.186276912689209\n",
            "Train loss 621/1000: 2.186216115951538\n",
            "Train loss 622/1000: 2.1861560344696045\n",
            "Train loss 623/1000: 2.186095714569092\n",
            "Train loss 624/1000: 2.186035633087158\n",
            "Train loss 625/1000: 2.1859757900238037\n",
            "Train loss 626/1000: 2.1859164237976074\n",
            "Train loss 627/1000: 2.185856819152832\n",
            "Train loss 628/1000: 2.1857974529266357\n",
            "Train loss 629/1000: 2.1857380867004395\n",
            "Train loss 630/1000: 2.1856791973114014\n",
            "Train loss 631/1000: 2.1856203079223633\n",
            "Train loss 632/1000: 2.1855618953704834\n",
            "Train loss 633/1000: 2.1855032444000244\n",
            "Train loss 634/1000: 2.1854448318481445\n",
            "Train loss 635/1000: 2.185386896133423\n",
            "Train loss 636/1000: 2.1853291988372803\n",
            "Train loss 637/1000: 2.1852712631225586\n",
            "Train loss 638/1000: 2.185213565826416\n",
            "Train loss 639/1000: 2.1851561069488525\n",
            "Train loss 640/1000: 2.18509840965271\n",
            "Train loss 641/1000: 2.185041904449463\n",
            "Train loss 642/1000: 2.1849844455718994\n",
            "Train loss 643/1000: 2.1849277019500732\n",
            "Train loss 644/1000: 2.184871196746826\n",
            "Train loss 645/1000: 2.184814691543579\n",
            "Train loss 646/1000: 2.1847586631774902\n",
            "Train loss 647/1000: 2.1847023963928223\n",
            "Train loss 648/1000: 2.1846461296081543\n",
            "Train loss 649/1000: 2.1845905780792236\n",
            "Train loss 650/1000: 2.184534788131714\n",
            "Train loss 651/1000: 2.184479236602783\n",
            "Train loss 652/1000: 2.1844236850738525\n",
            "Train loss 653/1000: 2.18436861038208\n",
            "Train loss 654/1000: 2.1843135356903076\n",
            "Train loss 655/1000: 2.1842586994171143\n",
            "Train loss 656/1000: 2.184203863143921\n",
            "Train loss 657/1000: 2.1841492652893066\n",
            "Train loss 658/1000: 2.1840946674346924\n",
            "Train loss 659/1000: 2.1840403079986572\n",
            "Train loss 660/1000: 2.1839864253997803\n",
            "Train loss 661/1000: 2.183932304382324\n",
            "Train loss 662/1000: 2.1838786602020264\n",
            "Train loss 663/1000: 2.1838250160217285\n",
            "Train loss 664/1000: 2.1837713718414307\n",
            "Train loss 665/1000: 2.183717727661133\n",
            "Train loss 666/1000: 2.1836647987365723\n",
            "Train loss 667/1000: 2.1836113929748535\n",
            "Train loss 668/1000: 2.183558464050293\n",
            "Train loss 669/1000: 2.1835057735443115\n",
            "Train loss 670/1000: 2.18345308303833\n",
            "Train loss 671/1000: 2.1834006309509277\n",
            "Train loss 672/1000: 2.1833484172821045\n",
            "Train loss 673/1000: 2.1832962036132812\n",
            "Train loss 674/1000: 2.183243751525879\n",
            "Train loss 675/1000: 2.1831917762756348\n",
            "Train loss 676/1000: 2.1831400394439697\n",
            "Train loss 677/1000: 2.1830883026123047\n",
            "Train loss 678/1000: 2.1830368041992188\n",
            "Train loss 679/1000: 2.182985544204712\n",
            "Train loss 680/1000: 2.182934284210205\n",
            "Train loss 681/1000: 2.1828832626342773\n",
            "Train loss 682/1000: 2.1828320026397705\n",
            "Train loss 683/1000: 2.182781457901001\n",
            "Train loss 684/1000: 2.1827304363250732\n",
            "Train loss 685/1000: 2.182680130004883\n",
            "Train loss 686/1000: 2.1826295852661133\n",
            "Train loss 687/1000: 2.182579278945923\n",
            "Train loss 688/1000: 2.1825292110443115\n",
            "Train loss 689/1000: 2.1824791431427\n",
            "Train loss 690/1000: 2.182429313659668\n",
            "Train loss 691/1000: 2.1823794841766357\n",
            "Train loss 692/1000: 2.1823296546936035\n",
            "Train loss 693/1000: 2.1822800636291504\n",
            "Train loss 694/1000: 2.1822309494018555\n",
            "Train loss 695/1000: 2.1821813583374023\n",
            "Train loss 696/1000: 2.1821324825286865\n",
            "Train loss 697/1000: 2.1820836067199707\n",
            "Train loss 698/1000: 2.182034730911255\n",
            "Train loss 699/1000: 2.181985855102539\n",
            "Train loss 700/1000: 2.1819374561309814\n",
            "Train loss 701/1000: 2.1818888187408447\n",
            "Train loss 702/1000: 2.181840658187866\n",
            "Train loss 703/1000: 2.1817920207977295\n",
            "Train loss 704/1000: 2.181744337081909\n",
            "Train loss 705/1000: 2.1816964149475098\n",
            "Train loss 706/1000: 2.1816482543945312\n",
            "Train loss 707/1000: 2.181600570678711\n",
            "Train loss 708/1000: 2.1815531253814697\n",
            "Train loss 709/1000: 2.1815059185028076\n",
            "Train loss 710/1000: 2.1814584732055664\n",
            "Train loss 711/1000: 2.1814112663269043\n",
            "Train loss 712/1000: 2.181364059448242\n",
            "Train loss 713/1000: 2.181317090988159\n",
            "Train loss 714/1000: 2.181270122528076\n",
            "Train loss 715/1000: 2.1812233924865723\n",
            "Train loss 716/1000: 2.1811766624450684\n",
            "Train loss 717/1000: 2.1811301708221436\n",
            "Train loss 718/1000: 2.181083917617798\n",
            "Train loss 719/1000: 2.181037664413452\n",
            "Train loss 720/1000: 2.1809914112091064\n",
            "Train loss 721/1000: 2.18094539642334\n",
            "Train loss 722/1000: 2.1808996200561523\n",
            "Train loss 723/1000: 2.180853843688965\n",
            "Train loss 724/1000: 2.1808080673217773\n",
            "Train loss 725/1000: 2.18076229095459\n",
            "Train loss 726/1000: 2.1807172298431396\n",
            "Train loss 727/1000: 2.1806719303131104\n",
            "Train loss 728/1000: 2.18062686920166\n",
            "Train loss 729/1000: 2.1805813312530518\n",
            "Train loss 730/1000: 2.1805365085601807\n",
            "Train loss 731/1000: 2.1804916858673096\n",
            "Train loss 732/1000: 2.1804471015930176\n",
            "Train loss 733/1000: 2.1804022789001465\n",
            "Train loss 734/1000: 2.1803579330444336\n",
            "Train loss 735/1000: 2.1803133487701416\n",
            "Train loss 736/1000: 2.180269241333008\n",
            "Train loss 737/1000: 2.180224895477295\n",
            "Train loss 738/1000: 2.180180788040161\n",
            "Train loss 739/1000: 2.1801369190216064\n",
            "Train loss 740/1000: 2.1800930500030518\n",
            "Train loss 741/1000: 2.180049180984497\n",
            "Train loss 742/1000: 2.1800057888031006\n",
            "Train loss 743/1000: 2.179961919784546\n",
            "Train loss 744/1000: 2.1799185276031494\n",
            "Train loss 745/1000: 2.179875135421753\n",
            "Train loss 746/1000: 2.1798319816589355\n",
            "Train loss 747/1000: 2.179788827896118\n",
            "Train loss 748/1000: 2.179746150970459\n",
            "Train loss 749/1000: 2.1797029972076416\n",
            "Train loss 750/1000: 2.1796603202819824\n",
            "Train loss 751/1000: 2.179617404937744\n",
            "Train loss 752/1000: 2.179574966430664\n",
            "Train loss 753/1000: 2.179532289505005\n",
            "Train loss 754/1000: 2.179490089416504\n",
            "Train loss 755/1000: 2.179447889328003\n",
            "Train loss 756/1000: 2.179405927658081\n",
            "Train loss 757/1000: 2.179363489151001\n",
            "Train loss 758/1000: 2.179321527481079\n",
            "Train loss 759/1000: 2.1792798042297363\n",
            "Train loss 760/1000: 2.1792380809783936\n",
            "Train loss 761/1000: 2.179196357727051\n",
            "Train loss 762/1000: 2.179154396057129\n",
            "Train loss 763/1000: 2.1791133880615234\n",
            "Train loss 764/1000: 2.1790719032287598\n",
            "Train loss 765/1000: 2.179030418395996\n",
            "Train loss 766/1000: 2.1789894104003906\n",
            "Train loss 767/1000: 2.1789486408233643\n",
            "Train loss 768/1000: 2.1789071559906006\n",
            "Train loss 769/1000: 2.1788666248321533\n",
            "Train loss 770/1000: 2.178825855255127\n",
            "Train loss 771/1000: 2.1787848472595215\n",
            "Train loss 772/1000: 2.1787445545196533\n",
            "Train loss 773/1000: 2.178703784942627\n",
            "Train loss 774/1000: 2.178663730621338\n",
            "Train loss 775/1000: 2.1786234378814697\n",
            "Train loss 776/1000: 2.1785829067230225\n",
            "Train loss 777/1000: 2.1785430908203125\n",
            "Train loss 778/1000: 2.1785032749176025\n",
            "Train loss 779/1000: 2.1784632205963135\n",
            "Train loss 780/1000: 2.1784231662750244\n",
            "Train loss 781/1000: 2.1783835887908936\n",
            "Train loss 782/1000: 2.1783440113067627\n",
            "Train loss 783/1000: 2.178304433822632\n",
            "Train loss 784/1000: 2.17826509475708\n",
            "Train loss 785/1000: 2.1782257556915283\n",
            "Train loss 786/1000: 2.1781864166259766\n",
            "Train loss 787/1000: 2.178147315979004\n",
            "Train loss 788/1000: 2.1781082153320312\n",
            "Train loss 789/1000: 2.1780693531036377\n",
            "Train loss 790/1000: 2.178030252456665\n",
            "Train loss 791/1000: 2.1779913902282715\n",
            "Train loss 792/1000: 2.177952766418457\n",
            "Train loss 793/1000: 2.1779141426086426\n",
            "Train loss 794/1000: 2.177875518798828\n",
            "Train loss 795/1000: 2.177837371826172\n",
            "Train loss 796/1000: 2.1777987480163574\n",
            "Train loss 797/1000: 2.177760362625122\n",
            "Train loss 798/1000: 2.177722454071045\n",
            "Train loss 799/1000: 2.1776843070983887\n",
            "Train loss 800/1000: 2.1776461601257324\n",
            "Train loss 801/1000: 2.1776084899902344\n",
            "Train loss 802/1000: 2.1775708198547363\n",
            "Train loss 803/1000: 2.17753267288208\n",
            "Train loss 804/1000: 2.177495002746582\n",
            "Train loss 805/1000: 2.177457571029663\n",
            "Train loss 806/1000: 2.177420139312744\n",
            "Train loss 807/1000: 2.177382707595825\n",
            "Train loss 808/1000: 2.1773455142974854\n",
            "Train loss 809/1000: 2.1773080825805664\n",
            "Train loss 810/1000: 2.1772706508636475\n",
            "Train loss 811/1000: 2.177234172821045\n",
            "Train loss 812/1000: 2.177197217941284\n",
            "Train loss 813/1000: 2.1771602630615234\n",
            "Train loss 814/1000: 2.1771230697631836\n",
            "Train loss 815/1000: 2.177086591720581\n",
            "Train loss 816/1000: 2.1770501136779785\n",
            "Train loss 817/1000: 2.177013635635376\n",
            "Train loss 818/1000: 2.1769766807556152\n",
            "Train loss 819/1000: 2.176940441131592\n",
            "Train loss 820/1000: 2.1769044399261475\n",
            "Train loss 821/1000: 2.176867961883545\n",
            "Train loss 822/1000: 2.1768319606781006\n",
            "Train loss 823/1000: 2.176795721054077\n",
            "Train loss 824/1000: 2.176759958267212\n",
            "Train loss 825/1000: 2.1767239570617676\n",
            "Train loss 826/1000: 2.1766881942749023\n",
            "Train loss 827/1000: 2.176652431488037\n",
            "Train loss 828/1000: 2.176616907119751\n",
            "Train loss 829/1000: 2.1765811443328857\n",
            "Train loss 830/1000: 2.1765453815460205\n",
            "Train loss 831/1000: 2.1765100955963135\n",
            "Train loss 832/1000: 2.1764748096466064\n",
            "Train loss 833/1000: 2.1764395236968994\n",
            "Train loss 834/1000: 2.1764042377471924\n",
            "Train loss 835/1000: 2.1763691902160645\n",
            "Train loss 836/1000: 2.1763341426849365\n",
            "Train loss 837/1000: 2.1762990951538086\n",
            "Train loss 838/1000: 2.1762642860412598\n",
            "Train loss 839/1000: 2.176229476928711\n",
            "Train loss 840/1000: 2.176194906234741\n",
            "Train loss 841/1000: 2.1761600971221924\n",
            "Train loss 842/1000: 2.1761257648468018\n",
            "Train loss 843/1000: 2.176090955734253\n",
            "Train loss 844/1000: 2.176056385040283\n",
            "Train loss 845/1000: 2.1760220527648926\n",
            "Train loss 846/1000: 2.175987958908081\n",
            "Train loss 847/1000: 2.1759538650512695\n",
            "Train loss 848/1000: 2.175919771194458\n",
            "Train loss 849/1000: 2.1758856773376465\n",
            "Train loss 850/1000: 2.175851583480835\n",
            "Train loss 851/1000: 2.1758177280426025\n",
            "Train loss 852/1000: 2.17578387260437\n",
            "Train loss 853/1000: 2.1757500171661377\n",
            "Train loss 854/1000: 2.1757161617279053\n",
            "Train loss 855/1000: 2.175682783126831\n",
            "Train loss 856/1000: 2.1756489276885986\n",
            "Train loss 857/1000: 2.1756155490875244\n",
            "Train loss 858/1000: 2.17558217048645\n",
            "Train loss 859/1000: 2.175549030303955\n",
            "Train loss 860/1000: 2.175515651702881\n",
            "Train loss 861/1000: 2.1754822731018066\n",
            "Train loss 862/1000: 2.1754491329193115\n",
            "Train loss 863/1000: 2.1754164695739746\n",
            "Train loss 864/1000: 2.1753833293914795\n",
            "Train loss 865/1000: 2.1753504276275635\n",
            "Train loss 866/1000: 2.1753177642822266\n",
            "Train loss 867/1000: 2.1752846240997314\n",
            "Train loss 868/1000: 2.1752519607543945\n",
            "Train loss 869/1000: 2.1752195358276367\n",
            "Train loss 870/1000: 2.1751866340637207\n",
            "Train loss 871/1000: 2.175154447555542\n",
            "Train loss 872/1000: 2.175122022628784\n",
            "Train loss 873/1000: 2.1750895977020264\n",
            "Train loss 874/1000: 2.1750571727752686\n",
            "Train loss 875/1000: 2.17502498626709\n",
            "Train loss 876/1000: 2.174992799758911\n",
            "Train loss 877/1000: 2.1749608516693115\n",
            "Train loss 878/1000: 2.174928903579712\n",
            "Train loss 879/1000: 2.174896717071533\n",
            "Train loss 880/1000: 2.174865245819092\n",
            "Train loss 881/1000: 2.174833059310913\n",
            "Train loss 882/1000: 2.1748013496398926\n",
            "Train loss 883/1000: 2.174769878387451\n",
            "Train loss 884/1000: 2.1747381687164307\n",
            "Train loss 885/1000: 2.17470645904541\n",
            "Train loss 886/1000: 2.1746749877929688\n",
            "Train loss 887/1000: 2.1746435165405273\n",
            "Train loss 888/1000: 2.174612522125244\n",
            "Train loss 889/1000: 2.1745808124542236\n",
            "Train loss 890/1000: 2.1745498180389404\n",
            "Train loss 891/1000: 2.174518585205078\n",
            "Train loss 892/1000: 2.174487352371216\n",
            "Train loss 893/1000: 2.1744565963745117\n",
            "Train loss 894/1000: 2.1744253635406494\n",
            "Train loss 895/1000: 2.1743946075439453\n",
            "Train loss 896/1000: 2.174363613128662\n",
            "Train loss 897/1000: 2.174333095550537\n",
            "Train loss 898/1000: 2.174302101135254\n",
            "Train loss 899/1000: 2.17427134513855\n",
            "Train loss 900/1000: 2.174241065979004\n",
            "Train loss 901/1000: 2.1742103099823\n",
            "Train loss 902/1000: 2.174180030822754\n",
            "Train loss 903/1000: 2.174149513244629\n",
            "Train loss 904/1000: 2.174119234085083\n",
            "Train loss 905/1000: 2.174088954925537\n",
            "Train loss 906/1000: 2.1740589141845703\n",
            "Train loss 907/1000: 2.1740286350250244\n",
            "Train loss 908/1000: 2.1739983558654785\n",
            "Train loss 909/1000: 2.173968553543091\n",
            "Train loss 910/1000: 2.173938274383545\n",
            "Train loss 911/1000: 2.1739087104797363\n",
            "Train loss 912/1000: 2.1738786697387695\n",
            "Train loss 913/1000: 2.173848867416382\n",
            "Train loss 914/1000: 2.173819065093994\n",
            "Train loss 915/1000: 2.1737895011901855\n",
            "Train loss 916/1000: 2.173759937286377\n",
            "Train loss 917/1000: 2.1737303733825684\n",
            "Train loss 918/1000: 2.1737008094787598\n",
            "Train loss 919/1000: 2.1736714839935303\n",
            "Train loss 920/1000: 2.173642158508301\n",
            "Train loss 921/1000: 2.1736128330230713\n",
            "Train loss 922/1000: 2.173583507537842\n",
            "Train loss 923/1000: 2.1735541820526123\n",
            "Train loss 924/1000: 2.173525094985962\n",
            "Train loss 925/1000: 2.1734960079193115\n",
            "Train loss 926/1000: 2.173466920852661\n",
            "Train loss 927/1000: 2.1734378337860107\n",
            "Train loss 928/1000: 2.1734092235565186\n",
            "Train loss 929/1000: 2.173379898071289\n",
            "Train loss 930/1000: 2.173351526260376\n",
            "Train loss 931/1000: 2.1733226776123047\n",
            "Train loss 932/1000: 2.1732940673828125\n",
            "Train loss 933/1000: 2.173265218734741\n",
            "Train loss 934/1000: 2.173236608505249\n",
            "Train loss 935/1000: 2.173208236694336\n",
            "Train loss 936/1000: 2.173179864883423\n",
            "Train loss 937/1000: 2.1731512546539307\n",
            "Train loss 938/1000: 2.1731231212615967\n",
            "Train loss 939/1000: 2.1730945110321045\n",
            "Train loss 940/1000: 2.1730666160583496\n",
            "Train loss 941/1000: 2.1730384826660156\n",
            "Train loss 942/1000: 2.1730103492736816\n",
            "Train loss 943/1000: 2.1729822158813477\n",
            "Train loss 944/1000: 2.1729540824890137\n",
            "Train loss 945/1000: 2.172926187515259\n",
            "Train loss 946/1000: 2.172898292541504\n",
            "Train loss 947/1000: 2.172870635986328\n",
            "Train loss 948/1000: 2.172842502593994\n",
            "Train loss 949/1000: 2.1728148460388184\n",
            "Train loss 950/1000: 2.1727869510650635\n",
            "Train loss 951/1000: 2.172759532928467\n",
            "Train loss 952/1000: 2.172731876373291\n",
            "Train loss 953/1000: 2.1727044582366943\n",
            "Train loss 954/1000: 2.1726768016815186\n",
            "Train loss 955/1000: 2.172649621963501\n",
            "Train loss 956/1000: 2.172621965408325\n",
            "Train loss 957/1000: 2.1725947856903076\n",
            "Train loss 958/1000: 2.172567367553711\n",
            "Train loss 959/1000: 2.1725401878356934\n",
            "Train loss 960/1000: 2.172513246536255\n",
            "Train loss 961/1000: 2.172485828399658\n",
            "Train loss 962/1000: 2.1724588871002197\n",
            "Train loss 963/1000: 2.172431707382202\n",
            "Train loss 964/1000: 2.1724050045013428\n",
            "Train loss 965/1000: 2.1723780632019043\n",
            "Train loss 966/1000: 2.172351121902466\n",
            "Train loss 967/1000: 2.1723246574401855\n",
            "Train loss 968/1000: 2.172297716140747\n",
            "Train loss 969/1000: 2.1722710132598877\n",
            "Train loss 970/1000: 2.1722443103790283\n",
            "Train loss 971/1000: 2.172217845916748\n",
            "Train loss 972/1000: 2.1721911430358887\n",
            "Train loss 973/1000: 2.1721646785736084\n",
            "Train loss 974/1000: 2.172137975692749\n",
            "Train loss 975/1000: 2.172111749649048\n",
            "Train loss 976/1000: 2.1720855236053467\n",
            "Train loss 977/1000: 2.1720592975616455\n",
            "Train loss 978/1000: 2.1720328330993652\n",
            "Train loss 979/1000: 2.172006607055664\n",
            "Train loss 980/1000: 2.171980619430542\n",
            "Train loss 981/1000: 2.1719541549682617\n",
            "Train loss 982/1000: 2.1719281673431396\n",
            "Train loss 983/1000: 2.1719024181365967\n",
            "Train loss 984/1000: 2.1718761920928955\n",
            "Train loss 985/1000: 2.1718506813049316\n",
            "Train loss 986/1000: 2.1718246936798096\n",
            "Train loss 987/1000: 2.1717987060546875\n",
            "Train loss 988/1000: 2.1717731952667236\n",
            "Train loss 989/1000: 2.1717474460601807\n",
            "Train loss 990/1000: 2.1717216968536377\n",
            "Train loss 991/1000: 2.171696186065674\n",
            "Train loss 992/1000: 2.171670436859131\n",
            "Train loss 993/1000: 2.171644926071167\n",
            "Train loss 994/1000: 2.171619415283203\n",
            "Train loss 995/1000: 2.1715939044952393\n",
            "Train loss 996/1000: 2.1715686321258545\n",
            "Train loss 997/1000: 2.1715431213378906\n",
            "Train loss 998/1000: 2.171517848968506\n",
            "Train loss 999/1000: 2.171492576599121\n",
            "Train loss 1000/1000: 2.1714675426483154\n",
            "junide\n",
            "janasid\n",
            "paus\n",
            "amainna\n",
            "jimritonia\n",
            "saree\n",
            "kalania\n",
            "yanileniassibduine\n",
            "imbressiyanayla\n",
            "te\n",
            "farmumarif\n",
            "demmer\n",
            "finslena\n",
            "jaylicora\n",
            "ya\n",
            "jocken\n",
            "jamilyn\n",
            "korin\n",
            "wyn\n",
            "ne\n",
            "gaasnhavi\n",
            "monszxhxdgor\n",
            "mathani\n",
            "zie\n",
            "paulithat\n",
            "jayreli\n",
            "isamey\n",
            "rumjlaurickivina\n",
            "ha\n",
            "kha\n",
            "cra\n",
            "raydnhwadorta\n",
            "malyn\n",
            "brey\n",
            "aur\n",
            "lavarock\n",
            "themiraya\n",
            "ath\n",
            "basely\n",
            "kaleentikeysalee\n",
            "marlen\n",
            "emafabethellianteah\n",
            "han\n",
            "jazaody\n",
            "drd\n",
            "johialiypjrgia\n",
            "tezra\n",
            "elia\n",
            "vywhielvani\n",
            "sa\n",
            "imah\n",
            "kellette\n",
            "braceryoni\n",
            "li\n",
            "alian\n",
            "denn\n",
            "jayannyah\n",
            "kennelyn\n",
            "marianner\n",
            "samotan\n",
            "kyroderihana\n",
            "shday\n",
            "vi\n",
            "olleah\n",
            "tertte\n",
            "keus\n",
            "dasia\n",
            "na\n",
            "calynevini\n",
            "aspi\n",
            "katalilondral\n",
            "fanna\n",
            "mat\n",
            "shana\n",
            "ve\n",
            "hanngeon\n",
            "resynivion\n",
            "uzien\n",
            "jalivyah\n",
            "altoravrafjlanayliannah\n",
            "norrisyli\n",
            "mson\n",
            "jorayiangkxdvjypnfria\n",
            "maulila\n",
            "acvjveer\n",
            "ael\n",
            "shirla\n",
            "sa\n",
            "pre\n",
            "azalie\n",
            "bion\n",
            "sperahmiha\n",
            "ivalisakamord\n",
            "bradeelyn\n",
            "zyarleertiliah\n",
            "dayda\n",
            "fbduqayceeyton\n",
            "za\n",
            "sabran\n",
            "han\n",
            "kennasslen\n",
            "conik\n",
            "ny\n",
            "noakowestisona\n",
            "isemrtellwes\n",
            "aleildareka\n",
            "jojaykjmhiffan\n",
            "kiyah\n",
            "lula\n",
            "jus\n",
            "b\n",
            "den\n",
            "hawyqsum\n",
            "ozie\n",
            "hasiah\n",
            "na\n",
            "adiamorie\n",
            "amiquidilah\n",
            "an\n",
            "keila\n",
            "viyah\n",
            "ax\n",
            "mandt\n",
            "tyjamanalingh\n",
            "jepekala\n",
            "koberishuraya\n",
            "khir\n",
            "hand\n",
            "wcalina\n",
            "ellini\n",
            "eminnehdiah\n",
            "pasdtiana\n",
            "ca\n",
            "coria\n",
            "andouros\n",
            "am\n",
            "kaws\n",
            "eva\n",
            "rie\n",
            "laydeni\n",
            "dasiradina\n",
            "te\n",
            "kariomirptvzb\n",
            "hazie\n",
            "firet\n",
            "esten\n",
            "die\n",
            "alacioree\n",
            "karrair\n",
            "ala\n",
            "der\n",
            "hambela\n",
            "jigh\n",
            "chansha\n",
            "alayane\n",
            "miybra\n",
            "zo\n",
            "truellan\n",
            "ix\n",
            "na\n",
            "ek\n",
            "maryena\n",
            "kemerytlett\n",
            "camaelqexeqrvi\n",
            "navan\n",
            "ann\n",
            "inyonisanyo\n",
            "elay\n",
            "ay\n",
            "yah\n",
            "dena\n",
            "bronidavicopkrnie\n",
            "luci\n",
            "kyahia\n",
            "to\n",
            "maylee\n",
            "tala\n",
            "laton\n",
            "gialiah\n",
            "joyarisen\n",
            "kamela\n",
            "la\n",
            "shus\n",
            "jobet\n",
            "it\n",
            "sundreen\n",
            "qsia\n",
            "mince\n",
            "marmileyah\n",
            "anus\n",
            "redon\n",
            "kilapxzdfvdrievitlyozel\n",
            "hanady\n",
            "woordi\n",
            "ille\n",
            "delaitzinah\n",
            "zyiah\n",
            "trayn\n",
            "ha\n",
            "lin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the parameters of the trained model on the other set\n",
        "def evaluate_model(arg_weights, arg_inputs, arg_targets):\n",
        "  logits_count = arg_weights[arg_inputs]\n",
        "  count_tensor = torch.exp(logits_count)\n",
        "  prob_tensor = count_tensor / torch.sum(count_tensor, dim=1, keepdim=True)\n",
        "\n",
        "  # calculate the loss (negative log likelihood)\n",
        "  loss = abs(prob_tensor[torch.arange(prob_tensor.shape[0]), arg_targets].log().mean())\n",
        "  print(f\"Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "b7ao5iva6dQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For the dev set:\")\n",
        "evaluate_model(train_weights, dev_inputs, dev_targets)\n",
        "\n",
        "print(\"For the test set:\")\n",
        "evaluate_model(train_weights, test_inputs, test_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCjLjfbr7IkN",
        "outputId": "4cb7abb5-3699-43fc-e86d-9ea675bf7f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the dev set:\n",
            "Loss: 2.4274251461029053\n",
            "For the test set:\n",
            "Loss: 2.449005126953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The loss for dev set and test set is much higher than that for the train set\n",
        "\n"
      ],
      "metadata": {
        "id": "IkLeyzE7Bk2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tune the smoothing of the model on the dev set"
      ],
      "metadata": {
        "id": "qwfAswEhI6-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_weights.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TE74DK-DSF9d",
        "outputId": "0450e093-a3da-4f8a-d57b-0d973e9a067c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.5886e-06,  3.7515e-07, -3.7217e-07,  ..., -3.9107e-07,\n",
              "         -3.6552e-07, -4.1003e-07],\n",
              "        [ 8.7005e-06, -4.3350e-07, -4.3295e-07,  ..., -5.4515e-07,\n",
              "         -4.3455e-07, -4.3623e-07],\n",
              "        [ 3.5884e-06, -5.6147e-06,  3.7776e-06,  ...,  2.7962e-06,\n",
              "         -5.7857e-06,  2.1665e-06],\n",
              "        ...,\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [-4.3128e-06, -4.4627e-06,  2.3119e-06,  ...,  2.6989e-06,\n",
              "          5.3450e-06,  2.7283e-06],\n",
              "        [-5.2416e-06, -8.6936e-06,  3.1826e-06,  ...,  4.5518e-06,\n",
              "         -1.2195e-05,  4.8728e-06]])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the obtained model on the train set on the dev set with the smoothing\n",
        "def smooth_model(arg_weights, arg_inputs, arg_targets):\n",
        "  arg_generator = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "  # clone the current weights parameters to the other tensor\n",
        "  smooth_weights = arg_weights.detach().clone()\n",
        "  smooth_weights.requires_grad = True\n",
        "  smooth_weights.grad = arg_weights.grad.clone()\n",
        "\n",
        "  for i in range(500):\n",
        "    # forward pass\n",
        "    # calculate the softmax\n",
        "    # index directly to the rows\n",
        "    logits_count = smooth_weights[arg_inputs]\n",
        "    count_tensor = torch.exp(logits_count)\n",
        "    prob_tensor = count_tensor / torch.sum(count_tensor, dim=1, keepdim=True)\n",
        "\n",
        "    # calculate the loss (negative log likelihood)\n",
        "    dev_loss = abs(prob_tensor[torch.arange(prob_tensor.shape[0]), arg_targets].log().mean())\n",
        "    # add the regularlization loss to smooth the model\n",
        "    dev_lost += (0.1) * (smooth_weights ** 2).mean()\n",
        "    print(f\"Dev loss {i+1}/500: {dev_loss.item()}\")\n",
        "\n",
        "    # backward pass (backprogagation comes into place)\n",
        "    dev_loss.backward()\n",
        "\n",
        "    # update the parameters of the weights\n",
        "    smooth_weights.data += -100 * smooth_weights.grad\n",
        "\n",
        "  # return the parameters of the model\n",
        "  return smooth_weights"
      ],
      "metadata": {
        "id": "BFTWwcuaBz4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smoothed_weights = smooth_model(train_weights, dev_inputs, dev_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4bcM1ezOey2",
        "outputId": "d85f4513-87d0-4e84-fbad-d6f013df0283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.6314, grad_fn=<MeanBackward0>)\n",
            "tensor([[ 9.5886e-06,  3.7515e-07, -3.7217e-07,  ..., -3.9107e-07,\n",
            "         -3.6552e-07, -4.1003e-07],\n",
            "        [ 8.7005e-06, -4.3350e-07, -4.3295e-07,  ..., -5.4515e-07,\n",
            "         -4.3455e-07, -4.3623e-07],\n",
            "        [ 3.5884e-06, -5.6147e-06,  3.7776e-06,  ...,  2.7962e-06,\n",
            "         -5.7857e-06,  2.1665e-06],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-4.3128e-06, -4.4627e-06,  2.3119e-06,  ...,  2.6989e-06,\n",
            "          5.3450e-06,  2.7283e-06],\n",
            "        [-5.2416e-06, -8.6936e-06,  3.1826e-06,  ...,  4.5518e-06,\n",
            "         -1.2195e-05,  4.8728e-06]])\n",
            "Dev loss 1/1000: 2.5905685424804688\n",
            "Dev loss 2/1000: 2.5645718574523926\n",
            "Dev loss 3/1000: 2.53695011138916\n",
            "Dev loss 4/1000: 2.5140600204467773\n",
            "Dev loss 5/1000: 2.4916210174560547\n",
            "Dev loss 6/1000: 2.475141763687134\n",
            "Dev loss 7/1000: 2.467463731765747\n",
            "Dev loss 8/1000: 2.457728862762451\n",
            "Dev loss 9/1000: 2.4463086128234863\n",
            "Dev loss 10/1000: 2.4425337314605713\n",
            "Dev loss 11/1000: 2.4447507858276367\n",
            "Dev loss 12/1000: 2.441951274871826\n",
            "Dev loss 13/1000: 2.4339187145233154\n",
            "Dev loss 14/1000: 2.428079605102539\n",
            "Dev loss 15/1000: 2.425685405731201\n",
            "Dev loss 16/1000: 2.4231009483337402\n",
            "Dev loss 17/1000: 2.4190056324005127\n",
            "Dev loss 18/1000: 2.4149861335754395\n",
            "Dev loss 19/1000: 2.4109954833984375\n",
            "Dev loss 20/1000: 2.405663251876831\n",
            "Dev loss 21/1000: 2.4025940895080566\n",
            "Dev loss 22/1000: 2.4036593437194824\n",
            "Dev loss 23/1000: 2.4036037921905518\n",
            "Dev loss 24/1000: 2.3997981548309326\n",
            "Dev loss 25/1000: 2.3962008953094482\n",
            "Dev loss 26/1000: 2.3942298889160156\n",
            "Dev loss 27/1000: 2.3930253982543945\n",
            "Dev loss 28/1000: 2.394988536834717\n",
            "Dev loss 29/1000: 2.396904945373535\n",
            "Dev loss 30/1000: 2.392916440963745\n",
            "Dev loss 31/1000: 2.388016700744629\n",
            "Dev loss 32/1000: 2.386516809463501\n",
            "Dev loss 33/1000: 2.3868257999420166\n",
            "Dev loss 34/1000: 2.3888611793518066\n",
            "Dev loss 35/1000: 2.3933145999908447\n",
            "Dev loss 36/1000: 2.3938469886779785\n",
            "Dev loss 37/1000: 2.3908655643463135\n",
            "Dev loss 38/1000: 2.3904354572296143\n",
            "Dev loss 39/1000: 2.392141342163086\n",
            "Dev loss 40/1000: 2.3948628902435303\n",
            "Dev loss 41/1000: 2.398742437362671\n",
            "Dev loss 42/1000: 2.4002914428710938\n",
            "Dev loss 43/1000: 2.3993139266967773\n",
            "Dev loss 44/1000: 2.4003851413726807\n",
            "Dev loss 45/1000: 2.4044713973999023\n",
            "Dev loss 46/1000: 2.4052889347076416\n",
            "Dev loss 47/1000: 2.403334617614746\n",
            "Dev loss 48/1000: 2.403066396713257\n",
            "Dev loss 49/1000: 2.4030468463897705\n",
            "Dev loss 50/1000: 2.4032487869262695\n",
            "Dev loss 51/1000: 2.408376455307007\n",
            "Dev loss 52/1000: 2.4135515689849854\n",
            "Dev loss 53/1000: 2.415621519088745\n",
            "Dev loss 54/1000: 2.415428638458252\n",
            "Dev loss 55/1000: 2.4152612686157227\n",
            "Dev loss 56/1000: 2.417160987854004\n",
            "Dev loss 57/1000: 2.42185640335083\n",
            "Dev loss 58/1000: 2.425638198852539\n",
            "Dev loss 59/1000: 2.426237106323242\n",
            "Dev loss 60/1000: 2.425041913986206\n",
            "Dev loss 61/1000: 2.4262635707855225\n",
            "Dev loss 62/1000: 2.430823802947998\n",
            "Dev loss 63/1000: 2.435957193374634\n",
            "Dev loss 64/1000: 2.437570571899414\n",
            "Dev loss 65/1000: 2.437480926513672\n",
            "Dev loss 66/1000: 2.4383597373962402\n",
            "Dev loss 67/1000: 2.4402987957000732\n",
            "Dev loss 68/1000: 2.4443795680999756\n",
            "Dev loss 69/1000: 2.448798418045044\n",
            "Dev loss 70/1000: 2.4492664337158203\n",
            "Dev loss 71/1000: 2.4452064037323\n",
            "Dev loss 72/1000: 2.444044828414917\n",
            "Dev loss 73/1000: 2.4491193294525146\n",
            "Dev loss 74/1000: 2.453108549118042\n",
            "Dev loss 75/1000: 2.452380895614624\n",
            "Dev loss 76/1000: 2.4534249305725098\n",
            "Dev loss 77/1000: 2.4536163806915283\n",
            "Dev loss 78/1000: 2.4507312774658203\n",
            "Dev loss 79/1000: 2.4517064094543457\n",
            "Dev loss 80/1000: 2.456987142562866\n",
            "Dev loss 81/1000: 2.458979606628418\n",
            "Dev loss 82/1000: 2.458463668823242\n",
            "Dev loss 83/1000: 2.4595041275024414\n",
            "Dev loss 84/1000: 2.4610674381256104\n",
            "Dev loss 85/1000: 2.4603376388549805\n",
            "Dev loss 86/1000: 2.459243059158325\n",
            "Dev loss 87/1000: 2.458970785140991\n",
            "Dev loss 88/1000: 2.4584500789642334\n",
            "Dev loss 89/1000: 2.4585390090942383\n",
            "Dev loss 90/1000: 2.4600651264190674\n",
            "Dev loss 91/1000: 2.4607996940612793\n",
            "Dev loss 92/1000: 2.459876298904419\n",
            "Dev loss 93/1000: 2.4577579498291016\n",
            "Dev loss 94/1000: 2.4554316997528076\n",
            "Dev loss 95/1000: 2.4546873569488525\n",
            "Dev loss 96/1000: 2.4545350074768066\n",
            "Dev loss 97/1000: 2.453385591506958\n",
            "Dev loss 98/1000: 2.4526522159576416\n",
            "Dev loss 99/1000: 2.4505832195281982\n",
            "Dev loss 100/1000: 2.447734832763672\n",
            "Dev loss 101/1000: 2.44738507270813\n",
            "Dev loss 102/1000: 2.4489853382110596\n",
            "Dev loss 103/1000: 2.4489080905914307\n",
            "Dev loss 104/1000: 2.448258399963379\n",
            "Dev loss 105/1000: 2.447648048400879\n",
            "Dev loss 106/1000: 2.4453659057617188\n",
            "Dev loss 107/1000: 2.441239595413208\n",
            "Dev loss 108/1000: 2.437807321548462\n",
            "Dev loss 109/1000: 2.4344539642333984\n",
            "Dev loss 110/1000: 2.4296162128448486\n",
            "Dev loss 111/1000: 2.4257142543792725\n",
            "Dev loss 112/1000: 2.4248292446136475\n",
            "Dev loss 113/1000: 2.4245569705963135\n",
            "Dev loss 114/1000: 2.423675298690796\n",
            "Dev loss 115/1000: 2.42268443107605\n",
            "Dev loss 116/1000: 2.4202194213867188\n",
            "Dev loss 117/1000: 2.4169154167175293\n",
            "Dev loss 118/1000: 2.4154534339904785\n",
            "Dev loss 119/1000: 2.4130542278289795\n",
            "Dev loss 120/1000: 2.4096593856811523\n",
            "Dev loss 121/1000: 2.4086928367614746\n",
            "Dev loss 122/1000: 2.4087066650390625\n",
            "Dev loss 123/1000: 2.408242702484131\n",
            "Dev loss 124/1000: 2.409818410873413\n",
            "Dev loss 125/1000: 2.41182017326355\n",
            "Dev loss 126/1000: 2.412529945373535\n",
            "Dev loss 127/1000: 2.4127793312072754\n",
            "Dev loss 128/1000: 2.4102954864501953\n",
            "Dev loss 129/1000: 2.406097650527954\n",
            "Dev loss 130/1000: 2.4050028324127197\n",
            "Dev loss 131/1000: 2.4076201915740967\n",
            "Dev loss 132/1000: 2.409946918487549\n",
            "Dev loss 133/1000: 2.41113018989563\n",
            "Dev loss 134/1000: 2.4134392738342285\n",
            "Dev loss 135/1000: 2.4154715538024902\n",
            "Dev loss 136/1000: 2.4149363040924072\n",
            "Dev loss 137/1000: 2.414952278137207\n",
            "Dev loss 138/1000: 2.415099620819092\n",
            "Dev loss 139/1000: 2.413968563079834\n",
            "Dev loss 140/1000: 2.4139392375946045\n",
            "Dev loss 141/1000: 2.414910078048706\n",
            "Dev loss 142/1000: 2.4150524139404297\n",
            "Dev loss 143/1000: 2.4160208702087402\n",
            "Dev loss 144/1000: 2.417009115219116\n",
            "Dev loss 145/1000: 2.41605281829834\n",
            "Dev loss 146/1000: 2.414695978164673\n",
            "Dev loss 147/1000: 2.4133028984069824\n",
            "Dev loss 148/1000: 2.4101924896240234\n",
            "Dev loss 149/1000: 2.4085710048675537\n",
            "Dev loss 150/1000: 2.410109043121338\n",
            "Dev loss 151/1000: 2.4130377769470215\n",
            "Dev loss 152/1000: 2.415759325027466\n",
            "Dev loss 153/1000: 2.4173431396484375\n",
            "Dev loss 154/1000: 2.4168572425842285\n",
            "Dev loss 155/1000: 2.417534351348877\n",
            "Dev loss 156/1000: 2.4194445610046387\n",
            "Dev loss 157/1000: 2.4204299449920654\n",
            "Dev loss 158/1000: 2.4211630821228027\n",
            "Dev loss 159/1000: 2.4228315353393555\n",
            "Dev loss 160/1000: 2.4226901531219482\n",
            "Dev loss 161/1000: 2.420102596282959\n",
            "Dev loss 162/1000: 2.419959545135498\n",
            "Dev loss 163/1000: 2.4233648777008057\n",
            "Dev loss 164/1000: 2.4242911338806152\n",
            "Dev loss 165/1000: 2.4240009784698486\n",
            "Dev loss 166/1000: 2.425705909729004\n",
            "Dev loss 167/1000: 2.4267208576202393\n",
            "Dev loss 168/1000: 2.427056312561035\n",
            "Dev loss 169/1000: 2.4291043281555176\n",
            "Dev loss 170/1000: 2.4309346675872803\n",
            "Dev loss 171/1000: 2.432201385498047\n",
            "Dev loss 172/1000: 2.43405818939209\n",
            "Dev loss 173/1000: 2.436164379119873\n",
            "Dev loss 174/1000: 2.438297748565674\n",
            "Dev loss 175/1000: 2.440355062484741\n",
            "Dev loss 176/1000: 2.4420578479766846\n",
            "Dev loss 177/1000: 2.443702459335327\n",
            "Dev loss 178/1000: 2.4460408687591553\n",
            "Dev loss 179/1000: 2.4489710330963135\n",
            "Dev loss 180/1000: 2.4509871006011963\n",
            "Dev loss 181/1000: 2.451082468032837\n",
            "Dev loss 182/1000: 2.4498674869537354\n",
            "Dev loss 183/1000: 2.4484810829162598\n",
            "Dev loss 184/1000: 2.44716215133667\n",
            "Dev loss 185/1000: 2.446762800216675\n",
            "Dev loss 186/1000: 2.4468185901641846\n",
            "Dev loss 187/1000: 2.4474101066589355\n",
            "Dev loss 188/1000: 2.4479618072509766\n",
            "Dev loss 189/1000: 2.4484763145446777\n",
            "Dev loss 190/1000: 2.4503939151763916\n",
            "Dev loss 191/1000: 2.4539053440093994\n",
            "Dev loss 192/1000: 2.4553871154785156\n",
            "Dev loss 193/1000: 2.4526207447052\n",
            "Dev loss 194/1000: 2.4520931243896484\n",
            "Dev loss 195/1000: 2.452061176300049\n",
            "Dev loss 196/1000: 2.449875593185425\n",
            "Dev loss 197/1000: 2.4486074447631836\n",
            "Dev loss 198/1000: 2.4496991634368896\n",
            "Dev loss 199/1000: 2.449769973754883\n",
            "Dev loss 200/1000: 2.450082302093506\n",
            "Dev loss 201/1000: 2.450169086456299\n",
            "Dev loss 202/1000: 2.4494640827178955\n",
            "Dev loss 203/1000: 2.448300361633301\n",
            "Dev loss 204/1000: 2.4470930099487305\n",
            "Dev loss 205/1000: 2.4453508853912354\n",
            "Dev loss 206/1000: 2.444021701812744\n",
            "Dev loss 207/1000: 2.44376540184021\n",
            "Dev loss 208/1000: 2.443535327911377\n",
            "Dev loss 209/1000: 2.442368268966675\n",
            "Dev loss 210/1000: 2.441960573196411\n",
            "Dev loss 211/1000: 2.441256523132324\n",
            "Dev loss 212/1000: 2.4388370513916016\n",
            "Dev loss 213/1000: 2.4373207092285156\n",
            "Dev loss 214/1000: 2.437822103500366\n",
            "Dev loss 215/1000: 2.436483383178711\n",
            "Dev loss 216/1000: 2.434084177017212\n",
            "Dev loss 217/1000: 2.4326765537261963\n",
            "Dev loss 218/1000: 2.4303855895996094\n",
            "Dev loss 219/1000: 2.428835868835449\n",
            "Dev loss 220/1000: 2.429088830947876\n",
            "Dev loss 221/1000: 2.4277255535125732\n",
            "Dev loss 222/1000: 2.4244110584259033\n",
            "Dev loss 223/1000: 2.421005964279175\n",
            "Dev loss 224/1000: 2.417717695236206\n",
            "Dev loss 225/1000: 2.4157259464263916\n",
            "Dev loss 226/1000: 2.4156181812286377\n",
            "Dev loss 227/1000: 2.4161922931671143\n",
            "Dev loss 228/1000: 2.416700601577759\n",
            "Dev loss 229/1000: 2.417741060256958\n",
            "Dev loss 230/1000: 2.4181692600250244\n",
            "Dev loss 231/1000: 2.416642189025879\n",
            "Dev loss 232/1000: 2.4154205322265625\n",
            "Dev loss 233/1000: 2.4148383140563965\n",
            "Dev loss 234/1000: 2.4133307933807373\n",
            "Dev loss 235/1000: 2.4124224185943604\n",
            "Dev loss 236/1000: 2.4128735065460205\n",
            "Dev loss 237/1000: 2.413416862487793\n",
            "Dev loss 238/1000: 2.413970470428467\n",
            "Dev loss 239/1000: 2.4152660369873047\n",
            "Dev loss 240/1000: 2.414508104324341\n",
            "Dev loss 241/1000: 2.4117252826690674\n",
            "Dev loss 242/1000: 2.409151554107666\n",
            "Dev loss 243/1000: 2.4062209129333496\n",
            "Dev loss 244/1000: 2.403686285018921\n",
            "Dev loss 245/1000: 2.4051315784454346\n",
            "Dev loss 246/1000: 2.407869815826416\n",
            "Dev loss 247/1000: 2.4093635082244873\n",
            "Dev loss 248/1000: 2.410641670227051\n",
            "Dev loss 249/1000: 2.4115002155303955\n",
            "Dev loss 250/1000: 2.4123787879943848\n",
            "Dev loss 251/1000: 2.413768768310547\n",
            "Dev loss 252/1000: 2.413966655731201\n",
            "Dev loss 253/1000: 2.413031816482544\n",
            "Dev loss 254/1000: 2.4128918647766113\n",
            "Dev loss 255/1000: 2.4143619537353516\n",
            "Dev loss 256/1000: 2.4173879623413086\n",
            "Dev loss 257/1000: 2.421774387359619\n",
            "Dev loss 258/1000: 2.4252076148986816\n",
            "Dev loss 259/1000: 2.4257030487060547\n",
            "Dev loss 260/1000: 2.425351619720459\n",
            "Dev loss 261/1000: 2.42814302444458\n",
            "Dev loss 262/1000: 2.4301817417144775\n",
            "Dev loss 263/1000: 2.428622007369995\n",
            "Dev loss 264/1000: 2.427612066268921\n",
            "Dev loss 265/1000: 2.4294447898864746\n",
            "Dev loss 266/1000: 2.4299139976501465\n",
            "Dev loss 267/1000: 2.4302992820739746\n",
            "Dev loss 268/1000: 2.432673215866089\n",
            "Dev loss 269/1000: 2.434802770614624\n",
            "Dev loss 270/1000: 2.434530019760132\n",
            "Dev loss 271/1000: 2.433145523071289\n",
            "Dev loss 272/1000: 2.43347430229187\n",
            "Dev loss 273/1000: 2.4366934299468994\n",
            "Dev loss 274/1000: 2.439859390258789\n",
            "Dev loss 275/1000: 2.440411329269409\n",
            "Dev loss 276/1000: 2.442253828048706\n",
            "Dev loss 277/1000: 2.4468233585357666\n",
            "Dev loss 278/1000: 2.448160171508789\n",
            "Dev loss 279/1000: 2.4463493824005127\n",
            "Dev loss 280/1000: 2.446902275085449\n",
            "Dev loss 281/1000: 2.448561668395996\n",
            "Dev loss 282/1000: 2.445646286010742\n",
            "Dev loss 283/1000: 2.4429636001586914\n",
            "Dev loss 284/1000: 2.44639253616333\n",
            "Dev loss 285/1000: 2.4494712352752686\n",
            "Dev loss 286/1000: 2.4490301609039307\n",
            "Dev loss 287/1000: 2.4480316638946533\n",
            "Dev loss 288/1000: 2.44549560546875\n",
            "Dev loss 289/1000: 2.4431076049804688\n",
            "Dev loss 290/1000: 2.442034959793091\n",
            "Dev loss 291/1000: 2.4401051998138428\n",
            "Dev loss 292/1000: 2.438826084136963\n",
            "Dev loss 293/1000: 2.439992904663086\n",
            "Dev loss 294/1000: 2.440751075744629\n",
            "Dev loss 295/1000: 2.441643476486206\n",
            "Dev loss 296/1000: 2.444692611694336\n",
            "Dev loss 297/1000: 2.446502447128296\n",
            "Dev loss 298/1000: 2.445786952972412\n",
            "Dev loss 299/1000: 2.446096420288086\n",
            "Dev loss 300/1000: 2.4459447860717773\n",
            "Dev loss 301/1000: 2.443162202835083\n",
            "Dev loss 302/1000: 2.4408061504364014\n",
            "Dev loss 303/1000: 2.44197678565979\n",
            "Dev loss 304/1000: 2.442183494567871\n",
            "Dev loss 305/1000: 2.438596487045288\n",
            "Dev loss 306/1000: 2.439039468765259\n",
            "Dev loss 307/1000: 2.4400336742401123\n",
            "Dev loss 308/1000: 2.4358882904052734\n",
            "Dev loss 309/1000: 2.432682991027832\n",
            "Dev loss 310/1000: 2.4337291717529297\n",
            "Dev loss 311/1000: 2.432594060897827\n",
            "Dev loss 312/1000: 2.4296605587005615\n",
            "Dev loss 313/1000: 2.42944073677063\n",
            "Dev loss 314/1000: 2.4321725368499756\n",
            "Dev loss 315/1000: 2.433480739593506\n",
            "Dev loss 316/1000: 2.4325647354125977\n",
            "Dev loss 317/1000: 2.4333384037017822\n",
            "Dev loss 318/1000: 2.435577392578125\n",
            "Dev loss 319/1000: 2.432741641998291\n",
            "Dev loss 320/1000: 2.4265081882476807\n",
            "Dev loss 321/1000: 2.4245409965515137\n",
            "Dev loss 322/1000: 2.425208806991577\n",
            "Dev loss 323/1000: 2.4212000370025635\n",
            "Dev loss 324/1000: 2.4168899059295654\n",
            "Dev loss 325/1000: 2.417058229446411\n",
            "Dev loss 326/1000: 2.4173688888549805\n",
            "Dev loss 327/1000: 2.414018392562866\n",
            "Dev loss 328/1000: 2.4130468368530273\n",
            "Dev loss 329/1000: 2.4161579608917236\n",
            "Dev loss 330/1000: 2.4168765544891357\n",
            "Dev loss 331/1000: 2.4147863388061523\n",
            "Dev loss 332/1000: 2.4151973724365234\n",
            "Dev loss 333/1000: 2.416337013244629\n",
            "Dev loss 334/1000: 2.4158499240875244\n",
            "Dev loss 335/1000: 2.4160423278808594\n",
            "Dev loss 336/1000: 2.4175941944122314\n",
            "Dev loss 337/1000: 2.420229434967041\n",
            "Dev loss 338/1000: 2.4210410118103027\n",
            "Dev loss 339/1000: 2.4179601669311523\n",
            "Dev loss 340/1000: 2.414113998413086\n",
            "Dev loss 341/1000: 2.4123237133026123\n",
            "Dev loss 342/1000: 2.4107749462127686\n",
            "Dev loss 343/1000: 2.4095892906188965\n",
            "Dev loss 344/1000: 2.41105580329895\n",
            "Dev loss 345/1000: 2.4132683277130127\n",
            "Dev loss 346/1000: 2.4142603874206543\n",
            "Dev loss 347/1000: 2.415400505065918\n",
            "Dev loss 348/1000: 2.417351484298706\n",
            "Dev loss 349/1000: 2.418444871902466\n",
            "Dev loss 350/1000: 2.4189887046813965\n",
            "Dev loss 351/1000: 2.420222759246826\n",
            "Dev loss 352/1000: 2.422024726867676\n",
            "Dev loss 353/1000: 2.423908233642578\n",
            "Dev loss 354/1000: 2.425325870513916\n",
            "Dev loss 355/1000: 2.4246082305908203\n",
            "Dev loss 356/1000: 2.423321008682251\n",
            "Dev loss 357/1000: 2.4249625205993652\n",
            "Dev loss 358/1000: 2.4270365238189697\n",
            "Dev loss 359/1000: 2.428030014038086\n",
            "Dev loss 360/1000: 2.428973913192749\n",
            "Dev loss 361/1000: 2.4289331436157227\n",
            "Dev loss 362/1000: 2.427633762359619\n",
            "Dev loss 363/1000: 2.4286725521087646\n",
            "Dev loss 364/1000: 2.431192636489868\n",
            "Dev loss 365/1000: 2.433394193649292\n",
            "Dev loss 366/1000: 2.436952590942383\n",
            "Dev loss 367/1000: 2.4400746822357178\n",
            "Dev loss 368/1000: 2.440826177597046\n",
            "Dev loss 369/1000: 2.44138240814209\n",
            "Dev loss 370/1000: 2.4411919116973877\n",
            "Dev loss 371/1000: 2.4396300315856934\n",
            "Dev loss 372/1000: 2.4382483959198\n",
            "Dev loss 373/1000: 2.4379990100860596\n",
            "Dev loss 374/1000: 2.437546968460083\n",
            "Dev loss 375/1000: 2.437411308288574\n",
            "Dev loss 376/1000: 2.4383456707000732\n",
            "Dev loss 377/1000: 2.4388587474823\n",
            "Dev loss 378/1000: 2.4381961822509766\n",
            "Dev loss 379/1000: 2.4387049674987793\n",
            "Dev loss 380/1000: 2.4404594898223877\n",
            "Dev loss 381/1000: 2.440934419631958\n",
            "Dev loss 382/1000: 2.441077709197998\n",
            "Dev loss 383/1000: 2.4422271251678467\n",
            "Dev loss 384/1000: 2.442572832107544\n",
            "Dev loss 385/1000: 2.4435067176818848\n",
            "Dev loss 386/1000: 2.445420026779175\n",
            "Dev loss 387/1000: 2.4460463523864746\n",
            "Dev loss 388/1000: 2.4460551738739014\n",
            "Dev loss 389/1000: 2.4461190700531006\n",
            "Dev loss 390/1000: 2.444876194000244\n",
            "Dev loss 391/1000: 2.4440176486968994\n",
            "Dev loss 392/1000: 2.44372296333313\n",
            "Dev loss 393/1000: 2.443789482116699\n",
            "Dev loss 394/1000: 2.443631410598755\n",
            "Dev loss 395/1000: 2.4431560039520264\n",
            "Dev loss 396/1000: 2.441579818725586\n",
            "Dev loss 397/1000: 2.4396920204162598\n",
            "Dev loss 398/1000: 2.438645839691162\n",
            "Dev loss 399/1000: 2.4392378330230713\n",
            "Dev loss 400/1000: 2.4395604133605957\n",
            "Dev loss 401/1000: 2.439296007156372\n",
            "Dev loss 402/1000: 2.439666509628296\n",
            "Dev loss 403/1000: 2.4396774768829346\n",
            "Dev loss 404/1000: 2.4399588108062744\n",
            "Dev loss 405/1000: 2.4394941329956055\n",
            "Dev loss 406/1000: 2.4364163875579834\n",
            "Dev loss 407/1000: 2.433654308319092\n",
            "Dev loss 408/1000: 2.43337082862854\n",
            "Dev loss 409/1000: 2.431295871734619\n",
            "Dev loss 410/1000: 2.428309202194214\n",
            "Dev loss 411/1000: 2.4290318489074707\n",
            "Dev loss 412/1000: 2.430147409439087\n",
            "Dev loss 413/1000: 2.4290547370910645\n",
            "Dev loss 414/1000: 2.4294497966766357\n",
            "Dev loss 415/1000: 2.4312970638275146\n",
            "Dev loss 416/1000: 2.431311845779419\n",
            "Dev loss 417/1000: 2.4300312995910645\n",
            "Dev loss 418/1000: 2.4298360347747803\n",
            "Dev loss 419/1000: 2.431030750274658\n",
            "Dev loss 420/1000: 2.431931972503662\n",
            "Dev loss 421/1000: 2.4317452907562256\n",
            "Dev loss 422/1000: 2.430145740509033\n",
            "Dev loss 423/1000: 2.427086591720581\n",
            "Dev loss 424/1000: 2.424377918243408\n",
            "Dev loss 425/1000: 2.4222569465637207\n",
            "Dev loss 426/1000: 2.4214742183685303\n",
            "Dev loss 427/1000: 2.4232797622680664\n",
            "Dev loss 428/1000: 2.4244604110717773\n",
            "Dev loss 429/1000: 2.42224383354187\n",
            "Dev loss 430/1000: 2.420348644256592\n",
            "Dev loss 431/1000: 2.4185056686401367\n",
            "Dev loss 432/1000: 2.4154279232025146\n",
            "Dev loss 433/1000: 2.4138989448547363\n",
            "Dev loss 434/1000: 2.4145028591156006\n",
            "Dev loss 435/1000: 2.4145286083221436\n",
            "Dev loss 436/1000: 2.4148921966552734\n",
            "Dev loss 437/1000: 2.4159815311431885\n",
            "Dev loss 438/1000: 2.4166016578674316\n",
            "Dev loss 439/1000: 2.416593313217163\n",
            "Dev loss 440/1000: 2.416654586791992\n",
            "Dev loss 441/1000: 2.416208505630493\n",
            "Dev loss 442/1000: 2.416046619415283\n",
            "Dev loss 443/1000: 2.4158787727355957\n",
            "Dev loss 444/1000: 2.4159765243530273\n",
            "Dev loss 445/1000: 2.41664457321167\n",
            "Dev loss 446/1000: 2.418311595916748\n",
            "Dev loss 447/1000: 2.41892671585083\n",
            "Dev loss 448/1000: 2.4193215370178223\n",
            "Dev loss 449/1000: 2.4202828407287598\n",
            "Dev loss 450/1000: 2.4202120304107666\n",
            "Dev loss 451/1000: 2.418625593185425\n",
            "Dev loss 452/1000: 2.4191365242004395\n",
            "Dev loss 453/1000: 2.4211955070495605\n",
            "Dev loss 454/1000: 2.4213812351226807\n",
            "Dev loss 455/1000: 2.4221599102020264\n",
            "Dev loss 456/1000: 2.424175977706909\n",
            "Dev loss 457/1000: 2.423072099685669\n",
            "Dev loss 458/1000: 2.420945167541504\n",
            "Dev loss 459/1000: 2.421327590942383\n",
            "Dev loss 460/1000: 2.421814441680908\n",
            "Dev loss 461/1000: 2.422497510910034\n",
            "Dev loss 462/1000: 2.4246304035186768\n",
            "Dev loss 463/1000: 2.426520347595215\n",
            "Dev loss 464/1000: 2.428260326385498\n",
            "Dev loss 465/1000: 2.4317314624786377\n",
            "Dev loss 466/1000: 2.434298276901245\n",
            "Dev loss 467/1000: 2.434527635574341\n",
            "Dev loss 468/1000: 2.4340670108795166\n",
            "Dev loss 469/1000: 2.433835029602051\n",
            "Dev loss 470/1000: 2.432651996612549\n",
            "Dev loss 471/1000: 2.431309700012207\n",
            "Dev loss 472/1000: 2.4308695793151855\n",
            "Dev loss 473/1000: 2.4302330017089844\n",
            "Dev loss 474/1000: 2.430250644683838\n",
            "Dev loss 475/1000: 2.4336187839508057\n",
            "Dev loss 476/1000: 2.4372432231903076\n",
            "Dev loss 477/1000: 2.438366651535034\n",
            "Dev loss 478/1000: 2.439610004425049\n",
            "Dev loss 479/1000: 2.4401612281799316\n",
            "Dev loss 480/1000: 2.4399542808532715\n",
            "Dev loss 481/1000: 2.440619945526123\n",
            "Dev loss 482/1000: 2.4410648345947266\n",
            "Dev loss 483/1000: 2.4410617351531982\n",
            "Dev loss 484/1000: 2.4428980350494385\n",
            "Dev loss 485/1000: 2.4450743198394775\n",
            "Dev loss 486/1000: 2.445293664932251\n",
            "Dev loss 487/1000: 2.4456236362457275\n",
            "Dev loss 488/1000: 2.445756435394287\n",
            "Dev loss 489/1000: 2.4453792572021484\n",
            "Dev loss 490/1000: 2.4447641372680664\n",
            "Dev loss 491/1000: 2.4445672035217285\n",
            "Dev loss 492/1000: 2.4439451694488525\n",
            "Dev loss 493/1000: 2.443692445755005\n",
            "Dev loss 494/1000: 2.4444546699523926\n",
            "Dev loss 495/1000: 2.4448726177215576\n",
            "Dev loss 496/1000: 2.4439198970794678\n",
            "Dev loss 497/1000: 2.4433107376098633\n",
            "Dev loss 498/1000: 2.442972421646118\n",
            "Dev loss 499/1000: 2.441554307937622\n",
            "Dev loss 500/1000: 2.441234827041626\n",
            "Dev loss 501/1000: 2.4427740573883057\n",
            "Dev loss 502/1000: 2.443498134613037\n",
            "Dev loss 503/1000: 2.443110466003418\n",
            "Dev loss 504/1000: 2.442112445831299\n",
            "Dev loss 505/1000: 2.4394123554229736\n",
            "Dev loss 506/1000: 2.4359421730041504\n",
            "Dev loss 507/1000: 2.433459758758545\n",
            "Dev loss 508/1000: 2.4324493408203125\n",
            "Dev loss 509/1000: 2.433743476867676\n",
            "Dev loss 510/1000: 2.434180736541748\n",
            "Dev loss 511/1000: 2.4326424598693848\n",
            "Dev loss 512/1000: 2.4319567680358887\n",
            "Dev loss 513/1000: 2.4333786964416504\n",
            "Dev loss 514/1000: 2.434032917022705\n",
            "Dev loss 515/1000: 2.434213399887085\n",
            "Dev loss 516/1000: 2.435002326965332\n",
            "Dev loss 517/1000: 2.4348928928375244\n",
            "Dev loss 518/1000: 2.4317564964294434\n",
            "Dev loss 519/1000: 2.428957939147949\n",
            "Dev loss 520/1000: 2.427700996398926\n",
            "Dev loss 521/1000: 2.4256179332733154\n",
            "Dev loss 522/1000: 2.4243783950805664\n",
            "Dev loss 523/1000: 2.4244415760040283\n",
            "Dev loss 524/1000: 2.422673225402832\n",
            "Dev loss 525/1000: 2.4216740131378174\n",
            "Dev loss 526/1000: 2.4230637550354004\n",
            "Dev loss 527/1000: 2.422781229019165\n",
            "Dev loss 528/1000: 2.422333002090454\n",
            "Dev loss 529/1000: 2.4247238636016846\n",
            "Dev loss 530/1000: 2.426234245300293\n",
            "Dev loss 531/1000: 2.4255402088165283\n",
            "Dev loss 532/1000: 2.4261350631713867\n",
            "Dev loss 533/1000: 2.426276922225952\n",
            "Dev loss 534/1000: 2.423511028289795\n",
            "Dev loss 535/1000: 2.4204440116882324\n",
            "Dev loss 536/1000: 2.418715715408325\n",
            "Dev loss 537/1000: 2.4170444011688232\n",
            "Dev loss 538/1000: 2.4152166843414307\n",
            "Dev loss 539/1000: 2.4132003784179688\n",
            "Dev loss 540/1000: 2.411886215209961\n",
            "Dev loss 541/1000: 2.411496162414551\n",
            "Dev loss 542/1000: 2.411808967590332\n",
            "Dev loss 543/1000: 2.4119374752044678\n",
            "Dev loss 544/1000: 2.4117226600646973\n",
            "Dev loss 545/1000: 2.4117870330810547\n",
            "Dev loss 546/1000: 2.411654233932495\n",
            "Dev loss 547/1000: 2.4119889736175537\n",
            "Dev loss 548/1000: 2.4140970706939697\n",
            "Dev loss 549/1000: 2.416191577911377\n",
            "Dev loss 550/1000: 2.4161272048950195\n",
            "Dev loss 551/1000: 2.4155991077423096\n",
            "Dev loss 552/1000: 2.4138972759246826\n",
            "Dev loss 553/1000: 2.4110326766967773\n",
            "Dev loss 554/1000: 2.41184663772583\n",
            "Dev loss 555/1000: 2.414457321166992\n",
            "Dev loss 556/1000: 2.4162068367004395\n",
            "Dev loss 557/1000: 2.4182169437408447\n",
            "Dev loss 558/1000: 2.4217662811279297\n",
            "Dev loss 559/1000: 2.423945903778076\n",
            "Dev loss 560/1000: 2.4257874488830566\n",
            "Dev loss 561/1000: 2.4270739555358887\n",
            "Dev loss 562/1000: 2.4275782108306885\n",
            "Dev loss 563/1000: 2.4276247024536133\n",
            "Dev loss 564/1000: 2.429849624633789\n",
            "Dev loss 565/1000: 2.4334006309509277\n",
            "Dev loss 566/1000: 2.436169385910034\n",
            "Dev loss 567/1000: 2.437809944152832\n",
            "Dev loss 568/1000: 2.4385039806365967\n",
            "Dev loss 569/1000: 2.437633991241455\n",
            "Dev loss 570/1000: 2.43796443939209\n",
            "Dev loss 571/1000: 2.4395289421081543\n",
            "Dev loss 572/1000: 2.4394290447235107\n",
            "Dev loss 573/1000: 2.438948392868042\n",
            "Dev loss 574/1000: 2.440176010131836\n",
            "Dev loss 575/1000: 2.4400691986083984\n",
            "Dev loss 576/1000: 2.4397287368774414\n",
            "Dev loss 577/1000: 2.4399948120117188\n",
            "Dev loss 578/1000: 2.439056396484375\n",
            "Dev loss 579/1000: 2.4383575916290283\n",
            "Dev loss 580/1000: 2.439584255218506\n",
            "Dev loss 581/1000: 2.440532684326172\n",
            "Dev loss 582/1000: 2.441075086593628\n",
            "Dev loss 583/1000: 2.4433884620666504\n",
            "Dev loss 584/1000: 2.4449830055236816\n",
            "Dev loss 585/1000: 2.445162057876587\n",
            "Dev loss 586/1000: 2.4461328983306885\n",
            "Dev loss 587/1000: 2.447563648223877\n",
            "Dev loss 588/1000: 2.4470396041870117\n",
            "Dev loss 589/1000: 2.445981502532959\n",
            "Dev loss 590/1000: 2.4458720684051514\n",
            "Dev loss 591/1000: 2.4456570148468018\n",
            "Dev loss 592/1000: 2.4443366527557373\n",
            "Dev loss 593/1000: 2.4434762001037598\n",
            "Dev loss 594/1000: 2.4424829483032227\n",
            "Dev loss 595/1000: 2.4413108825683594\n",
            "Dev loss 596/1000: 2.4416756629943848\n",
            "Dev loss 597/1000: 2.4420433044433594\n",
            "Dev loss 598/1000: 2.442304849624634\n",
            "Dev loss 599/1000: 2.44429349899292\n",
            "Dev loss 600/1000: 2.444927930831909\n",
            "Dev loss 601/1000: 2.4429149627685547\n",
            "Dev loss 602/1000: 2.4420552253723145\n",
            "Dev loss 603/1000: 2.441739082336426\n",
            "Dev loss 604/1000: 2.4396779537200928\n",
            "Dev loss 605/1000: 2.4386372566223145\n",
            "Dev loss 606/1000: 2.4382736682891846\n",
            "Dev loss 607/1000: 2.436758518218994\n",
            "Dev loss 608/1000: 2.434441566467285\n",
            "Dev loss 609/1000: 2.432466983795166\n",
            "Dev loss 610/1000: 2.430608034133911\n",
            "Dev loss 611/1000: 2.429155111312866\n",
            "Dev loss 612/1000: 2.4286839962005615\n",
            "Dev loss 613/1000: 2.4291510581970215\n",
            "Dev loss 614/1000: 2.429924488067627\n",
            "Dev loss 615/1000: 2.431023359298706\n",
            "Dev loss 616/1000: 2.4299800395965576\n",
            "Dev loss 617/1000: 2.4266088008880615\n",
            "Dev loss 618/1000: 2.4241504669189453\n",
            "Dev loss 619/1000: 2.4231467247009277\n",
            "Dev loss 620/1000: 2.420557975769043\n",
            "Dev loss 621/1000: 2.4186763763427734\n",
            "Dev loss 622/1000: 2.418372392654419\n",
            "Dev loss 623/1000: 2.417698383331299\n",
            "Dev loss 624/1000: 2.4177839756011963\n",
            "Dev loss 625/1000: 2.419481039047241\n",
            "Dev loss 626/1000: 2.420058488845825\n",
            "Dev loss 627/1000: 2.420672655105591\n",
            "Dev loss 628/1000: 2.421746015548706\n",
            "Dev loss 629/1000: 2.421236515045166\n",
            "Dev loss 630/1000: 2.4210236072540283\n",
            "Dev loss 631/1000: 2.42214298248291\n",
            "Dev loss 632/1000: 2.4220526218414307\n",
            "Dev loss 633/1000: 2.420753240585327\n",
            "Dev loss 634/1000: 2.420384645462036\n",
            "Dev loss 635/1000: 2.42073392868042\n",
            "Dev loss 636/1000: 2.4205503463745117\n",
            "Dev loss 637/1000: 2.420581817626953\n",
            "Dev loss 638/1000: 2.4207983016967773\n",
            "Dev loss 639/1000: 2.420124053955078\n",
            "Dev loss 640/1000: 2.419004440307617\n",
            "Dev loss 641/1000: 2.4193789958953857\n",
            "Dev loss 642/1000: 2.419039249420166\n",
            "Dev loss 643/1000: 2.418935775756836\n",
            "Dev loss 644/1000: 2.4195733070373535\n",
            "Dev loss 645/1000: 2.4187686443328857\n",
            "Dev loss 646/1000: 2.4174039363861084\n",
            "Dev loss 647/1000: 2.4185829162597656\n",
            "Dev loss 648/1000: 2.4199018478393555\n",
            "Dev loss 649/1000: 2.41888689994812\n",
            "Dev loss 650/1000: 2.4185357093811035\n",
            "Dev loss 651/1000: 2.4189486503601074\n",
            "Dev loss 652/1000: 2.4199554920196533\n",
            "Dev loss 653/1000: 2.421363115310669\n",
            "Dev loss 654/1000: 2.4231040477752686\n",
            "Dev loss 655/1000: 2.424407720565796\n",
            "Dev loss 656/1000: 2.426309585571289\n",
            "Dev loss 657/1000: 2.427199602127075\n",
            "Dev loss 658/1000: 2.426238536834717\n",
            "Dev loss 659/1000: 2.424764394760132\n",
            "Dev loss 660/1000: 2.42317795753479\n",
            "Dev loss 661/1000: 2.4208078384399414\n",
            "Dev loss 662/1000: 2.4196603298187256\n",
            "Dev loss 663/1000: 2.420912742614746\n",
            "Dev loss 664/1000: 2.4219717979431152\n",
            "Dev loss 665/1000: 2.4219110012054443\n",
            "Dev loss 666/1000: 2.4240357875823975\n",
            "Dev loss 667/1000: 2.427347421646118\n",
            "Dev loss 668/1000: 2.429792642593384\n",
            "Dev loss 669/1000: 2.432939052581787\n",
            "Dev loss 670/1000: 2.435981273651123\n",
            "Dev loss 671/1000: 2.4364352226257324\n",
            "Dev loss 672/1000: 2.4373397827148438\n",
            "Dev loss 673/1000: 2.4399516582489014\n",
            "Dev loss 674/1000: 2.4414784908294678\n",
            "Dev loss 675/1000: 2.442659616470337\n",
            "Dev loss 676/1000: 2.444136381149292\n",
            "Dev loss 677/1000: 2.443596124649048\n",
            "Dev loss 678/1000: 2.443310260772705\n",
            "Dev loss 679/1000: 2.445070743560791\n",
            "Dev loss 680/1000: 2.4474024772644043\n",
            "Dev loss 681/1000: 2.4486887454986572\n",
            "Dev loss 682/1000: 2.449045181274414\n",
            "Dev loss 683/1000: 2.448211669921875\n",
            "Dev loss 684/1000: 2.4479377269744873\n",
            "Dev loss 685/1000: 2.4486241340637207\n",
            "Dev loss 686/1000: 2.4495747089385986\n",
            "Dev loss 687/1000: 2.4493441581726074\n",
            "Dev loss 688/1000: 2.4501662254333496\n",
            "Dev loss 689/1000: 2.4509809017181396\n",
            "Dev loss 690/1000: 2.4490232467651367\n",
            "Dev loss 691/1000: 2.4456913471221924\n",
            "Dev loss 692/1000: 2.4432382583618164\n",
            "Dev loss 693/1000: 2.439720630645752\n",
            "Dev loss 694/1000: 2.4364728927612305\n",
            "Dev loss 695/1000: 2.436016082763672\n",
            "Dev loss 696/1000: 2.4360690116882324\n",
            "Dev loss 697/1000: 2.436134099960327\n",
            "Dev loss 698/1000: 2.4367589950561523\n",
            "Dev loss 699/1000: 2.436858654022217\n",
            "Dev loss 700/1000: 2.4363296031951904\n",
            "Dev loss 701/1000: 2.4366636276245117\n",
            "Dev loss 702/1000: 2.43601393699646\n",
            "Dev loss 703/1000: 2.435319662094116\n",
            "Dev loss 704/1000: 2.4366402626037598\n",
            "Dev loss 705/1000: 2.4380970001220703\n",
            "Dev loss 706/1000: 2.437161922454834\n",
            "Dev loss 707/1000: 2.436049222946167\n",
            "Dev loss 708/1000: 2.4351470470428467\n",
            "Dev loss 709/1000: 2.4332776069641113\n",
            "Dev loss 710/1000: 2.4309444427490234\n",
            "Dev loss 711/1000: 2.430250883102417\n",
            "Dev loss 712/1000: 2.429854393005371\n",
            "Dev loss 713/1000: 2.4290318489074707\n",
            "Dev loss 714/1000: 2.4295127391815186\n",
            "Dev loss 715/1000: 2.430624485015869\n",
            "Dev loss 716/1000: 2.430570602416992\n",
            "Dev loss 717/1000: 2.4309356212615967\n",
            "Dev loss 718/1000: 2.430349111557007\n",
            "Dev loss 719/1000: 2.4282166957855225\n",
            "Dev loss 720/1000: 2.426725149154663\n",
            "Dev loss 721/1000: 2.425527334213257\n",
            "Dev loss 722/1000: 2.423332929611206\n",
            "Dev loss 723/1000: 2.423048734664917\n",
            "Dev loss 724/1000: 2.4238383769989014\n",
            "Dev loss 725/1000: 2.424062490463257\n",
            "Dev loss 726/1000: 2.424614191055298\n",
            "Dev loss 727/1000: 2.424795389175415\n",
            "Dev loss 728/1000: 2.423344612121582\n",
            "Dev loss 729/1000: 2.421751022338867\n",
            "Dev loss 730/1000: 2.421151638031006\n",
            "Dev loss 731/1000: 2.420125961303711\n",
            "Dev loss 732/1000: 2.4185502529144287\n",
            "Dev loss 733/1000: 2.418161630630493\n",
            "Dev loss 734/1000: 2.4176816940307617\n",
            "Dev loss 735/1000: 2.415774345397949\n",
            "Dev loss 736/1000: 2.4143998622894287\n",
            "Dev loss 737/1000: 2.4144325256347656\n",
            "Dev loss 738/1000: 2.414245128631592\n",
            "Dev loss 739/1000: 2.4149181842803955\n",
            "Dev loss 740/1000: 2.416802167892456\n",
            "Dev loss 741/1000: 2.4174976348876953\n",
            "Dev loss 742/1000: 2.4170405864715576\n",
            "Dev loss 743/1000: 2.4158456325531006\n",
            "Dev loss 744/1000: 2.4130301475524902\n",
            "Dev loss 745/1000: 2.4116203784942627\n",
            "Dev loss 746/1000: 2.4122743606567383\n",
            "Dev loss 747/1000: 2.4123902320861816\n",
            "Dev loss 748/1000: 2.413240432739258\n",
            "Dev loss 749/1000: 2.4156808853149414\n",
            "Dev loss 750/1000: 2.416799783706665\n",
            "Dev loss 751/1000: 2.416787624359131\n",
            "Dev loss 752/1000: 2.41833758354187\n",
            "Dev loss 753/1000: 2.4213216304779053\n",
            "Dev loss 754/1000: 2.4235827922821045\n",
            "Dev loss 755/1000: 2.4257006645202637\n",
            "Dev loss 756/1000: 2.427793025970459\n",
            "Dev loss 757/1000: 2.4285237789154053\n",
            "Dev loss 758/1000: 2.4275050163269043\n",
            "Dev loss 759/1000: 2.4258499145507812\n",
            "Dev loss 760/1000: 2.4242091178894043\n",
            "Dev loss 761/1000: 2.4231200218200684\n",
            "Dev loss 762/1000: 2.423354387283325\n",
            "Dev loss 763/1000: 2.4237544536590576\n",
            "Dev loss 764/1000: 2.4244446754455566\n",
            "Dev loss 765/1000: 2.4267687797546387\n",
            "Dev loss 766/1000: 2.428575277328491\n",
            "Dev loss 767/1000: 2.4302611351013184\n",
            "Dev loss 768/1000: 2.433403968811035\n",
            "Dev loss 769/1000: 2.4355318546295166\n",
            "Dev loss 770/1000: 2.4364113807678223\n",
            "Dev loss 771/1000: 2.4385857582092285\n",
            "Dev loss 772/1000: 2.4398694038391113\n",
            "Dev loss 773/1000: 2.4393861293792725\n",
            "Dev loss 774/1000: 2.4395651817321777\n",
            "Dev loss 775/1000: 2.4395627975463867\n",
            "Dev loss 776/1000: 2.43881893157959\n",
            "Dev loss 777/1000: 2.439112901687622\n",
            "Dev loss 778/1000: 2.441704511642456\n",
            "Dev loss 779/1000: 2.4435112476348877\n",
            "Dev loss 780/1000: 2.4441466331481934\n",
            "Dev loss 781/1000: 2.445402145385742\n",
            "Dev loss 782/1000: 2.4470410346984863\n",
            "Dev loss 783/1000: 2.447242021560669\n",
            "Dev loss 784/1000: 2.448000431060791\n",
            "Dev loss 785/1000: 2.448887825012207\n",
            "Dev loss 786/1000: 2.4492127895355225\n",
            "Dev loss 787/1000: 2.450143337249756\n",
            "Dev loss 788/1000: 2.4505019187927246\n",
            "Dev loss 789/1000: 2.4493465423583984\n",
            "Dev loss 790/1000: 2.4485785961151123\n",
            "Dev loss 791/1000: 2.447270631790161\n",
            "Dev loss 792/1000: 2.4437224864959717\n",
            "Dev loss 793/1000: 2.4411673545837402\n",
            "Dev loss 794/1000: 2.440786361694336\n",
            "Dev loss 795/1000: 2.439767360687256\n",
            "Dev loss 796/1000: 2.4392242431640625\n",
            "Dev loss 797/1000: 2.4403209686279297\n",
            "Dev loss 798/1000: 2.4414682388305664\n",
            "Dev loss 799/1000: 2.4425699710845947\n",
            "Dev loss 800/1000: 2.4436402320861816\n",
            "Dev loss 801/1000: 2.443268299102783\n",
            "Dev loss 802/1000: 2.441904306411743\n",
            "Dev loss 803/1000: 2.439647674560547\n",
            "Dev loss 804/1000: 2.437333583831787\n",
            "Dev loss 805/1000: 2.435472249984741\n",
            "Dev loss 806/1000: 2.436448812484741\n",
            "Dev loss 807/1000: 2.4382476806640625\n",
            "Dev loss 808/1000: 2.4375555515289307\n",
            "Dev loss 809/1000: 2.4343740940093994\n",
            "Dev loss 810/1000: 2.431013822555542\n",
            "Dev loss 811/1000: 2.4265425205230713\n",
            "Dev loss 812/1000: 2.4242537021636963\n",
            "Dev loss 813/1000: 2.4255897998809814\n",
            "Dev loss 814/1000: 2.426792860031128\n",
            "Dev loss 815/1000: 2.427550792694092\n",
            "Dev loss 816/1000: 2.429224729537964\n",
            "Dev loss 817/1000: 2.42903733253479\n",
            "Dev loss 818/1000: 2.4271230697631836\n",
            "Dev loss 819/1000: 2.426093578338623\n",
            "Dev loss 820/1000: 2.424424171447754\n",
            "Dev loss 821/1000: 2.4220759868621826\n",
            "Dev loss 822/1000: 2.4217875003814697\n",
            "Dev loss 823/1000: 2.423322916030884\n",
            "Dev loss 824/1000: 2.4237005710601807\n",
            "Dev loss 825/1000: 2.423081636428833\n",
            "Dev loss 826/1000: 2.4217309951782227\n",
            "Dev loss 827/1000: 2.4201366901397705\n",
            "Dev loss 828/1000: 2.418567657470703\n",
            "Dev loss 829/1000: 2.4180147647857666\n",
            "Dev loss 830/1000: 2.4171743392944336\n",
            "Dev loss 831/1000: 2.416292667388916\n",
            "Dev loss 832/1000: 2.4162395000457764\n",
            "Dev loss 833/1000: 2.416041612625122\n",
            "Dev loss 834/1000: 2.415719985961914\n",
            "Dev loss 835/1000: 2.4168128967285156\n",
            "Dev loss 836/1000: 2.416562557220459\n",
            "Dev loss 837/1000: 2.4144937992095947\n",
            "Dev loss 838/1000: 2.4144859313964844\n",
            "Dev loss 839/1000: 2.4155938625335693\n",
            "Dev loss 840/1000: 2.415418863296509\n",
            "Dev loss 841/1000: 2.4162838459014893\n",
            "Dev loss 842/1000: 2.4176485538482666\n",
            "Dev loss 843/1000: 2.417689323425293\n",
            "Dev loss 844/1000: 2.418039083480835\n",
            "Dev loss 845/1000: 2.4187591075897217\n",
            "Dev loss 846/1000: 2.418026924133301\n",
            "Dev loss 847/1000: 2.4171836376190186\n",
            "Dev loss 848/1000: 2.4171242713928223\n",
            "Dev loss 849/1000: 2.4181435108184814\n",
            "Dev loss 850/1000: 2.4199602603912354\n",
            "Dev loss 851/1000: 2.421762466430664\n",
            "Dev loss 852/1000: 2.421358823776245\n",
            "Dev loss 853/1000: 2.4190762042999268\n",
            "Dev loss 854/1000: 2.41750431060791\n",
            "Dev loss 855/1000: 2.417262554168701\n",
            "Dev loss 856/1000: 2.4176180362701416\n",
            "Dev loss 857/1000: 2.4194960594177246\n",
            "Dev loss 858/1000: 2.4212377071380615\n",
            "Dev loss 859/1000: 2.4209580421447754\n",
            "Dev loss 860/1000: 2.4223365783691406\n",
            "Dev loss 861/1000: 2.4251809120178223\n",
            "Dev loss 862/1000: 2.4259936809539795\n",
            "Dev loss 863/1000: 2.427884578704834\n",
            "Dev loss 864/1000: 2.4319448471069336\n",
            "Dev loss 865/1000: 2.4335741996765137\n",
            "Dev loss 866/1000: 2.4336438179016113\n",
            "Dev loss 867/1000: 2.4356443881988525\n",
            "Dev loss 868/1000: 2.4373021125793457\n",
            "Dev loss 869/1000: 2.437549591064453\n",
            "Dev loss 870/1000: 2.438070297241211\n",
            "Dev loss 871/1000: 2.4386041164398193\n",
            "Dev loss 872/1000: 2.4374446868896484\n",
            "Dev loss 873/1000: 2.43674898147583\n",
            "Dev loss 874/1000: 2.4375369548797607\n",
            "Dev loss 875/1000: 2.439213752746582\n",
            "Dev loss 876/1000: 2.441173791885376\n",
            "Dev loss 877/1000: 2.44267201423645\n",
            "Dev loss 878/1000: 2.442898750305176\n",
            "Dev loss 879/1000: 2.4444773197174072\n",
            "Dev loss 880/1000: 2.4475293159484863\n",
            "Dev loss 881/1000: 2.449211597442627\n",
            "Dev loss 882/1000: 2.4498376846313477\n",
            "Dev loss 883/1000: 2.4512460231781006\n",
            "Dev loss 884/1000: 2.4504871368408203\n",
            "Dev loss 885/1000: 2.447232723236084\n",
            "Dev loss 886/1000: 2.4451544284820557\n",
            "Dev loss 887/1000: 2.4436068534851074\n",
            "Dev loss 888/1000: 2.442033052444458\n",
            "Dev loss 889/1000: 2.4422683715820312\n",
            "Dev loss 890/1000: 2.443230628967285\n",
            "Dev loss 891/1000: 2.443502426147461\n",
            "Dev loss 892/1000: 2.443258762359619\n",
            "Dev loss 893/1000: 2.441118001937866\n",
            "Dev loss 894/1000: 2.4385228157043457\n",
            "Dev loss 895/1000: 2.439218759536743\n",
            "Dev loss 896/1000: 2.4417636394500732\n",
            "Dev loss 897/1000: 2.442678689956665\n",
            "Dev loss 898/1000: 2.443005323410034\n",
            "Dev loss 899/1000: 2.444129705429077\n",
            "Dev loss 900/1000: 2.444110631942749\n",
            "Dev loss 901/1000: 2.4422593116760254\n",
            "Dev loss 902/1000: 2.4423208236694336\n",
            "Dev loss 903/1000: 2.442103862762451\n",
            "Dev loss 904/1000: 2.4388468265533447\n",
            "Dev loss 905/1000: 2.43620228767395\n",
            "Dev loss 906/1000: 2.435879945755005\n",
            "Dev loss 907/1000: 2.434955596923828\n",
            "Dev loss 908/1000: 2.434067487716675\n",
            "Dev loss 909/1000: 2.4338903427124023\n",
            "Dev loss 910/1000: 2.4332504272460938\n",
            "Dev loss 911/1000: 2.431992530822754\n",
            "Dev loss 912/1000: 2.431297540664673\n",
            "Dev loss 913/1000: 2.4303715229034424\n",
            "Dev loss 914/1000: 2.4296178817749023\n",
            "Dev loss 915/1000: 2.428028106689453\n",
            "Dev loss 916/1000: 2.425262928009033\n",
            "Dev loss 917/1000: 2.4226441383361816\n",
            "Dev loss 918/1000: 2.4197275638580322\n",
            "Dev loss 919/1000: 2.416654348373413\n",
            "Dev loss 920/1000: 2.415931463241577\n",
            "Dev loss 921/1000: 2.4175217151641846\n",
            "Dev loss 922/1000: 2.4183595180511475\n",
            "Dev loss 923/1000: 2.418018102645874\n",
            "Dev loss 924/1000: 2.4196808338165283\n",
            "Dev loss 925/1000: 2.4222452640533447\n",
            "Dev loss 926/1000: 2.4221630096435547\n",
            "Dev loss 927/1000: 2.4208359718322754\n",
            "Dev loss 928/1000: 2.4206066131591797\n",
            "Dev loss 929/1000: 2.419081449508667\n",
            "Dev loss 930/1000: 2.416966676712036\n",
            "Dev loss 931/1000: 2.417027235031128\n",
            "Dev loss 932/1000: 2.4183828830718994\n",
            "Dev loss 933/1000: 2.4187307357788086\n",
            "Dev loss 934/1000: 2.419329881668091\n",
            "Dev loss 935/1000: 2.418393135070801\n",
            "Dev loss 936/1000: 2.417634963989258\n",
            "Dev loss 937/1000: 2.418062210083008\n",
            "Dev loss 938/1000: 2.4177749156951904\n",
            "Dev loss 939/1000: 2.4170148372650146\n",
            "Dev loss 940/1000: 2.4175286293029785\n",
            "Dev loss 941/1000: 2.4180023670196533\n",
            "Dev loss 942/1000: 2.4182939529418945\n",
            "Dev loss 943/1000: 2.420287847518921\n",
            "Dev loss 944/1000: 2.4219954013824463\n",
            "Dev loss 945/1000: 2.4219956398010254\n",
            "Dev loss 946/1000: 2.420992851257324\n",
            "Dev loss 947/1000: 2.420515298843384\n",
            "Dev loss 948/1000: 2.4191792011260986\n",
            "Dev loss 949/1000: 2.41786527633667\n",
            "Dev loss 950/1000: 2.4171805381774902\n",
            "Dev loss 951/1000: 2.416414260864258\n",
            "Dev loss 952/1000: 2.4164299964904785\n",
            "Dev loss 953/1000: 2.4183855056762695\n",
            "Dev loss 954/1000: 2.4185099601745605\n",
            "Dev loss 955/1000: 2.417468547821045\n",
            "Dev loss 956/1000: 2.41903018951416\n",
            "Dev loss 957/1000: 2.420687437057495\n",
            "Dev loss 958/1000: 2.420565128326416\n",
            "Dev loss 959/1000: 2.422475814819336\n",
            "Dev loss 960/1000: 2.4249019622802734\n",
            "Dev loss 961/1000: 2.4246721267700195\n",
            "Dev loss 962/1000: 2.4248173236846924\n",
            "Dev loss 963/1000: 2.427690267562866\n",
            "Dev loss 964/1000: 2.430541753768921\n",
            "Dev loss 965/1000: 2.4327471256256104\n",
            "Dev loss 966/1000: 2.4341542720794678\n",
            "Dev loss 967/1000: 2.434154510498047\n",
            "Dev loss 968/1000: 2.433919668197632\n",
            "Dev loss 969/1000: 2.434723377227783\n",
            "Dev loss 970/1000: 2.4361138343811035\n",
            "Dev loss 971/1000: 2.4377527236938477\n",
            "Dev loss 972/1000: 2.4397594928741455\n",
            "Dev loss 973/1000: 2.4414281845092773\n",
            "Dev loss 974/1000: 2.442171096801758\n",
            "Dev loss 975/1000: 2.4434964656829834\n",
            "Dev loss 976/1000: 2.4448773860931396\n",
            "Dev loss 977/1000: 2.4441897869110107\n",
            "Dev loss 978/1000: 2.443861722946167\n",
            "Dev loss 979/1000: 2.444793224334717\n",
            "Dev loss 980/1000: 2.444063663482666\n",
            "Dev loss 981/1000: 2.4441404342651367\n",
            "Dev loss 982/1000: 2.446113348007202\n",
            "Dev loss 983/1000: 2.4459609985351562\n",
            "Dev loss 984/1000: 2.4455440044403076\n",
            "Dev loss 985/1000: 2.447453737258911\n",
            "Dev loss 986/1000: 2.4486801624298096\n",
            "Dev loss 987/1000: 2.448066473007202\n",
            "Dev loss 988/1000: 2.447348117828369\n",
            "Dev loss 989/1000: 2.446727991104126\n",
            "Dev loss 990/1000: 2.4461891651153564\n",
            "Dev loss 991/1000: 2.445908784866333\n",
            "Dev loss 992/1000: 2.4458603858947754\n",
            "Dev loss 993/1000: 2.4465391635894775\n",
            "Dev loss 994/1000: 2.447962522506714\n",
            "Dev loss 995/1000: 2.447920322418213\n",
            "Dev loss 996/1000: 2.4453210830688477\n",
            "Dev loss 997/1000: 2.443164110183716\n",
            "Dev loss 998/1000: 2.442607879638672\n",
            "Dev loss 999/1000: 2.4419965744018555\n",
            "Dev loss 1000/1000: 2.441124677658081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_weights.grad)\n",
        "smoothed_weights.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ILs0POXWBi1",
        "outputId": "85ada056-8741-48fa-df6d-58b6a144eaa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 9.5886e-06,  3.7515e-07, -3.7217e-07,  ..., -3.9107e-07,\n",
            "         -3.6552e-07, -4.1003e-07],\n",
            "        [ 8.7005e-06, -4.3350e-07, -4.3295e-07,  ..., -5.4515e-07,\n",
            "         -4.3455e-07, -4.3623e-07],\n",
            "        [ 3.5884e-06, -5.6147e-06,  3.7776e-06,  ...,  2.7962e-06,\n",
            "         -5.7857e-06,  2.1665e-06],\n",
            "        ...,\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [-4.3128e-06, -4.4627e-06,  2.3119e-06,  ...,  2.6989e-06,\n",
            "          5.3450e-06,  2.7283e-06],\n",
            "        [-5.2416e-06, -8.6936e-06,  3.1826e-06,  ...,  4.5518e-06,\n",
            "         -1.2195e-05,  4.8728e-06]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.7007e-04, -1.0763e-04,  1.0887e-03,  ...,  6.3785e-04,\n",
              "          2.2499e-04, -4.4541e-04],\n",
              "        [-3.4200e-04, -9.4844e-04,  2.4531e-03,  ...,  3.3765e-04,\n",
              "         -7.0552e-04, -1.4215e-03],\n",
              "        [ 9.4209e-05, -1.0226e-03,  9.5708e-05,  ...,  8.6067e-05,\n",
              "          2.3030e-04,  7.2270e-05],\n",
              "        ...,\n",
              "        [-8.5193e-05,  5.7341e-05, -4.7001e-05,  ..., -7.2111e-05,\n",
              "         -2.8910e-04, -2.2474e-05],\n",
              "        [-1.1804e-03, -7.5837e-04, -3.3226e-04,  ..., -2.6523e-04,\n",
              "          1.1146e-04, -2.6019e-04],\n",
              "        [-3.2234e-04, -9.6969e-04,  1.2076e-04,  ...,  8.0720e-05,\n",
              "         -4.2856e-04,  7.2831e-05]])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_names(smoothed_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oe2s_rsrUy_3",
        "outputId": "0bb7bb89-e5a7-4f6a-836d-2a4aa1342e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "junidedion\n",
            "kar\n",
            "prisay\n",
            "adin\n",
            "kovin\n",
            "to\n",
            "shamareem\n",
            "sameiaurio\n",
            "levencedbduinrwin\n",
            "ads\n",
            "jainarvarterinfeumeryfoeturjachitsuf\n",
            "hesan\n",
            "rhore\n",
            "ya\n",
            "jocfpyjakeir\n",
            "edim\n",
            "ki\n",
            "wyni\n",
            "san\n",
            "asnhavilaspsenhddion\n",
            "matteric\n",
            "seremungslatia\n",
            "som\n",
            "a\n",
            "ish\n",
            "dyn\n",
            "rajer\n",
            "jemah\n",
            "dawath\n",
            "khyleiggraydornesonta\n",
            "malyn\n",
            "balihi\n",
            "iseaurpotblahemir\n",
            "tawath\n",
            "basel\n",
            "khiveertikeysaleever\n",
            "nan\n",
            "em\n",
            "fab\n",
            "tremraxx\n",
            "el\n",
            "chwi\n",
            "jachodrio\n",
            "dwin\n",
            "jay\n",
            "kpvrgeveemariancidendo\n",
            "quiv\n",
            "emon\n",
            "hib\n",
            "hakel\n",
            "grilwarsendyon\n",
            "ali\n",
            "alian\n",
            "tewsen\n",
            "kenfddir\n",
            "carvjvokeyor\n",
            "nalrashartafeldroderighno\n",
            "shd\n",
            "bris\n",
            "olleass\n",
            "saysia\n",
            "bus\n",
            "dagaven\n",
            "tyn\n",
            "codfddqzlydo\n",
            "katalm\n",
            "otteslef\n",
            "dman\n",
            "stestanadedihannge\n",
            "jameseniveer\n",
            "uze\n",
            "natgjjvovegfjt\n",
            "mufrafelan\n",
            "yurileon\n",
            "dorreste\n",
            "arso\n",
            "revertiparkin\n",
            "jay\n",
            "shia\n",
            "maud\n",
            "la\n",
            "acvjvon\n",
            "saalextimso\n",
            "salpb\n",
            "brianieri\n",
            "otz\n",
            "narkbhhhi\n",
            "isaughakamorven\n",
            "kamel\n",
            "taz\n",
            "arson\n",
            "taliah\n",
            "dayrgehbalqad\n",
            "muyton\n",
            "zachamedi\n",
            "han\n",
            "ke\n",
            "etsultreextik\n",
            "ny\n",
            "naaston\n",
            "tryonakisemman\n",
            "wwlsxxelicklxfkta\n",
            "lj\n",
            "gkjmsofinfenicnaviula\n",
            "jvon\n",
            "\n",
            "den\n",
            "hawyleemilasen\n",
            "kolum\n",
            "nakadicton\n",
            "zaam\n",
            "quaditadey\n",
            "vin\n",
            "kayviyavia\n",
            "hukoltillo\n",
            "man\n",
            "lib\n",
            "hartae\n",
            "azah\n",
            "fbsiile\n",
            "rays\n",
            "khin\n",
            "hand\n",
            "wcaripqkwu\n",
            "prileanneerdalinpwqdkianancsgltrzarandoub\n",
            "myah\n",
            "kwwb\n",
            "evari\n",
            "leoke\n",
            "luithasir\n",
            "den\n",
            "mee\n",
            "kariom\n",
            "mptavi\n",
            "hazie\n",
            "fik\n",
            "taesten\n",
            "trie\n",
            "facikel\n",
            "giakyan\n",
            "stas\n",
            "derric\n",
            "bende\n",
            "dalechanshamalmannejmfxbxgsto\n",
            "trael\n",
            "tas\n",
            "xan\n",
            "aek\n",
            "marceyo\n",
            "kete\n",
            "yton\n",
            "kacamaryqexon\n",
            "carixdon\n",
            "abduinsonds\n",
            "nyfwayvee\n",
            "ya\n",
            "ahmielav\n",
            "noaid\n",
            "quckpkrnephlase\n",
            "kyreigsto\n",
            "maposvith\n",
            "all\n",
            "tonritalius\n",
            "joharisen\n",
            "kamerae\n",
            "siy\n",
            "osharben\n",
            "ab\n",
            "sufdleon\n",
            "qdi\n",
            "yuvick\n",
            "manmileekqytneldredon\n",
            "kisspizjavarsel\n",
            "menozyme\n",
            "muss\n",
            "twoord\n",
            "miel\n",
            "leximithan\n",
            "alux\n",
            "alcidzsn\n",
            "hamin\n",
            "halariyo\n",
            "vkvekhtaeck\n",
            "helon\n",
            "rydoncdnu\n",
            "badhishamay\n",
            "nadfinnetmuthanicklyn\n",
            "cob\n",
            "aryseffum\n",
            "ya\n",
            "blananaftforylogereextoncevrilib\n",
            "saanatarki\n",
            "aqadrinexzay\n",
            "karem\n",
            "aison\n",
            "aizymoziz\n",
            "keiveonatchlvaderaghnieem\n",
            "galeion\n",
            "ziron\n",
            "dalmirem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(smoothed_weights, test_inputs, test_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr6wFPm-U8fG",
        "outputId": "7ee38c16-f4bc-4e5f-aa30-90faacaae37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.54518723487854\n"
          ]
        }
      ]
    }
  ]
}